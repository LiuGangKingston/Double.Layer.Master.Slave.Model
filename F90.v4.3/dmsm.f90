!    Double-layer Master-Slave Model Library
!           Version 4.3
!    Copyright (c) 2009-2015 by High Performance Computing Virtual Laboratory
!    www.hpcvl.org
!
!    This file containes a set of modules and
!    routines, as a library for
!    Double-layer Master-Slave Model (DMSM).
!    This library can be used in two scenarios.
!    One is to employ the MPI Master-Slave and
!    OpenMP all-slave Models nestedly for the
!    job distribution and each of many
!    independent jobs is completed by one thread.
!    The other scenario is to employ the MPI
!    Master-Slave model for job distribution
!    and each job is done either by all OpenMP
!    threads of a certain process or by a subset
!    of MPI processes, in parallel. In any
!    scenario, this library does all job
!    distribution work and calls a user supplied
!    routine to perform all user's specific jobs.
!
!    For many independent small jobs with CPU-time
!    unknown and/or uneven, the first scenatio is
!    a good choice, while for many independent
!    relative big jobs, the second scenario is
!    better. While a regular existing
!    serial code can be converted into a user's
!    specific job routine and this library is
!    employed to complete many such jobs as in
!    the first scenario, a regular existing OpenMP
!    or MPI parallel code can be converted into a
!    user's specific job routine and this library
!    is used to perform many such jobs as in the
!    second scenatio, provided all I/O operations
!    and MPI communicators handled properly. An
!    MPI code normally run with 64 processes for a
!    job, for an example, can be converted and
!    combined with this library to run with 257
!    processes at a time. While one of the processes
!    works as the Master to distribute jobs, all the
!    rest processes are divided into four subsets
!    with 64 processes each. Any subset can accept a
!    certain job, which is then executed by the 64
!    processes of the subset in parallel. As a
!    result, total 4 jobs at maximum can be executed
!    by all subsets simultaneously. In one word,
!    this library can be used as an outter additional
!    layer parallelism (of job distribution) over a
!    regular existing serial/parallel code.
!
!    This library strictly adopts a naming
!    convention: all names of global constants,
!    variables, routines, and files generated by
!    this file begin with DMSM_ as a prefix.
!
!    The cluster to complete the whole calculation
!    task is assumed made of some nodes and each
!    node is assumed as a shared-memory machine.
!    Although many MPI processes may be in the same
!    shared-memory node, for simplicity and clarity,
!    let us further regard the (MPI) Master and
!    (MPI) Slaves as corresponding nodes and assume
!    that inside each node there are OpenMP threads,
!    including the main thread and the rest threads,
!    as usual.
!
!    In order to reduce communication for job
!    distribution, all the independent jobs are
!    grouped into groups, each of which has
!    NUM_OF_JOBS_PER_GROUP independent jobs.
!    In this library, the Master always assign
!    individual job group as a whole to the
!    Slaves, by employing the MPI Master-slave
!    model.
!
!    Then in the first scenario, the Slaves
!    normally start an OpenMP parallel region
!    to create threads, who then complete all
!    jobs of the group based on OpenMP
!    "all-slave" Model. Let us discuss the
!    usage of this library for the first
!    scenario in the following firstly.
!
!    In this scenario, considering the nature of
!    the mixture of OpenMP and MPI, MPI
!    communication is only allowed between
!    different nodes and with one thread of each
!    node involved at a time (in OpenMP
!    protected regions, if applicable), for the
!    sake of safety. Job group assignment to the
!    rest threads in the Master node by the main
!    thread is done via shared variables. However,
!    this library still allows users to choose any
!    combination of the following three working
!    modes (i=1,2,3) of the Master and three working
!    modes (j=1,2,3) of the Slaves, by supplying
!    corresponding integer value of
!    JOB_DISTRIBUTION_PLAN=i*10+j as an argument,
!    when the library is called. All of these nine
!    combinations have been tested successfully
!    in the HPCVL Sun-Fire Clusters.
!
!    In mode i=1, the Master never enters an OpenMP
!    parallel region, but distributes job groups
!    to Slaves only.
!
!    In mode i=2, the Master enters an OpenMP
!    parallel region, where the main thread
!    distributes job groups to Slaves, while the rest
!    threads complete some preallocated job groups
!    in the "all-slave" Model in each job group.
!
!    In mode i=3, the Master enters an OpenMP
!    parallel region, where the main thread
!    distributes job groups to the Slaves and to the
!    rest threads of the same node, while the rest
!    threads complete every dynamically received job
!    group in the "all-slave" Model. Again the
!    communication between the main thread and the
!    rest threads are not done with MPI but with
!    shared varibles for job group assignment,
!    since they are in the same Master node.
!
!    In mode j=1, a Slave (always) talks with and
!    receives a job group from the Master outside
!    any OpenMP parallel region. Then it enters an
!    OpenMP parallel region, spawns threads, and
!    the threads complete all jobs of the received
!    group. When the job group is done, closes the
!    OpenMP region and repeats the whole procedure.
!
!    In mode j=2, any Slave always lives inside an
!    OpenMP parallel region, but only the main
!    thread of it is allowed to talk with and to
!    receive a job group from the Master, whenever
!    it finds out no more job left un-assigned in
!    the node.
!
!    In mode j=3, anything is the same with mode
!    j=2 except that in this mode, any thread is
!    allowed to talk with and to receive job
!    groups from the Master.
!
!    However additionally, the following three
!    special cases for job group distribution are
!    also implimented.
!
!    Special case 1, if both the total number
!    of MPI processes and the total number of
!    threads per process are 1, a pure serial version
!    is employed to complete jobs.
!
!    Special case 2, otherwise, if the total number
!    of MPI processes is 1, a pure OpenMP "all-slave"
!    model is employed.
!
!    Special case 3, otherwise, if the total number
!    of threads per process is 1, a pure MPI
!    Master-Slave model is employed.
!
!    For a simple usage of this library, code the
!    following statement
!       USE DMSM_MODULE
!    into the user's code firstly, then call the
!    following four routines in sequences:
!       CALL DMSM_INITIALIZE(...)
!       CALL DMSM_WORKING(...)
!       CALL DMSM_JOB_DISTRIBUTION_CHECKUP()
!       CALL DMSM_FINALIZE()
!    or call the one wrapper routine
!       CALL DMSM_ALL(...)
!    for all of the four routines.
!
!    Some of these routines are overloaded. For
!    detailed arguments, please check the manual,
!    the module DMSM_MODULE below, and/or the
!    test example files.
!
!    One must code his/her own routine for specific
!    jobs, and uses its name as an argument when calls
!    the above routine(s). The job routine has only
!    one argument, which is the job sequencial number
!    from 1, assigned by this library.
!       SUBROUTINE DO_THE_JOB(JOB_NUMBER)
!    is a good one. In all the total six test examples,
!    the computational tasks/jobs are the same: to
!    calculate sqare root summation from 0 to an upper
!    integer limit with step 1. Although the summation
!    with the maximum upper limit includes all the
!    rest, we regard all of the summation jobs as
!    independant ones for demonstration purpose.
!
!    The simpliest way to use this library is to
!    broadcast all initial data across processes
!    and make them available to any thread at the
!    beginning, then call this library to complete
!    all jobs, and then collect all results in
!    one's own seperate routine. In other words,
!    this library only assigns jobs essentially.
!    Example 1 is of this situation.
!
!    Some users may want to prepare the initial
!    data for a job group, only when the job group
!    is assigned/received, rather than prepare all
!    initial data beforehand, for any reason. Then
!    users should code such a routine and pass it
!    as another argument into the library, then the
!    library runs it when any job group is assigned
!    and received. Since the initial data for the
!    received job group should be shared in OpenMP
!    essentially, inside each process/node, there
!    would be a race condition when one thread
!    is trying to access the initial data for a job
!    of the current job group, and at the same time
!    another thread is trying to update the whole
!    initial data for the received next job group.
!    In order to avoid this race condition, the
!    library impliments and manages some locks or
!    equivalent ones. Users are required and only
!    required to
!        CALL DMSM_UNSET_AN_INITIAL_LOCK()
!    as soon as after the initial data for the job is
!    fetched (usually copied to some temporary places)
!    in the user's job routine, and required to
!        CALL DMSM_WAIT_FOR_INITIAL_LOCKS()
!    at the beginning of the user's initial data
!    preparation routine for job groups.
!    Example 2 is of this situation.
!
!    The library also supplies the following integer
!    functions
!        DMSM_GET_GROUP_START(JOB_NUMBER)
!        DMSM_GET_GROUP_END(JOB_NUMBER)
!        DMSM_GET_GROUP_NUMBER(JOB_NUMBER)
!    returning the beginning job sequential number,
!    the end job sequential number, and the job group
!    number of the job group, where the job number
!    JOB_NUMBER resites, respectively. At least,
!    the first one is often needed in the user's job
!    routine, when the initial data for job groups
!    prepared dynamically.
!
!    Furthermore, some users may want to collect
!    computed results as quickly as possible. So
!    this library also allows users to collect
!    results accessible when a job group is assigned
!    or received. For this purpose, the user's own
!    result collecting routine should be passed as
!    another argument followed by a logical argument
!    when the library is called. If the logical
!    argument is .TRUE., the result collecting
!    routine is called when a job group is
!    assigned/received, when "no more job group"
!    message is sent/received, and when all jobs
!    are finished; otherwise, called only when all
!    the jobs are finished, to collect any results
!    not collected in any case.
!
!    The user's result collection routine should be
!    coded in the following way. 1, Once any result(s)
!    is(are) sent from any thread to the Master, the
!    result(s) should be deleted by the sender to
!    avoid being sent again. 2, In every MPI
!    communication, a Slave should send the Master
!    an integer first, which is the number (of elements)
!    of results to be sent next immediately. If it
!    is zero, there will be no the expected "next
!    sending" and no the exptected "next receiving".
!
!    As all results are assumed to be collected
!    into some fixed place of the Master node, the
!    user's result collection routine is called
!    by the library from a locked/protected region,
!    only if applicable, to avoid some race
!    condition on the destination data struction.
!
!    As the user's job routine and the user's
!    result collection routine are always called
!    independently, the result from each job can
!    not be placed into any private data structure
!    for later collection. Example 3 uses a
!    THREADPRIVATE array FINAL_RESULT_IN_THREAD,
!    instead, and collects results dynamically.
!
!    Definitely, OpenMP shared data structure
!    can also be employed to hold results for
!    each job and collected later. However
!    whenever such a shared data structure is
!    updated, there is possible race condition.
!    To avoid it, the library supplies and manages
!    additional locks, and users can make use of
!    them by just
!        CALL DMSM_SET_NODE_RESULT_LOCK()
!    and
!        CALL DMSM_UNSET_NODE_RESULT_LOCK()
!    before and after the updating.
!    Example 4, using the shared array
!    FINAL_RESULT_IN_PROCESS, is of this situation.
!
!    The last issue regarding result collection is
!    whether it is called from an OpenMP parallel
!    region of this library, after all jobs are
!    finished. The default is in parallel region,
!    as the constant
!        DMSM_COLLECT_RESULTS_PARALLEL=1
!    set at the beginning of this code. If this
!    parameter is changed into any other value,
!    the user's result collection routine is
!    called outside any OpenMP parallel region
!    after all jobs are finished. This way of
!    usage of the library may also need the
!    following logical function
!        FUNCTION DMSM_ALL_JOBS_DONE()
!    to determine whether all jobs are done.
!    Although the library provides such a
!    flexibility, however, it is not
!    encouraged, as more users' programming is
!    needed. All our examples are coded for the
!    default DMSM_COLLECT_RESULTS_PARALLEL=1
!    setting.
!
!    For any reason, if only some of the
!    processes, rather than all of them, are
!    expected to call this library, create an
!    MPI communicator of the processes, do
!       CALL DMSM_SET_COMM(THE_COMMUNICATOR)
!    and have all and only the processes
!    belonging to the communication
!    THE_COMMUNICATOR call this library.
!
!    The routine DMSM_SET_COMM(...) must be
!    called before any other routine call of
!    this library. This facility enables the
!    second scenario become possible. From
!    now on, let us discuss the second
!    scenario.
!
!    It is very easy to get any user's job
!    completed by all threads of a slave node in
!    parallel, in OpenMP region(s) of the user's
!    own job routine, with a pure MPI
!    Master-Slave model, special case 3 of this
!    library, enabled to distribute jobs, by
!    passing the total number of threads per
!    process as 1 when this library is called,
!    although it is bigger than 1 in reality.
!    Example dmsm.example.job.openmp.f90 is of
!    this situation.
!
!    Now let us focus on a more complicated
!    situation, where each job is done by a
!    subset of MPI processes, as the example
!    of 1+4*64=257 processes illustrates at
!    the beginning of this comment area. In
!    order to have this situation to work,
!    more communicators and more
!    communications are needed.
!
!    Since each user's job will be completed
!    by a subset of MPI processes in parallel,
!    an MPI communicator for each subset is
!    needed. Let us call it as job
!    communicator, while the communicator of
!    all processes involved in the whole
!    computation task as the original
!    communicator. The Master process will form
!    a job communicator only containing itself.
!    As inside this library, each job or job
!    group is assigned to one process only,
!    another communicator, made of all processes
!    of zero ranks of all job communicators, is
!    also needed. Let us call this third
!    communicator as distribute communicator,
!    as it will be used for job distribution.
!
!    For this situation, the library supplies
!    two more routines:
!       CALL DMSM_GEN_COMM_MPI_ALL(...)
!       CALL DMSM_MPI_ALL(...)
!    to replace DMSM_ALL(...) calling.
!    The former with 10 integer auguments will
!    generate the job communicator, the
!    distribute communicator from the original
!    communicator (second argument), and set
!    them in the library accordingly for later
!    usage. The job communicator will be
!    created based on the first argument, which
!    is the number of processes in each job
!    communicator. However apparently the last
!    job communicator will get processes whatever
!    it can get. The latter call will perform
!    additional necessary communications and all
!    computational jobs. The code
!        dmsm.example.mpi.f90
!    is an example of this situation.
!
!    As in dmsm.example.mpi.f90, user's routines
!    to prepare the initial data for a job group
!    and to collect results will only work inside
!    the distribution communicator. This means
!    only the process of rank zero of each job
!    communicator will be involved in these
!    communications. Then further communications
!    for data initialization, computation, and
!    result collection inside each job are
!    usually needed within job communicators
!    respectively.
!
!    In HPCVL machines, examples can be compiled
!    with Cluster_Tool 8 of the command
!    use ct8
!    mpif90 -dalign -fast -xopenmp -C \
!          dmsm.f90 EXAMPLEk.f90 -lm
!    and run with commands
!        echo THREADS > \
!          TotalNumberOfOpenMPThreadsPerProcess
!        mpirun -np PROCESSES x THREADS ./a.out
!    or
!        echo SIZE_Of_JOB_COMM > \
!          MPI.PROCESSES.IN.JOB.COMM.dat
!        mpirun -np SIZE_OF_ORIGINAL_COMM ./a.out
!    or by using script files, just command
!    hk PROCESSES THREADS (for example k)
!    h3 4 5 (for an example)
!    or
!    ho PROCESSES THREADS
!    or
!    hm PROCESSES SIZE_Of_JOB_COMM
!    to compile and run.
!
!    Please send problems to
!           gang.liu@queensu.ca
!
!    www.hpcvl.org
!    Copyright (c) 2009-2015 by High Performance Computing Virtual Laboratory


MODULE DMSM_FEATURE_CONSTANTS
  INTEGER, PARAMETER   :: DMSM_JOB_GROUPING=1
  INTEGER, PARAMETER   :: DMSM_COLLECT_RESULTS_PARALLEL=1
  INTEGER, PARAMETER   :: DMSM_TIMING_SUMMARY_TO_SCREEN=0
  INTEGER, PARAMETER   :: DMSM_MASTER_SLAVE_SHOWING=0
  INTEGER, PARAMETER   :: DMSM_INITIAL_LOCKS_EMPLOYED=0
  INTEGER, PARAMETER   :: DMSM_JOBS_DONE_BY_WHOM_SWITCH=1
  INTEGER, PARAMETER   :: DMSM_MAXIMUM_TUNNEL=99
  INTEGER, PARAMETER   :: DMSM_MPI_TAG_ZERO_POINT=10
  INTEGER, PARAMETER   :: DMSM_ONLY_MPI_TRUE_MASTER_PLAN=1
END MODULE DMSM_FEATURE_CONSTANTS


MODULE DMSM_ESSENTIAL_CONSTANTS
  INTEGER, PARAMETER   :: DMSM_ALWAYS_ZERO=0
  INTEGER, PARAMETER   :: DMSM_GROUP_INFORMATION_SIZE=3
END MODULE DMSM_ESSENTIAL_CONSTANTS


#ifdef DMSM_FORTRAN_RS16_TIMER
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
#else
MODULE DMSM_CTIMER_INTERFACE
  INTERFACE
    DOUBLE PRECISION FUNCTION DMSM_C_TIMER()
    END FUNCTION DMSM_C_TIMER
  END INTERFACE
END MODULE DMSM_CTIMER_INTERFACE
#endif
#endif


MODULE DMSM_INITIALIZE_C_INTERFACE
  INTERFACE
  SUBROUTINE DMSM_INITIALIZE_CORE(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN, &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
      IMPLICIT NONE
      INTEGER ::           THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  END SUBROUTINE DMSM_INITIALIZE_CORE
  END INTERFACE
END MODULE DMSM_INITIALIZE_C_INTERFACE


MODULE DMSM_INITIALIZE_INTERFACE
  INTERFACE	DMSM_INITIALIZE
    SUBROUTINE DMSM_INITIALIZE_1(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN, &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
    IMPLICIT NONE
    INTEGER ::             THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
    END SUBROUTINE DMSM_INITIALIZE_1
  END INTERFACE	DMSM_INITIALIZE
END MODULE DMSM_INITIALIZE_INTERFACE


MODULE DMSM_WORKING_CORE_INTERFACE
  INTERFACE
  SUBROUTINE DMSM_WORKING_CORE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
      IMPLICIT NONE
      LOGICAL :: RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_WORKING_CORE
  END INTERFACE
END MODULE DMSM_WORKING_CORE_INTERFACE


MODULE DMSM_WORKING_INTERFACE
  INTERFACE	 DMSM_WORKING
  SUBROUTINE DMSM_WORKING_1(DO_THE_JOB)
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
  END SUBROUTINE DMSM_WORKING_1

  SUBROUTINE DMSM_WORKING_2(DO_THE_JOB, JOB_GROUP_PREPARATION)
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
  END SUBROUTINE DMSM_WORKING_2

  SUBROUTINE DMSM_WORKING_3(DO_THE_JOB, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  LOGICAL :: RESULT_COLLECTION_ENABLED
  END SUBROUTINE DMSM_WORKING_3

  SUBROUTINE DMSM_WORKING_4(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  LOGICAL :: RESULT_COLLECTION_ENABLED
  END SUBROUTINE DMSM_WORKING_4

  END INTERFACE	 DMSM_WORKING
END MODULE DMSM_WORKING_INTERFACE


MODULE DMSM_ALL_INTERFACE
  INTERFACE	 DMSM_ALL

    SUBROUTINE    DMSM_ALL_A(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB)
    IMPLICIT NONE
    INTEGER ::         THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
    END SUBROUTINE DMSM_ALL_A

    SUBROUTINE    DMSM_ALL_I(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION)
    IMPLICIT NONE
    INTEGER ::         THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
    END SUBROUTINE DMSM_ALL_I

    SUBROUTINE    DMSM_ALL_S(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
    IMPLICIT NONE
    INTEGER ::         THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
    LOGICAL ::         RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
    END SUBROUTINE DMSM_ALL_S

    SUBROUTINE    DMSM_ALL_Y(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
    IMPLICIT NONE
    INTEGER ::         THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
    LOGICAL ::         RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
    END SUBROUTINE DMSM_ALL_Y

  END INTERFACE	DMSM_ALL
END MODULE DMSM_ALL_INTERFACE


MODULE DMSM_SET_COMM_CORE_INTERFACE
  INTERFACE
    SUBROUTINE DMSM_SET_COMM_CORE(AN_MPI_COMMUNICATOR)
      IMPLICIT NONE
      INTEGER :: AN_MPI_COMMUNICATOR
    END SUBROUTINE DMSM_SET_COMM_CORE
  END INTERFACE
END MODULE DMSM_SET_COMM_CORE_INTERFACE


MODULE DMSM_SET_COMM_INTERFACE
  INTERFACE DMSM_SET_COMM

    SUBROUTINE DMSM_SET_COMM_01(AN_MPI_COMMUNICATOR)
      IMPLICIT NONE
      INTEGER :: AN_MPI_COMMUNICATOR
    END SUBROUTINE DMSM_SET_COMM_01

    SUBROUTINE DMSM_SET_COMM_10(TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                         ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                         , DIST_COMM, DIST_RANK, DIST_PROCESSES )
      IMPLICIT NONE
      INTEGER ::                TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                         ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                         , DIST_COMM, DIST_RANK, DIST_PROCESSES
    END SUBROUTINE DMSM_SET_COMM_10

  END INTERFACE DMSM_SET_COMM
END MODULE DMSM_SET_COMM_INTERFACE


MODULE DMSM_MPI_ONLY_CORE_INTERFACE
  INTERFACE
    SUBROUTINE DMSM_MPI_ONLY_CORE(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
      IMPLICIT NONE
      INTEGER ::           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      LOGICAL ::           RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
    END SUBROUTINE DMSM_MPI_ONLY_CORE
  END INTERFACE
END MODULE DMSM_MPI_ONLY_CORE_INTERFACE


MODULE DMSM_MPI_ALL_INTERFACE
  INTERFACE DMSM_MPI_ALL

    SUBROUTINE DMSM_MPI_ONLY_A(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB)
      IMPLICIT NONE
      INTEGER ::       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
    END SUBROUTINE DMSM_MPI_ONLY_A

    SUBROUTINE DMSM_MPI_ONLY_I(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION)
      IMPLICIT NONE
      INTEGER ::       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
    END SUBROUTINE DMSM_MPI_ONLY_I

    SUBROUTINE DMSM_MPI_ONLY_S(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
      IMPLICIT NONE
      INTEGER ::       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      LOGICAL ::       RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
    END SUBROUTINE DMSM_MPI_ONLY_S

    SUBROUTINE DMSM_MPI_ONLY_Y(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
      IMPLICIT NONE
      INTEGER ::       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      LOGICAL ::       RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
    END SUBROUTINE DMSM_MPI_ONLY_Y

  END INTERFACE DMSM_MPI_ALL
END MODULE DMSM_MPI_ALL_INTERFACE


MODULE DMSM_MODULE
  USE DMSM_ESSENTIAL_CONSTANTS
  USE DMSM_INITIALIZE_INTERFACE
  USE DMSM_WORKING_INTERFACE
  USE DMSM_ALL_INTERFACE
  USE DMSM_MPI_ALL_INTERFACE
  USE DMSM_SET_COMM_INTERFACE
  INTERFACE
    SUBROUTINE DMSM_SET_INITIAL_LOCK()
    END SUBROUTINE DMSM_SET_INITIAL_LOCK
    SUBROUTINE DMSM_UNSET_AN_INITIAL_LOCK()
    END SUBROUTINE DMSM_UNSET_AN_INITIAL_LOCK
    SUBROUTINE DMSM_WAIT_FOR_INITIAL_LOCKS()
    END SUBROUTINE DMSM_WAIT_FOR_INITIAL_LOCKS
    SUBROUTINE DMSM_SET_JOB_SIGNALS(GROUP_NUMBER)
      IMPLICIT NONE
      INTEGER:: GROUP_NUMBER
    END SUBROUTINE DMSM_SET_JOB_SIGNALS
    SUBROUTINE DMSM_UNSET_A_JOB_SIGNAL(JOB_NUMBER)
      IMPLICIT NONE
      INTEGER:: JOB_NUMBER
    END SUBROUTINE DMSM_UNSET_A_JOB_SIGNAL
    SUBROUTINE DMSM_WAIT_FOR_JOB_SIGNALS()
    END SUBROUTINE DMSM_WAIT_FOR_JOB_SIGNALS
    SUBROUTINE DMSM_SET_NODE_RESULT_LOCK()
    END SUBROUTINE DMSM_SET_NODE_RESULT_LOCK
    SUBROUTINE DMSM_UNSET_NODE_RESULT_LOCK()
    END SUBROUTINE DMSM_UNSET_NODE_RESULT_LOCK
    !SUBROUTINE DMSM_SET_FINAL_RESULT_LOCK()
    !END SUBROUTINE DMSM_SET_FINAL_RESULT_LOCK
    !SUBROUTINE DMSM_UNSET_FINAL_RESULT_LOCK()
    !END SUBROUTINE DMSM_UNSET_FINAL_RESULT_LOCK
    LOGICAL FUNCTION DMSM_ALL_JOBS_DONE()
    END FUNCTION DMSM_ALL_JOBS_DONE
    SUBROUTINE     DMSM_JOB_DISTRIBUTION_CHECKUP()
    END SUBROUTINE DMSM_JOB_DISTRIBUTION_CHECKUP
    SUBROUTINE     DMSM_FINALIZE()
    END SUBROUTINE DMSM_FINALIZE
    INTEGER FUNCTION DMSM_GET_DISTRIBUTION_PLAN()
    END FUNCTION DMSM_GET_DISTRIBUTION_PLAN
    INTEGER FUNCTION DMSM_GET_TAG()
    END FUNCTION DMSM_GET_TAG
    INTEGER FUNCTION DMSM_GET_GROUP_START(JOB_NUMBER)
      IMPLICIT NONE
      INTEGER:: JOB_NUMBER
    END FUNCTION DMSM_GET_GROUP_START
    INTEGER FUNCTION DMSM_GET_GROUP_END(JOB_NUMBER)
      IMPLICIT NONE
      INTEGER:: JOB_NUMBER
    END FUNCTION DMSM_GET_GROUP_END
    INTEGER FUNCTION DMSM_GET_GROUP_NUMBER(JOB_NUMBER)
      IMPLICIT NONE
      INTEGER:: JOB_NUMBER
    END FUNCTION DMSM_GET_GROUP_NUMBER

    INTEGER FUNCTION DMSM_GET_JOB_COMM()
    END FUNCTION DMSM_GET_JOB_COMM
    INTEGER FUNCTION DMSM_GET_JOB_RANK()
    END FUNCTION DMSM_GET_JOB_RANK
    INTEGER FUNCTION DMSM_GET_JOB_PROCS()
    END FUNCTION DMSM_GET_JOB_PROCS
    INTEGER FUNCTION DMSM_GET_DIST_COMM()
    END FUNCTION DMSM_GET_DIST_COMM
    INTEGER FUNCTION DMSM_GET_DIST_RANK()
    END FUNCTION DMSM_GET_DIST_RANK
    INTEGER FUNCTION DMSM_GET_DIST_PROCS()
    END FUNCTION DMSM_GET_DIST_PROCS

    SUBROUTINE DMSM_GEN_COMM_MPI_ALL(TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                         ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                         , DIST_COMM, DIST_RANK, DIST_PROCESSES )
      IMPLICIT NONE
      INTEGER ::                TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                         ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                         , DIST_COMM, DIST_RANK, DIST_PROCESSES
    END SUBROUTINE DMSM_GEN_COMM_MPI_ALL

    INTEGER FUNCTION DMSM_GET_MASTER()
      IMPLICIT NONE
    END FUNCTION DMSM_GET_MASTER

    SUBROUTINE DMSM_SET_MASTER(M)
      IMPLICIT NONE
      INTEGER ::  M
    END SUBROUTINE DMSM_SET_MASTER

    SUBROUTINE DMSM_SET_JOURNAL_NUMBER(JN)
      IMPLICIT NONE
      INTEGER :: JN
    END SUBROUTINE DMSM_SET_JOURNAL_NUMBER

  END INTERFACE
END MODULE DMSM_MODULE


MODULE DMSM_DO_NOTHING_INTERFACE
  INTERFACE
    SUBROUTINE DMSM_DO_NOTHING_2(I,J)
    IMPLICIT NONE
    INTEGER :: I,J
    END SUBROUTINE DMSM_DO_NOTHING_2

    SUBROUTINE DMSM_DO_NOTHING_3(I,J,K)
    IMPLICIT NONE
    INTEGER :: I,J,K
    END SUBROUTINE DMSM_DO_NOTHING_3

    SUBROUTINE DMSM_DO_NOTHING_4(I,J,K,L)
    IMPLICIT NONE
    INTEGER :: I,J,K,L
    END SUBROUTINE DMSM_DO_NOTHING_4
  END INTERFACE
END MODULE DMSM_DO_NOTHING_INTERFACE


MODULE DMSM_GET_CPU_TIME_INTERFACE
  INTERFACE
    SUBROUTINE DMSM_GET_CPU_TIME(A)
    IMPLICIT NONE
#ifdef DMSM_FORTRAN_RS16_TIMER
    REAL*16 :: A
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
    REAL*8  :: A
#else
    REAL*8  :: A
#endif
#endif
    END SUBROUTINE DMSM_GET_CPU_TIME
  END INTERFACE
END MODULE DMSM_GET_CPU_TIME_INTERFACE


MODULE DMSM_MODULE_NOT_FOR_USER
  USE                       DMSM_FEATURE_CONSTANTS
  USE                       DMSM_ESSENTIAL_CONSTANTS
  USE                       OMP_LIB
  INCLUDE                   'mpif.h'
  INTEGER, PARAMETER     :: DMSM_IDLING_CONSTANT=-103
  INTEGER, PARAMETER     :: DMSM_JOB_RECORD_LENTH=3
  INTEGER                :: DMSM_MASTER=DMSM_ALWAYS_ZERO
  INTEGER                :: DMSM_JOB_MASTER=DMSM_ALWAYS_ZERO
  INTEGER                :: DMSM_JOB_GROUP_COUNTER=0
  INTEGER                :: DMSM_IDLING_SIGNAL
  INTEGER                :: DMSM_DISTRIBUTION_PLAN=-1
  INTEGER                :: DMSM_TOTAL_JOBS
  INTEGER                :: DMSM_TOTAL_JOB_GROUPS
  INTEGER                :: DMSM_NUM_OF_JOBS_PER_GROUP
  INTEGER                :: DMSM_END_OF_GROUP_JOBS
  INTEGER                :: DMSM_PREALLOCATION_ENABLED=1
  INTEGER                :: DMSM_FIRST_GROUPS_ALLOCATED=0
  INTEGER                :: DMSM_FIRST_JOBS_ALLOCATED=0
  INTEGER                :: DMSM_CURRENT_GROUP_NUMBER=0
  INTEGER, ALLOCATABLE   :: DMSM_JOB_I_DID(:,:)
  INTEGER, ALLOCATABLE   :: DMSM_SUM_JOB_I_DID(:,:)
  INTEGER, ALLOCATABLE   :: DMSM_JOBS_SENT_BY_MASTER(:)
  INTEGER, ALLOCATABLE   :: DMSM_JOBS_TO_THREAD_BY_MASTER(:)
  INTEGER, ALLOCATABLE   :: DMSM_NO_MORE_WORK_INSTRUCTED(:,:)
  INTEGER, ALLOCATABLE   :: DMSM_NO_MORE_GROUPS_FROM(:)
  INTEGER, ALLOCATABLE   :: DMSM_NO_MORE_GROUPS_TO(:)
  INTEGER, ALLOCATABLE   :: DMSM_NO_MORE_WORK_RECEIVED(:,:)
  INTEGER, ALLOCATABLE   :: DMSM_SUM_NO_MORE_WORK_RECEIVED(:,:)
  INTEGER, ALLOCATABLE   :: DMSM_JOB_GROUP_INFORMATION(:,:)
  INTEGER                :: DMSM_THREADS_PP_EXPECTED=0
  INTEGER, ALLOCATABLE   :: DMSM_THREADS_PER_PROCESS_GOT(:)
  INTEGER                :: DMSM_MY_MPI_RANK, DMSM_TOTAL_MPI_PROCESSES, DMSM_IERR
  INTEGER                :: DMSM_A_FILE_TUNNEL
  CHARACTER(*),PARAMETER :: DMSM_JOURNAL_FILE_HEAD='DMSM_journal'
  CHARACTER(*),PARAMETER :: DMSM_PROBLEM_FILE_HEAD='DMSM_problem'
  INTEGER                :: DMSM_JOURNAL_NUMBER=0
  INTEGER                :: DMSM_MY_THREAD_NUMBER
  INTEGER                :: DMSM_JOB_ASSIGNED, DMSM_JOB_FROM, DMSM_JOB_TO, DMSM_MY_JOB, DMSM_A_TAG_FOR_MPI
!$OMP THREADPRIVATE(DMSM_MY_THREAD_NUMBER, DMSM_MY_JOB, DMSM_A_TAG_FOR_MPI)
#ifdef DMSM_FORTRAN_RS16_TIMER
  REAL*16                :: DMSM_WALL_BEGINNING_TIME, DMSM_WALL_ENDDING_TIME
  REAL*16                :: DMSM_TOTAL_JOB_CPU_TIME
  REAL*16                :: DMSM_BEGINNING_TIME, DMSM_ENDDING_TIME
  REAL*16, ALLOCATABLE   :: DMSM_CPU_TIME_OF_JOBS(:)
  REAL*16, ALLOCATABLE   :: DMSM_CPU_TIME_OF_ALL_JOBS(:)
  REAL*16, ALLOCATABLE   :: DMSM_CPU_TIME_OF_THREADS(:)
  REAL*16, ALLOCATABLE   :: DMSM_CPU_TIME_OF_ALL(:,:)
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
  REAL*8                 :: DMSM_WALL_BEGINNING_TIME, DMSM_WALL_ENDDING_TIME
  REAL*8                 :: DMSM_TOTAL_JOB_CPU_TIME
  REAL*8                 :: DMSM_BEGINNING_TIME, DMSM_ENDDING_TIME
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_JOBS(:)
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_ALL_JOBS(:)
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_THREADS(:)
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_ALL(:,:)
#else
  REAL*8                 :: DMSM_WALL_BEGINNING_TIME, DMSM_WALL_ENDDING_TIME
  REAL*8                 :: DMSM_TOTAL_JOB_CPU_TIME
  REAL*8                 :: DMSM_BEGINNING_TIME, DMSM_ENDDING_TIME
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_JOBS(:)
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_ALL_JOBS(:)
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_THREADS(:)
  REAL*8,  ALLOCATABLE   :: DMSM_CPU_TIME_OF_ALL(:,:)
#endif
#endif
  INTEGER                :: DMSM_JOBS_COMPLETED
!$OMP THREADPRIVATE(DMSM_BEGINNING_TIME, DMSM_ENDDING_TIME, DMSM_JOBS_COMPLETED)
  INTEGER, ALLOCATABLE   :: DMSM_JOBS_COMPLETED_BY_THREADS(:)
  INTEGER, ALLOCATABLE   :: DMSM_JOBS_COMPLETED_ALL(:,:)
  INTEGER, ALLOCATABLE   :: DMSM_INTEGER_SIGNALS_BIG(:)
  INTEGER                :: DMSM_TOTAL_JOB_SIGNALS_BIG
  INTEGER, ALLOCATABLE   :: DMSM_INTEGER_SIGNALS_SMALL(:)

  INTEGER                :: DMSM_COMMUNICATOR
  LOGICAL                :: DMSM_SET_COMM_CALLED=.FALSE.
  LOGICAL                :: DMSM_INITIAL_DATA_PREP_ENABLED=.FALSE.
  LOGICAL                :: DMSM_RESULT_COLLECTION_ENABLED=.FALSE.
  LOGICAL                :: DMSM_ALREADY_INITIALIZED=.FALSE.
  INTEGER, ALLOCATABLE   :: DMSM_NUM_OF_TALKS_WITH_PROCESS(:)
  INTEGER                :: DMSM_NUM_OF_TALKS_WITH_MASTER=0
  LOGICAL                :: DMSM_ALL_JOBS_COMPLETED=.FALSE.
  INTEGER(KIND=OMP_LOCK_KIND) :: DMSM_JOB_NODE_RESULT_LOCK
  INTEGER(KIND=OMP_LOCK_KIND) :: DMSM_JOB_FINAL_RESULT_LOCK
  INTEGER(KIND=OMP_LOCK_KIND), ALLOCATABLE :: DMSM_JOB_INITIAL_DATA_LOCKS(:)

  INTEGER                :: DMSM_TOTAL_SLAVE_SHOULD_HOME=0
  INTEGER                :: DMSM_TOTAL_SLAVE_GONE_HOME=0
  INTEGER                :: DMSM_TOTAL_M_THREAD_GONE_HOME=0
  INTEGER                :: DMSM_TOTAL_M_THREAD_SHOULD_HM=0
  INTEGER, PARAMETER     :: DMSM_JOB_RANGE_SIZE=3
  INTEGER, PARAMETER     :: DMSM_DECIDED_TO_EXIT=1
  INTEGER                :: DMSM_JOB_RANGE(DMSM_JOB_RANGE_SIZE)

  LOGICAL                :: DMSM_ONLY_MPI_ENABLED=.FALSE.
  LOGICAL                :: DMSM_SET_COMM_10_CALLED=.FALSE.
  INTEGER                :: DMSM_ONLY_MPI_ALL_COMM
  INTEGER                :: DMSM_ONLY_MPI_JOB_COMM
  INTEGER                :: DMSM_ONLY_MPI_DIST_COMM
  INTEGER                :: DMSM_ONLY_MPI_ALL_RANK, DMSM_ONLY_MPI_ALL_PROCESSES
  INTEGER                :: DMSM_ONLY_MPI_JOB_RANK, DMSM_ONLY_MPI_JOB_PROCESSES
  INTEGER                :: DMSM_ONLY_MPI_DIST_RANK,DMSM_ONLY_MPI_DIST_PROCESSES
  INTEGER, ALLOCATABLE   :: DMSM_ONLY_MPI_JOB_PROCESSES_T(:)
  INTEGER                :: DMSM_ONLY_MPI_JOB_PROCESSES_E
  INTEGER                :: DMSM_ONLY_MPI_NO_MORE_JOBS=-1


  INTERFACE
    SUBROUTINE DMSM_CPU_TIME_IN_OPENMP(A)
      IMPLICIT NONE
#ifdef DMSM_FORTRAN_RS16_TIMER
         REAL*16 :: A
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
         REAL*8  :: A
#else
         REAL*8  :: A
#endif
#endif
    END SUBROUTINE DMSM_CPU_TIME_IN_OPENMP
  END INTERFACE


  INTERFACE DMSM_ERROR_AND_STOP
    SUBROUTINE DMSM_ERROR_AND_STOP1(A1,I1)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1
      INTEGER           :: I1
    END SUBROUTINE DMSM_ERROR_AND_STOP1

    SUBROUTINE DMSM_ERROR_AND_STOP2(A1,I1,A2,I2)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2
      INTEGER           :: I1,I2
    END SUBROUTINE DMSM_ERROR_AND_STOP2

    SUBROUTINE DMSM_ERROR_AND_STOP3(A1,I1,A2,I2,A3,I3)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2,A3
      INTEGER           :: I1,I2,I3
    END SUBROUTINE DMSM_ERROR_AND_STOP3

    SUBROUTINE DMSM_ERROR_AND_STOP4(A1,I1,A2,I2,A3,I3,A4,I4)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2,A3,A4
      INTEGER           :: I1,I2,I3,I4
    END SUBROUTINE DMSM_ERROR_AND_STOP4

    SUBROUTINE DMSM_ERROR_AND_STOP5(A1,I1,A2,I2,A3,I3,A4,I4,A5,I5)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2,A3,A4,A5
      INTEGER           :: I1,I2,I3,I4,I5
    END SUBROUTINE DMSM_ERROR_AND_STOP5

  END INTERFACE


  INTERFACE
     FUNCTION DMSM_INTEGER_TO_CHARACTER(I,LENGTH)
        IMPLICIT NONE
        CHARACTER*20 :: DMSM_INTEGER_TO_CHARACTER
        INTEGER :: I,LENGTH,J,K,L
     END FUNCTION DMSM_INTEGER_TO_CHARACTER
  END INTERFACE


END MODULE DMSM_MODULE_NOT_FOR_USER


MODULE DMSM_ERROR_NOT_STOP_INTERF
  INTERFACE DMSM_ERROR_NOT_STOP
    SUBROUTINE DMSM_ERROR_NOT_STOP1(A1,I1)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1
      INTEGER           :: I1
    END SUBROUTINE DMSM_ERROR_NOT_STOP1

    SUBROUTINE DMSM_ERROR_NOT_STOP2(A1,I1,A2,I2)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2
      INTEGER           :: I1,I2
    END SUBROUTINE DMSM_ERROR_NOT_STOP2

    SUBROUTINE DMSM_ERROR_NOT_STOP3(A1,I1,A2,I2,A3,I3)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2,A3
      INTEGER           :: I1,I2,I3
    END SUBROUTINE DMSM_ERROR_NOT_STOP3

    SUBROUTINE DMSM_ERROR_NOT_STOP4(A1,I1,A2,I2,A3,I3,A4,I4)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2,A3,A4
      INTEGER           :: I1,I2,I3,I4
    END SUBROUTINE DMSM_ERROR_NOT_STOP4

    SUBROUTINE DMSM_ERROR_NOT_STOP5(A1,I1,A2,I2,A3,I3,A4,I4,A5,I5)
      IMPLICIT NONE
      CHARACTER (LEN=*) :: A1,A2,A3,A4,A5
      INTEGER           :: I1,I2,I3,I4,I5
    END SUBROUTINE DMSM_ERROR_NOT_STOP5

END INTERFACE
END MODULE DMSM_ERROR_NOT_STOP_INTERF


MODULE DMSM_RE_COLL_LOCKED_INTERFACE
    INTERFACE
       SUBROUTINE DMSM_RESULT_COLLECTION_LOCKED(MY_RANK,FROM,RESULT_COLLECTION)
         IMPLICIT NONE
         INTEGER :: MY_RANK,FROM
         INTERFACE
            SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
              IMPLICIT NONE
              INTEGER :: DUMMY_J,DUMMY_K
            END SUBROUTINE RESULT_COLLECTION
         END INTERFACE
       END SUBROUTINE DMSM_RESULT_COLLECTION_LOCKED
    END INTERFACE
END MODULE DMSM_RE_COLL_LOCKED_INTERFACE


MODULE DMSM_LAST_RE_COLL_INTERFACE
    INTERFACE
       SUBROUTINE DMSM_LAST_RESULT_COLLECTION(RESULT_COLLECTION)
         IMPLICIT NONE
         INTERFACE
            SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
              IMPLICIT NONE
              INTEGER :: DUMMY_J,DUMMY_K
            END SUBROUTINE RESULT_COLLECTION
         END INTERFACE
       END SUBROUTINE DMSM_LAST_RESULT_COLLECTION
    END INTERFACE
END MODULE DMSM_LAST_RE_COLL_INTERFACE


MODULE DMSM_LEVEL_T_INTERFACE
  INTERFACE
    SUBROUTINE DMSM_DO_THE_SERIAL_JOB_WRAPER(DO_THE_JOB)
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
    END SUBROUTINE DMSM_DO_THE_SERIAL_JOB_WRAPER
  END INTERFACE

  INTERFACE
    SUBROUTINE DMSM_DO_THE_JOB_WRAPER(DO_THE_JOB)
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
    END SUBROUTINE DMSM_DO_THE_JOB_WRAPER
  END INTERFACE

  INTERFACE
     SUBROUTINE DMSM_GROUP_PREPARE_WRAPER(FROM,TO,DESTINATION,JOB_GROUP_PREPARATION,RESULT_COLLECTION)
        USE DMSM_MODULE_NOT_FOR_USER
        IMPLICIT NONE
        INTEGER :: FROM,TO,DESTINATION
        INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
        END INTERFACE
        INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
        END INTERFACE
     END SUBROUTINE DMSM_GROUP_PREPARE_WRAPER
  END INTERFACE
END MODULE DMSM_LEVEL_T_INTERFACE


MODULE DMSM_LEVEL_S_INTERFACE
  INTERFACE
  SUBROUTINE DMSM_DO_THE_JOB_GROUP(GROUP, DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT   NONE
  INTEGER :: GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_DO_THE_JOB_GROUP
  END INTERFACE
END MODULE DMSM_LEVEL_S_INTERFACE


MODULE DMSM_LEVEL_N_INTERFACE
  USE DMSM_LEVEL_T_INTERFACE
  INTERFACE
  SUBROUTINE DMSM_MASTER_RECORDS(SC, TD, DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER   :: SC, TD
  END SUBROUTINE DMSM_MASTER_RECORDS

  SUBROUTINE DMSM_SLAVE_APPLYING(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
      IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_SLAVE_APPLYING
  END INTERFACE
END MODULE DMSM_LEVEL_N_INTERFACE


MODULE DMSM_LEVEL_M_INTERFACE
  USE DMSM_LEVEL_N_INTERFACE
  INTERFACE
    SUBROUTINE DMSM_MASTER_ASSIGN_SLAVE(SC, TD, TG,DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
    IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
      INTEGER:: SC, TD, TG
    END SUBROUTINE DMSM_MASTER_ASSIGN_SLAVE
  END INTERFACE
END MODULE DMSM_LEVEL_M_INTERFACE


MODULE DMSM_LEVEL_G_INTERFACE
  INTERFACE
  SUBROUTINE DMSM_SLAVE_OUTSIDE_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_N_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_SLAVE_OUTSIDE_OPENMP

  SUBROUTINE DMSM_SERIAL_CORE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT   NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_SERIAL_CORE

  SUBROUTINE DMSM_MASTER_THREADS_ALLOCATE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT   NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MASTER_THREADS_ALLOCATE

  SUBROUTINE DMSM_SLAVES_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_SLAVES_PURE_MPI

  SUBROUTINE DMSM_ONE_SLAVE_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_ONE_SLAVE_IN_OPENMP

  SUBROUTINE DMSM_MASTER_THREADS_DYNAMIC(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MASTER_THREADS_DYNAMIC

  SUBROUTINE DMSM_MASTER_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_N_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MASTER_PURE_MPI

  SUBROUTINE DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_N_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MASTER_ALL

  SUBROUTINE DMSM_ALL_SLAVES_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_ALL_SLAVES_IN_OPENMP

  END INTERFACE
END MODULE DMSM_LEVEL_G_INTERFACE


MODULE DMSM_LEVEL_F_INTERFACE
  INTERFACE
  END INTERFACE
END MODULE DMSM_LEVEL_F_INTERFACE


MODULE DMSM_LEVEL_E_INTERFACE
  INTERFACE

  SUBROUTINE DMSM_SERIAL_VERSION(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_SERIAL_VERSION

  SUBROUTINE DMSM_PURE_OPENMP_MODEL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_PURE_OPENMP_MODEL

  SUBROUTINE DMSM_PURE_MPI_MODEL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_PURE_MPI_MODEL

  SUBROUTINE DMSM_MODEL_PLAN11(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN11

  SUBROUTINE DMSM_MODEL_PLAN12(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN12

  SUBROUTINE DMSM_MODEL_PLAN13(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN13

  SUBROUTINE DMSM_MODEL_PLAN21(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN21

  SUBROUTINE DMSM_MODEL_PLAN22(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN22

  SUBROUTINE DMSM_MODEL_PLAN23(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN23

  SUBROUTINE DMSM_MODEL_PLAN31(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN31

  SUBROUTINE DMSM_MODEL_PLAN32(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN32

  SUBROUTINE DMSM_MODEL_PLAN33(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  END SUBROUTINE DMSM_MODEL_PLAN33

  END INTERFACE
END MODULE DMSM_LEVEL_E_INTERFACE


MODULE DMSM_LEVEL_D_INTERFACE
  INTERFACE
  END INTERFACE
END MODULE DMSM_LEVEL_D_INTERFACE




SUBROUTINE DMSM_GET_CPU_TIME(A)
#ifdef DMSM_FORTRAN_RS16_TIMER
  IMPLICIT NONE
  REAL*16 :: A
  CALL CPU_TIME(A)
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
  IMPLICIT NONE
  REAL*8  :: A
  CALL CPU_TIME(A)
#else
  USE DMSM_CTIMER_INTERFACE
  IMPLICIT NONE
  REAL*8  :: A
  A=DMSM_C_TIMER()
#endif
#endif
  RETURN
END SUBROUTINE DMSM_GET_CPU_TIME




SUBROUTINE DMSM_CPU_TIME_IN_OPENMP(A)
  USE DMSM_GET_CPU_TIME_INTERFACE
  IMPLICIT NONE
#ifdef DMSM_FORTRAN_RS16_TIMER
    REAL*16 :: A
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
    REAL*8  :: A
#else
    REAL*8  :: A
#endif
#endif
  !$OMP CRITICAL(DMSM_TM)
  CALL DMSM_GET_CPU_TIME(A)
  !$OMP END CRITICAL(DMSM_TM)
  RETURN
END SUBROUTINE DMSM_CPU_TIME_IN_OPENMP




SUBROUTINE DMSM_ERROR_AND_STOP1(A1,I1)
  IMPLICIT NONE
  INTEGER           :: DMSM_IERR
  CHARACTER (LEN=*) :: A1
  INTEGER           :: I1
  WRITE(*,*) 'ERROR AND STOP!'
  WRITE(*,*) A1,I1
  CALL  DMSM_FINALIZE
  CALL  MPI_FINALIZE(DMSM_IERR)
  STOP
  RETURN
END SUBROUTINE DMSM_ERROR_AND_STOP1




SUBROUTINE DMSM_ERROR_AND_STOP2(A1,I1,A2,I2)
  IMPLICIT NONE
  INTEGER           :: DMSM_IERR
  CHARACTER (LEN=*) :: A1,A2
  INTEGER           :: I1,I2
  WRITE(*,*) 'ERROR AND STOP!'
  WRITE(*,*) A1,I1,A2,I2
  CALL  DMSM_FINALIZE
  CALL  MPI_FINALIZE(DMSM_IERR)
  STOP
  RETURN
END SUBROUTINE DMSM_ERROR_AND_STOP2




SUBROUTINE DMSM_ERROR_AND_STOP3(A1,I1,A2,I2,A3,I3)
  IMPLICIT NONE
  INTEGER           :: DMSM_IERR
  CHARACTER (LEN=*) :: A1,A2,A3
  INTEGER           :: I1,I2,I3
  WRITE(*,*) 'ERROR AND STOP!'
  WRITE(*,*) A1,I1,A2,I2,A3,I3
  CALL  DMSM_FINALIZE
  CALL  MPI_FINALIZE(DMSM_IERR)
  STOP
  RETURN
END SUBROUTINE DMSM_ERROR_AND_STOP3




SUBROUTINE DMSM_ERROR_AND_STOP4(A1,I1,A2,I2,A3,I3,A4,I4)
  IMPLICIT NONE
  INTEGER           :: DMSM_IERR
  CHARACTER (LEN=*) :: A1,A2,A3,A4
  INTEGER           :: I1,I2,I3,I4
  WRITE(*,*) 'ERROR AND STOP!'
  WRITE(*,*) A1,I1,A2,I2,A3,I3,A4,I4
  CALL  DMSM_FINALIZE
  CALL  MPI_FINALIZE(DMSM_IERR)
  STOP
  RETURN
END SUBROUTINE DMSM_ERROR_AND_STOP4




SUBROUTINE DMSM_ERROR_AND_STOP5(A1,I1,A2,I2,A3,I3,A4,I4,A5,I5)
  IMPLICIT NONE
  INTEGER           :: DMSM_IERR
  CHARACTER (LEN=*) :: A1,A2,A3,A4,A5
  INTEGER           :: I1,I2,I3,I4,I5
  WRITE(*,*) 'ERROR AND STOP!'
  WRITE(*,*) A1,I1,A2,I2,A3,I3,A4,I4,A5,I5
  CALL  DMSM_FINALIZE
  CALL  MPI_FINALIZE(DMSM_IERR)
  STOP
  RETURN
END SUBROUTINE DMSM_ERROR_AND_STOP5




SUBROUTINE DMSM_ERROR_NOT_STOP1(A1,I1)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  CHARACTER (LEN=*) :: A1
  INTEGER           :: I1
  WRITE(*,*) 'ERROR BUT NOT STOP.!'
  WRITE(*,*) A1,I1
  IF (DMSM_MY_MPI_RANK .EQ. DMSM_MASTER)   THEN
      CALL DMSM_OPEN_PROBLEM_FILE()
      WRITE(DMSM_A_FILE_TUNNEL,*)' 1 '
      WRITE(DMSM_A_FILE_TUNNEL,*)'ERROR BUT NOT STOP.!'
      WRITE(DMSM_A_FILE_TUNNEL,*)A1,I1
      CLOSE(DMSM_A_FILE_TUNNEL)
  END IF
  IF(DMSM_DECIDED_TO_EXIT.EQ.1) THEN
     CALL  DMSM_FINALIZE
     CALL  MPI_FINALIZE(DMSM_IERR)
     STOP
  END IF
  RETURN
END SUBROUTINE DMSM_ERROR_NOT_STOP1




SUBROUTINE DMSM_ERROR_NOT_STOP2(A1,I1,A2,I2)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  CHARACTER (LEN=*) :: A1,A2
  INTEGER           :: I1,I2
  WRITE(*,*) 'ERROR BUT NOT STOP.!'
  WRITE(*,*) A1,I1,A2,I2
  IF (DMSM_MY_MPI_RANK .EQ. DMSM_MASTER)   THEN
      CALL DMSM_OPEN_PROBLEM_FILE()
      WRITE(DMSM_A_FILE_TUNNEL,*)' 1 '
      WRITE(DMSM_A_FILE_TUNNEL,*)'ERROR BUT NOT STOP.!'
      WRITE(DMSM_A_FILE_TUNNEL,*)A1,I1,A2,I2
      CLOSE(DMSM_A_FILE_TUNNEL)
  END IF
  IF(DMSM_DECIDED_TO_EXIT.EQ.1) THEN
     CALL  DMSM_FINALIZE
     CALL  MPI_FINALIZE(DMSM_IERR)
     STOP
  END IF
  RETURN
END SUBROUTINE DMSM_ERROR_NOT_STOP2




SUBROUTINE DMSM_ERROR_NOT_STOP3(A1,I1,A2,I2,A3,I3)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  CHARACTER (LEN=*) :: A1,A2,A3
  INTEGER           :: I1,I2,I3
  WRITE(*,*) 'ERROR BUT NOT STOP.!'
  WRITE(*,*) A1,I1,A2,I2,A3,I3
  IF (DMSM_MY_MPI_RANK .EQ. DMSM_MASTER)   THEN
      CALL DMSM_OPEN_PROBLEM_FILE()
      WRITE(DMSM_A_FILE_TUNNEL,*)' 1 '
      WRITE(DMSM_A_FILE_TUNNEL,*)'ERROR BUT NOT STOP.!'
      WRITE(DMSM_A_FILE_TUNNEL,*)A1,I1,A2,I2,A3,I3
      CLOSE(DMSM_A_FILE_TUNNEL)
  END IF
  IF(DMSM_DECIDED_TO_EXIT.EQ.1) THEN
     CALL  DMSM_FINALIZE
     CALL  MPI_FINALIZE(DMSM_IERR)
     STOP
  END IF
  RETURN
END SUBROUTINE DMSM_ERROR_NOT_STOP3




SUBROUTINE DMSM_ERROR_NOT_STOP4(A1,I1,A2,I2,A3,I3,A4,I4)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  CHARACTER (LEN=*) :: A1,A2,A3,A4
  INTEGER           :: I1,I2,I3,I4
  WRITE(*,*) 'ERROR BUT NOT STOP.!'
  WRITE(*,*) A1,I1,A2,I2,A3,I3,A4,I4
  IF (DMSM_MY_MPI_RANK .EQ. DMSM_MASTER)   THEN
      CALL DMSM_OPEN_PROBLEM_FILE()
      WRITE(DMSM_A_FILE_TUNNEL,*)' 1 '
      WRITE(DMSM_A_FILE_TUNNEL,*)'ERROR BUT NOT STOP.!'
      WRITE(DMSM_A_FILE_TUNNEL,*)A1,I1,A2,I2,A3,I3,A4,I4
      CLOSE(DMSM_A_FILE_TUNNEL)
  END IF
  IF(DMSM_DECIDED_TO_EXIT.EQ.1) THEN
     CALL  DMSM_FINALIZE
     CALL  MPI_FINALIZE(DMSM_IERR)
     STOP
  END IF
  RETURN
END SUBROUTINE DMSM_ERROR_NOT_STOP4




SUBROUTINE DMSM_ERROR_NOT_STOP5(A1,I1,A2,I2,A3,I3,A4,I4,A5,I5)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  CHARACTER (LEN=*) :: A1,A2,A3,A4,A5
  INTEGER           :: I1,I2,I3,I4,I5
  WRITE(*,*) 'ERROR BUT NOT STOP.!'
  WRITE(*,*) A1,I1,A2,I2,A3,I3,A4,I4,A5,I5
  IF (DMSM_MY_MPI_RANK .EQ. DMSM_MASTER)   THEN
      CALL DMSM_OPEN_PROBLEM_FILE()
      WRITE(DMSM_A_FILE_TUNNEL,*)' 1 '
      WRITE(DMSM_A_FILE_TUNNEL,*)'ERROR BUT NOT STOP.!'
      WRITE(DMSM_A_FILE_TUNNEL,*)A1,I1,A2,I2,A3,I3,A4,I4,A5,I5
      CLOSE(DMSM_A_FILE_TUNNEL)
  END IF
  IF(DMSM_DECIDED_TO_EXIT.EQ.1) THEN
     CALL  DMSM_FINALIZE
     CALL  MPI_FINALIZE(DMSM_IERR)
     STOP
  END IF
  RETURN
END SUBROUTINE DMSM_ERROR_NOT_STOP5




SUBROUTINE DMSM_SLAVE_OUTSIDE_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_RE_COLL_LOCKED_INTERFACE
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_S_INTERFACE
  USE DMSM_LEVEL_N_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER   :: K, MPISTTS(MPI_STATUS_SIZE)
  GROUP_ASSIGNING: DO
       CALL DMSM_SLAVE_APPLYING(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
       IF(DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1).LE.0) THEN
          CALL DMSM_DO_THE_JOB_GROUP(K, DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
       ELSE
          EXIT GROUP_ASSIGNING
       END IF
  END DO GROUP_ASSIGNING
  RETURN
END SUBROUTINE DMSM_SLAVE_OUTSIDE_OPENMP




SUBROUTINE DMSM_DO_THE_JOB_GROUP(GROUP, DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_T_INTERFACE
  IMPLICIT   NONE
  INTEGER :: GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  !$OMP PARALLEL DEFAULT(SHARED)
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
  IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_DO_THE_JOB_GROUP) ", DMSM_MY_MPI_RANK)
  END IF
  END IF
  COMPLETE_THE_JOBS: DO
     CALL DMSM_GET_CRITICAL_JOB(DMSM_MY_JOB)
     IF(DMSM_MY_JOB.NE.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_DO_THE_JOB_WRAPER(DO_THE_JOB)
     ELSE
        EXIT COMPLETE_THE_JOBS
     END IF
  END DO COMPLETE_THE_JOBS
  !$OMP END PARALLEL
  RETURN

END SUBROUTINE DMSM_DO_THE_JOB_GROUP




SUBROUTINE DMSM_GET_CRITICAL_JOB(THE_JOB)
  ! JOB ASSIGNMENT
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_ERROR_NOT_STOP_INTERF
  IMPLICIT   NONE
  INTEGER :: THE_JOB, OLD
  !$OMP CRITICAL(DMSM_IT)
  OLD=DMSM_JOB_ASSIGNED
  THE_JOB=DMSM_END_OF_GROUP_JOBS
  IF (DMSM_JOB_ASSIGNED.LT.DMSM_JOB_TO) THEN
      DMSM_JOB_ASSIGNED=DMSM_JOB_ASSIGNED+1
      THE_JOB=          DMSM_JOB_ASSIGNED
      DMSM_JOB_GROUP_INFORMATION(1,THE_JOB)=DMSM_JOB_FROM
      DMSM_JOB_GROUP_INFORMATION(2,THE_JOB)=DMSM_JOB_TO
      DMSM_JOB_GROUP_INFORMATION(3,THE_JOB)=DMSM_CURRENT_GROUP_NUMBER
     !DMSM_JOB_GROUP_INFORMATION(4,THE_JOB)=DMSM_MY_THREAD_NUMBER
      CALL DMSM_SET_INITIAL_LOCK()
      IF(DMSM_MY_JOB.NE.(OLD+1)) THEN
         WRITE(*,*) OLD, DMSM_MY_JOB, DMSM_JOB_FROM, DMSM_JOB_ASSIGNED, DMSM_JOB_TO,  &
                    DMSM_CURRENT_GROUP_NUMBER, DMSM_MY_MPI_RANK, DMSM_MY_THREAD_NUMBER
         CALL DMSM_ERROR_NOT_STOP("ERROR IN DMSM_GET_CRITICAL_JOB: ", OLD, " BUT ", DMSM_MY_JOB)
      END IF
  END IF
  !$OMP END CRITICAL(DMSM_IT)
  RETURN
END SUBROUTINE DMSM_GET_CRITICAL_JOB




SUBROUTINE DMSM_GET_JOB(THE_JOB)
  ! JOB ASSIGNMENT
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_ERROR_NOT_STOP_INTERF
  IMPLICIT   NONE
  INTEGER :: THE_JOB, OLD
  OLD=DMSM_JOB_ASSIGNED
  THE_JOB=DMSM_END_OF_GROUP_JOBS
  IF (DMSM_JOB_ASSIGNED.LT.DMSM_JOB_TO) THEN
      DMSM_JOB_ASSIGNED=DMSM_JOB_ASSIGNED+1
      THE_JOB=          DMSM_JOB_ASSIGNED
      DMSM_JOB_GROUP_INFORMATION(1,THE_JOB)=DMSM_JOB_FROM
      DMSM_JOB_GROUP_INFORMATION(2,THE_JOB)=DMSM_JOB_TO
      DMSM_JOB_GROUP_INFORMATION(3,THE_JOB)=DMSM_CURRENT_GROUP_NUMBER
     !DMSM_JOB_GROUP_INFORMATION(4,THE_JOB)=DMSM_MY_THREAD_NUMBER
      CALL DMSM_SET_INITIAL_LOCK()
      IF(DMSM_MY_JOB.NE.(OLD+1)) THEN
         WRITE(*,*) OLD, DMSM_MY_JOB, DMSM_JOB_FROM, DMSM_JOB_ASSIGNED, DMSM_JOB_TO,  &
                    DMSM_CURRENT_GROUP_NUMBER, DMSM_MY_MPI_RANK, DMSM_MY_THREAD_NUMBER
         CALL DMSM_ERROR_NOT_STOP("ERROR IN DMSM_GET_JOB: ", OLD, " BUT ", DMSM_MY_JOB)
      END IF
  END IF
  RETURN
END SUBROUTINE DMSM_GET_JOB




SUBROUTINE DMSM_SERIAL_CORE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_T_INTERFACE
  USE DMSM_RE_COLL_LOCKED_INTERFACE
  IMPLICIT   NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  COMPLETE_THE_JOBS: DO
     CALL DMSM_GET_JOB(DMSM_MY_JOB)
     IF(DMSM_MY_JOB.EQ.DMSM_END_OF_GROUP_JOBS) THEN
        DMSM_CURRENT_GROUP_NUMBER=DMSM_CURRENT_GROUP_NUMBER+1
        IF(DMSM_CURRENT_GROUP_NUMBER.LE.DMSM_TOTAL_JOB_GROUPS) THEN
           DMSM_JOB_FROM=(DMSM_CURRENT_GROUP_NUMBER-1)*DMSM_NUM_OF_JOBS_PER_GROUP+1
           DMSM_JOB_TO=DMSM_JOB_FROM-1+DMSM_NUM_OF_JOBS_PER_GROUP
           IF(DMSM_JOB_TO.GT.DMSM_TOTAL_JOBS) DMSM_JOB_TO=DMSM_TOTAL_JOBS
           DMSM_JOB_ASSIGNED=DMSM_JOB_FROM-1
           IF(DMSM_JOB_FROM.GT.DMSM_JOB_TO) &
              CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB NUMBERS IN DMSM_SERIAL_CORE FROM: ",DMSM_JOB_FROM, &
                        " TO ", DMSM_JOB_TO, " OF THE RANK ", DMSM_MY_MPI_RANK, " AND THREAD ", DMSM_MY_THREAD_NUMBER)
           CALL DMSM_GROUP_PREPARE_WRAPER(DMSM_JOB_FROM, DMSM_JOB_TO, DMSM_MY_MPI_RANK, &
                                               JOB_GROUP_PREPARATION, RESULT_COLLECTION)
           CALL DMSM_GET_JOB(DMSM_MY_JOB)
        ELSE
           IF(DMSM_RESULT_COLLECTION_ENABLED) &
           CALL DMSM_RESULT_COLLECTION_LOCKED(DMSM_MY_MPI_RANK,DMSM_MY_MPI_RANK,RESULT_COLLECTION)
           !DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1)= &
        END IF
     END IF

     IF(DMSM_MY_JOB.NE.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_DO_THE_SERIAL_JOB_WRAPER(DO_THE_JOB)
     ELSE
        EXIT COMPLETE_THE_JOBS
     END IF
  END DO COMPLETE_THE_JOBS
  RETURN
END SUBROUTINE DMSM_SERIAL_CORE




SUBROUTINE DMSM_MASTER_THREADS_ALLOCATE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_T_INTERFACE
  USE DMSM_RE_COLL_LOCKED_INTERFACE
  IMPLICIT   NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  COMPLETE_THE_JOBS: DO
     !$OMP CRITICAL(DMSM_01)
     CALL DMSM_GET_JOB(DMSM_MY_JOB)
     IF(DMSM_MY_JOB.EQ.DMSM_END_OF_GROUP_JOBS) THEN
        DMSM_CURRENT_GROUP_NUMBER=DMSM_CURRENT_GROUP_NUMBER+1
        IF(DMSM_CURRENT_GROUP_NUMBER.LE.DMSM_FIRST_GROUPS_ALLOCATED) THEN
           DMSM_JOB_FROM=(DMSM_CURRENT_GROUP_NUMBER-1)*DMSM_NUM_OF_JOBS_PER_GROUP+1
           DMSM_JOB_TO=DMSM_JOB_FROM-1+DMSM_NUM_OF_JOBS_PER_GROUP
           IF(DMSM_JOB_TO.GT.DMSM_TOTAL_JOBS) DMSM_JOB_TO=DMSM_TOTAL_JOBS
           DMSM_JOB_ASSIGNED=DMSM_JOB_FROM-1
           IF(DMSM_JOB_FROM.GT.DMSM_JOB_TO) &
              CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB NUMBERS IN DMSM_MASTER_THREADS_ALLOCATE FROM: ",DMSM_JOB_FROM, &
                        " TO ", DMSM_JOB_TO, " OF THE RANK ", DMSM_MY_MPI_RANK, " AND THREAD ", DMSM_MY_THREAD_NUMBER)
           CALL DMSM_GROUP_PREPARE_WRAPER(DMSM_JOB_FROM, DMSM_JOB_TO, DMSM_MY_MPI_RANK, &
                                               JOB_GROUP_PREPARATION, RESULT_COLLECTION)
           CALL DMSM_GET_JOB(DMSM_MY_JOB)
        ELSE
           IF(DMSM_RESULT_COLLECTION_ENABLED) &
           CALL DMSM_RESULT_COLLECTION_LOCKED(DMSM_MY_MPI_RANK,DMSM_MY_MPI_RANK,RESULT_COLLECTION)
           !DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1)= &
        END IF
     END IF
     !$OMP END CRITICAL(DMSM_01)
     IF(DMSM_MY_JOB.NE.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_DO_THE_JOB_WRAPER(DO_THE_JOB)
     ELSE
        EXIT COMPLETE_THE_JOBS
     END IF
  END DO COMPLETE_THE_JOBS
  RETURN
END SUBROUTINE DMSM_MASTER_THREADS_ALLOCATE




SUBROUTINE DMSM_MASTER_RECORDS(SC, TD, DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  INTEGER   :: SC, TD, K

  IF((SC.LT.0).OR.(SC.GE.DMSM_TOTAL_MPI_PROCESSES)) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR RANK IN DMSM_MASTER_RECORDS: ",SC)
  IF((TD.LT.-1).OR.(TD.GE.DMSM_THREADS_PER_PROCESS_GOT(SC+1))) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR THREAD NUMBER IN DMSM_MASTER_RECORDS: ",TD)

  IF(DMSM_JOB_RANGE(1).LE.0) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB NUM IN DMSM_MASTER_RECORDS: ",DMSM_JOB_RANGE(1))
  ELSE IF(DMSM_JOB_RANGE(1).LE.DMSM_TOTAL_JOBS) THEN
     IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) &
        PRINT*,'MASTER     SAYS: "SLAVE #',SC,', DO JOBS ',DMSM_JOB_RANGE(1), ' through ', DMSM_JOB_RANGE(2), ' ! "'
     K=MIN(DMSM_JOB_RANGE(2),DMSM_TOTAL_JOBS)
     DMSM_JOBS_SENT_BY_MASTER(DMSM_JOB_RANGE(1):K)=SC
     DMSM_JOBS_TO_THREAD_BY_MASTER(DMSM_JOB_RANGE(1))=TD
  ELSE
     DMSM_NO_MORE_WORK_INSTRUCTED(TD+1,SC+1)=DMSM_NO_MORE_WORK_INSTRUCTED(TD+1,SC+1)+1
     IF(SC.NE.DMSM_MASTER) THEN
        DMSM_TOTAL_SLAVE_GONE_HOME=DMSM_TOTAL_SLAVE_GONE_HOME+1
     ELSE
        DMSM_TOTAL_M_THREAD_GONE_HOME=DMSM_TOTAL_M_THREAD_GONE_HOME+1
     END IF
     IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) &
        PRINT*,'MASTER     SAYS: "SLAVE #',SC, ', NO MORE JOB TODAY! "'
  END IF

  IF(OMP_IN_PARALLEL()) THEN
    !$OMP flush(DMSM_TOTAL_M_THREAD_GONE_HOME, DMSM_TOTAL_SLAVE_GONE_HOME)
  END IF

  RETURN
END SUBROUTINE DMSM_MASTER_RECORDS




SUBROUTINE DMSM_ONE_SLAVE_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_N_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  COMPLETE_THE_JOBS: DO
     !$OMP CRITICAL(DMSM_06)
     CALL DMSM_GET_JOB(DMSM_MY_JOB)
     IF((DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO).AND.(DMSM_MY_JOB.EQ.DMSM_END_OF_GROUP_JOBS)) THEN
        CALL DMSM_SLAVE_APPLYING(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CALL DMSM_GET_JOB(DMSM_MY_JOB)
     END IF
     !$OMP END CRITICAL(DMSM_06)

     IF(DMSM_MY_JOB.NE.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_DO_THE_JOB_WRAPER(DO_THE_JOB)
     ELSE IF(DMSM_NO_MORE_WORK_RECEIVED(1,DMSM_MY_MPI_RANK+1).GT.0) THEN
        EXIT COMPLETE_THE_JOBS
     END IF

  END DO COMPLETE_THE_JOBS

  RETURN
END SUBROUTINE DMSM_ONE_SLAVE_IN_OPENMP




SUBROUTINE DMSM_SLAVES_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_N_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  COMPLETE_THE_JOBS: DO
     CALL DMSM_GET_JOB(DMSM_MY_JOB)
     IF(DMSM_MY_JOB.EQ.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_SLAVE_APPLYING(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CALL DMSM_GET_JOB(DMSM_MY_JOB)
     END IF

     IF(DMSM_MY_JOB.NE.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_DO_THE_SERIAL_JOB_WRAPER(DO_THE_JOB)
     ELSE IF(DMSM_NO_MORE_WORK_RECEIVED(1,DMSM_MY_MPI_RANK+1).GT.0) THEN
        EXIT COMPLETE_THE_JOBS
     END IF

  END DO COMPLETE_THE_JOBS

  RETURN
END SUBROUTINE DMSM_SLAVES_PURE_MPI




SUBROUTINE DMSM_MASTER_THREADS_DYNAMIC(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_T_INTERFACE
  USE DMSM_RE_COLL_LOCKED_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER   :: K

  COMPLETE_THE_JOBS: DO
     !$OMP CRITICAL(DMSM_12)
     CALL DMSM_GET_JOB(DMSM_MY_JOB)
     IF(DMSM_MY_JOB.EQ.DMSM_END_OF_GROUP_JOBS) THEN
        IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) PRINT*,'SLAVE #',DMSM_MY_MPI_RANK, ' SAYS: "', 'MASTER, I AM IDLE. "'
        !$OMP CRITICAL(DMSM_JOB_GROUP_12)
           CALL DMSM_NEXT_JOB_RANGE()
           DMSM_JOB_FROM=DMSM_JOB_RANGE(1)
           DMSM_JOB_TO = DMSM_JOB_RANGE(2)
           DMSM_JOB_ASSIGNED=DMSM_JOB_FROM-1
           DMSM_CURRENT_GROUP_NUMBER = DMSM_JOB_RANGE(3)
           CALL DMSM_MASTER_RECORDS(DMSM_MASTER,DMSM_MY_THREAD_NUMBER,DO_THE_JOB,JOB_GROUP_PREPARATION,RESULT_COLLECTION)
        !$OMP END CRITICAL(DMSM_JOB_GROUP_12)
        IF(DMSM_JOB_FROM.LE.0) THEN
           CALL DMSM_ERROR_AND_STOP("ERROR JOB NUMBER IN DMSM_MASTER_THREADS_DYNAMIC: ",DMSM_JOB_FROM)
        ELSE IF(DMSM_JOB_FROM.LE.DMSM_TOTAL_JOBS) THEN
           IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) &
           PRINT*,'SLAVE #',DMSM_MY_MPI_RANK, ' SAYS: ','"OK, I WILL BE DOING JOB #',K, ' IMMEDIATELY. "'
           IF(DMSM_JOB_FROM.GT.DMSM_JOB_TO) &
              CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB NUMBERS IN DMSM_MASTER_THREADS_DYNAMIC FROM: ",DMSM_JOB_FROM, &
                        " TO ", DMSM_JOB_TO, " OF THE RANK ", DMSM_MY_MPI_RANK, " AND THREAD ", DMSM_MY_THREAD_NUMBER)
           CALL DMSM_GROUP_PREPARE_WRAPER(DMSM_JOB_FROM, DMSM_JOB_TO, DMSM_MY_MPI_RANK, &
                                               JOB_GROUP_PREPARATION, RESULT_COLLECTION)
           CALL DMSM_GET_JOB(DMSM_MY_JOB)
        ELSE
           IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) &
           PRINT*,'SLAVE #',DMSM_MY_MPI_RANK, ' SAYS: ','"THANKS! MASTER, I AM ', 'GOING HOME."'
           IF(DMSM_RESULT_COLLECTION_ENABLED) &
           CALL DMSM_RESULT_COLLECTION_LOCKED(DMSM_MY_MPI_RANK,DMSM_MY_MPI_RANK,RESULT_COLLECTION)
           DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1)= &
           DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1)+1
        END IF
     END IF
     !$OMP END CRITICAL(DMSM_12)

     IF(DMSM_MY_JOB.NE.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_DO_THE_JOB_WRAPER(DO_THE_JOB)
     ELSE IF(DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1).GT.0) THEN
        EXIT COMPLETE_THE_JOBS
     END IF

  END DO COMPLETE_THE_JOBS
  RETURN
END SUBROUTINE DMSM_MASTER_THREADS_DYNAMIC




SUBROUTINE DMSM_NEXT_JOB_RANGE()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT  NONE

  DMSM_JOB_RANGE(1)=DMSM_JOB_RANGE(2)+1

  IF(DMSM_JOB_GROUPING.EQ.1) THEN
     IF(DMSM_JOB_RANGE(2).LT.DMSM_TOTAL_JOBS) THEN
        DMSM_JOB_RANGE(2)=DMSM_JOB_RANGE(1)-1+DMSM_NUM_OF_JOBS_PER_GROUP
     ELSE
     END IF
  ELSE
     CALL DMSM_ERROR_AND_STOP("NOT SUPPORTED JOB GROUPING: ", DMSM_JOB_GROUPING)
  END IF

  IF(DMSM_JOB_RANGE(2).GT.DMSM_TOTAL_JOBS) DMSM_JOB_RANGE(2)=DMSM_TOTAL_JOBS

  DMSM_JOB_GROUP_COUNTER=DMSM_JOB_GROUP_COUNTER+1
  DMSM_JOB_RANGE(3)=DMSM_JOB_GROUP_COUNTER

  RETURN
END SUBROUTINE DMSM_NEXT_JOB_RANGE




SUBROUTINE DMSM_MASTER_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_M_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER:: SC, TD, TG, TG2, TDSHARED
  INTEGER:: MPISTTS(MPI_STATUS_SIZE), AN_MPI_REQUEST

  IF(DMSM_TOTAL_MPI_PROCESSES.LE.1) THEN
     CALL DMSM_ERROR_AND_STOP("The routine DMSM_MASTER_PURE_MPI is not for so few processes/nodes: ", &
                                                                        DMSM_TOTAL_MPI_PROCESSES)
  END IF

  JOB_ASSIGNING: DO
     CALL DMSM_NEXT_JOB_RANGE()
     CALL MPI_RECV( TD ,1,MPI_INTEGER,MPI_ANY_SOURCE,MPI_ANY_TAG,DMSM_COMMUNICATOR,MPISTTS,DMSM_IERR)
     SC=MPISTTS(MPI_SOURCE)
     TG=MPISTTS(MPI_TAG)
     TG2=DMSM_MPI_TAG_ZERO_POINT+TD+SC*DMSM_THREADS_PP_EXPECTED
     IF(TG.NE.TG2) THEN
        WRITE(*,*)'TAGS: ',TG,TG2,DMSM_MPI_TAG_ZERO_POINT,TD,SC,DMSM_THREADS_PP_EXPECTED
                 CALL DMSM_ERROR_AND_STOP("ERROR FOR TAG DISCRIPANCY IN DMSM_MASTER_PURE_MPI: ",TG,&
                                          " AND ", TG2, " WITH SOURCE ", SC, &
                                          " FOR JOBS FROM ", DMSM_JOB_RANGE(1), " TO ", DMSM_JOB_RANGE(2))
     END IF
     CALL DMSM_MASTER_ASSIGN_SLAVE(SC, TD , TG, DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     IF(DMSM_TOTAL_SLAVE_GONE_HOME.GE.DMSM_TOTAL_SLAVE_SHOULD_HOME) THEN
        EXIT JOB_ASSIGNING
     END IF
  END DO JOB_ASSIGNING

  RETURN
END SUBROUTINE DMSM_MASTER_PURE_MPI




SUBROUTINE DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_M_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER:: SC, TD, TG, TG2
  INTEGER:: MPISTTS(MPI_STATUS_SIZE)

  IF(DMSM_TOTAL_MPI_PROCESSES.LE.1) THEN
     CALL DMSM_ERROR_AND_STOP("The routine DMSM_MASTER_ALL is not for so few processes/nodes: ", &
                                                                        DMSM_TOTAL_MPI_PROCESSES)
  END IF

  JOB_ASSIGNING: DO
     CALL MPI_RECV(TD,1,MPI_INTEGER,MPI_ANY_SOURCE,MPI_ANY_TAG,DMSM_COMMUNICATOR,MPISTTS,DMSM_IERR)
     !$OMP CRITICAL(DMSM_JOB_GROUP_12)
           CALL DMSM_NEXT_JOB_RANGE()
           SC=MPISTTS(MPI_SOURCE)
           TG=MPISTTS(MPI_TAG)
           TG2=DMSM_MPI_TAG_ZERO_POINT+TD+SC*DMSM_THREADS_PP_EXPECTED
           IF(TG.NE.TG2) THEN
              WRITE(*,*)'TAGS: ',TG,TG2,DMSM_MPI_TAG_ZERO_POINT,TD,SC,DMSM_THREADS_PP_EXPECTED
              CALL DMSM_ERROR_AND_STOP("ERROR FOR TAG DISCRIPANCY IN DMSM_MASTER_ALLA: ",TG,&
                                          " AND ", TG2, " WITH SOURCE ", SC, &
                                          " FOR JOBS FROM ", DMSM_JOB_RANGE(1), " TO ", DMSM_JOB_RANGE(2))
           END IF
           CALL DMSM_MASTER_ASSIGN_SLAVE(SC, TD, TG, DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     !$OMP END CRITICAL(DMSM_JOB_GROUP_12)
     ! This flush not necessary, then disabled: $OMP flush(DMSM_TOTAL_M_THREAD_GONE_HOME, DMSM_TOTAL_SLAVE_GONE_HOME)
     IF(DMSM_TOTAL_SLAVE_GONE_HOME.GE.DMSM_TOTAL_SLAVE_SHOULD_HOME) EXIT JOB_ASSIGNING
  END DO JOB_ASSIGNING

  RETURN
END SUBROUTINE DMSM_MASTER_ALL




SUBROUTINE DMSM_MASTER_ASSIGN_SLAVE(SC, TD, TG,DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_T_INTERFACE
  USE DMSM_RE_COLL_LOCKED_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER:: SC, TD, TG
  CALL MPI_SEND(DMSM_JOB_RANGE,DMSM_JOB_RANGE_SIZE,MPI_INTEGER,SC,TG,DMSM_COMMUNICATOR,DMSM_IERR)
  CALL DMSM_MASTER_RECORDS(SC,TD,DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  IF(DMSM_JOB_RANGE(1).LE.DMSM_TOTAL_JOBS) THEN
     CALL DMSM_GROUP_PREPARE_WRAPER(DMSM_JOB_RANGE(1), DMSM_JOB_RANGE(2), SC, &
                                 JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  ELSE
     IF(DMSM_RESULT_COLLECTION_ENABLED) &
     CALL DMSM_RESULT_COLLECTION_LOCKED(DMSM_MY_MPI_RANK,SC,RESULT_COLLECTION)
  END IF
  RETURN
END SUBROUTINE DMSM_MASTER_ASSIGN_SLAVE




SUBROUTINE DMSM_ALL_SLAVES_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_N_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  COMPLETE_THE_JOBS: DO
     !$OMP CRITICAL(DMSM_05)
     CALL DMSM_GET_JOB(DMSM_MY_JOB)
     IF(DMSM_MY_JOB.EQ.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_SLAVE_APPLYING(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CALL DMSM_GET_JOB(DMSM_MY_JOB)
     END IF
     !$OMP END CRITICAL(DMSM_05)

     IF(DMSM_MY_JOB.NE.DMSM_END_OF_GROUP_JOBS) THEN
        CALL DMSM_DO_THE_JOB_WRAPER(DO_THE_JOB)
     ELSE IF(DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1).GT.0) THEN
        EXIT COMPLETE_THE_JOBS
     END IF

  END DO COMPLETE_THE_JOBS

  RETURN
END SUBROUTINE DMSM_ALL_SLAVES_IN_OPENMP




SUBROUTINE DMSM_SLAVE_APPLYING(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_T_INTERFACE
  USE DMSM_RE_COLL_LOCKED_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER   :: MPISTTS(MPI_STATUS_SIZE)
        !TG=DMSM_MPI_TAG_ZERO_POINT+DMSM_MY_THREAD_NUMBER+DMSM_MY_MPI_RANK*DMSM_THREADS_PP_EXPECTED
        IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) PRINT*,'SLAVE #',DMSM_MY_MPI_RANK, ' SAYS: "', 'MASTER, I AM IDLE. "'
        CALL MPI_SEND(DMSM_MY_THREAD_NUMBER,1,MPI_INTEGER,DMSM_MASTER,DMSM_A_TAG_FOR_MPI,DMSM_COMMUNICATOR,DMSM_IERR)
        CALL MPI_RECV(DMSM_JOB_RANGE,DMSM_JOB_RANGE_SIZE,MPI_INTEGER,DMSM_MASTER,DMSM_A_TAG_FOR_MPI, &
                                                                                 DMSM_COMMUNICATOR,MPISTTS,DMSM_IERR)
        IF(DMSM_JOB_RANGE(1).LE.0) THEN
          CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB NUM IN DMSM_SLAVE_APPLYING: ",DMSM_JOB_RANGE(1))
        ELSE IF(DMSM_JOB_RANGE(1).LE.DMSM_TOTAL_JOBS) THEN
           IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) &
           PRINT*,'SLAVE #',DMSM_MY_MPI_RANK, ' SAYS: ','"OK, I WILL BE DOING JOB #',DMSM_JOB_RANGE(1:2), ' IMMEDIATELY. "'
           CALL DMSM_GROUP_PREPARE_WRAPER(DMSM_JOB_RANGE(1), DMSM_JOB_RANGE(2), DMSM_MY_MPI_RANK, &
                                               JOB_GROUP_PREPARATION, RESULT_COLLECTION)
           DMSM_JOB_FROM=DMSM_JOB_RANGE(1)
           DMSM_JOB_TO = DMSM_JOB_RANGE(2)
           DMSM_CURRENT_GROUP_NUMBER = DMSM_JOB_RANGE(3)
           DMSM_JOB_ASSIGNED=DMSM_JOB_FROM-1
           IF(DMSM_JOB_FROM.GT.DMSM_JOB_TO) &
              CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB NUMBERS IN DMSM_SLAVE_APPLYING FROM: ",DMSM_JOB_FROM, &
                " TO ", DMSM_JOB_TO, " OF THE RANK ", DMSM_MY_MPI_RANK, " AND THREAD ", DMSM_MY_THREAD_NUMBER)

        ELSE
           IF(DMSM_MASTER_SLAVE_SHOWING.EQ.1) &
           PRINT*,'SLAVE #',DMSM_MY_MPI_RANK, ' SAYS: ','"THANKS! MASTER, I AM ', 'GOING HOME."'
           IF(DMSM_RESULT_COLLECTION_ENABLED) &
           CALL DMSM_RESULT_COLLECTION_LOCKED(DMSM_MY_MPI_RANK,DMSM_MY_MPI_RANK,RESULT_COLLECTION)
           DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1)=&
                                 DMSM_NO_MORE_WORK_RECEIVED(DMSM_MY_THREAD_NUMBER+1,DMSM_MY_MPI_RANK+1)+1
        END IF
  RETURN
END SUBROUTINE DMSM_SLAVE_APPLYING




SUBROUTINE DMSM_DO_THE_SERIAL_JOB_WRAPER(DO_THE_JOB)
        USE DMSM_MODULE_NOT_FOR_USER
        USE DMSM_GET_CPU_TIME_INTERFACE
        IMPLICIT NONE
        INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
        END INTERFACE
        CALL DMSM_JOB_RECORDER()
        CALL DMSM_GET_CPU_TIME(DMSM_BEGINNING_TIME)
        IF(DMSM_ONLY_MPI_ENABLED) THEN
           CALL MPI_BCAST(DMSM_MY_JOB,1,MPI_INTEGER,DMSM_MASTER,DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
        END IF
        CALL DO_THE_JOB(DMSM_MY_JOB)
        CALL DMSM_GET_CPU_TIME(DMSM_ENDDING_TIME)
        DMSM_CPU_TIME_OF_JOBS(DMSM_MY_JOB)=DMSM_ENDDING_TIME-DMSM_BEGINNING_TIME
        RETURN
END SUBROUTINE DMSM_DO_THE_SERIAL_JOB_WRAPER




SUBROUTINE DMSM_DO_THE_JOB_WRAPER(DO_THE_JOB)
        USE DMSM_MODULE_NOT_FOR_USER
        IMPLICIT NONE
        INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
        END INTERFACE
        CALL DMSM_JOB_RECORDER()
        CALL DMSM_CPU_TIME_IN_OPENMP(DMSM_BEGINNING_TIME)
        IF(DMSM_ONLY_MPI_ENABLED) THEN
           CALL MPI_BCAST(DMSM_MY_JOB,1,MPI_INTEGER,DMSM_MASTER,DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
        END IF
        CALL DO_THE_JOB(DMSM_MY_JOB)
        CALL DMSM_CPU_TIME_IN_OPENMP(DMSM_ENDDING_TIME)
        DMSM_CPU_TIME_OF_JOBS(DMSM_MY_JOB)=DMSM_ENDDING_TIME-DMSM_BEGINNING_TIME
        RETURN
END SUBROUTINE DMSM_DO_THE_JOB_WRAPER




SUBROUTINE DMSM_GROUP_PREPARE_WRAPER(FROM,TO,DESTINATION,JOB_GROUP_PREPARATION,RESULT_COLLECTION)
        USE DMSM_MODULE_NOT_FOR_USER
        USE DMSM_RE_COLL_LOCKED_INTERFACE
        IMPLICIT NONE
        INTEGER :: FROM,TO,DESTINATION
        INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
        END INTERFACE
        INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
        END INTERFACE

  !$OMP CRITICAL(DMSM_JOB_WRAPPER)
        IF(DMSM_INITIAL_DATA_PREP_ENABLED) &
        CALL JOB_GROUP_PREPARATION(FROM,TO,DMSM_MY_MPI_RANK,DESTINATION)
  !$OMP END CRITICAL(DMSM_JOB_WRAPPER)
        IF(DMSM_RESULT_COLLECTION_ENABLED) &
        CALL DMSM_RESULT_COLLECTION_LOCKED(DMSM_MY_MPI_RANK,DESTINATION,RESULT_COLLECTION)
        !   DMSM_JOB_FROM=DMSM_JOB_RANGE(1)
        !   DMSM_JOB_TO = DMSM_JOB_RANGE(2)
        !   DMSM_JOB_ASSIGNED=DMSM_JOB_FROM-1
        !DMSM_JOB_FROM=(K-1)*DMSM_NUM_OF_JOBS_PER_GROUP+1
        !DMSM_JOB_TO=DMSM_JOB_FROM-1+DMSM_NUM_OF_JOBS_PER_GROUP
        !IF(DMSM_JOB_TO.GT.DMSM_TOTAL_JOBS) DMSM_JOB_TO=DMSM_TOTAL_JOBS
        !DMSM_JOB_ASSIGNED=DMSM_JOB_FROM-1

        RETURN
END SUBROUTINE DMSM_GROUP_PREPARE_WRAPER




SUBROUTINE DMSM_RESULT_COLLECTION_LOCKED(MY_RANK,FROM,RESULT_COLLECTION)
         IMPLICIT NONE
         INTEGER :: MY_RANK,FROM
         INTERFACE
            SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
              IMPLICIT NONE
              INTEGER :: DUMMY_J,DUMMY_K
            END SUBROUTINE RESULT_COLLECTION
         END INTERFACE
         INTERFACE
            FUNCTION OMP_IN_PARALLEL()
              IMPLICIT NONE
              LOGICAL :: OMP_IN_PARALLEL
            END FUNCTION OMP_IN_PARALLEL
         END INTERFACE
         CALL DMSM_SET_FINAL_RESULT_LOCK()
         CALL RESULT_COLLECTION(MY_RANK,FROM)
         CALL DMSM_UNSET_FINAL_RESULT_LOCK()
         RETURN
END SUBROUTINE DMSM_RESULT_COLLECTION_LOCKED




SUBROUTINE DMSM_SERIAL_VERSION(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  IF(DMSM_TOTAL_MPI_PROCESSES.NE.1) THEN
     CALL DMSM_ERROR_AND_STOP("The routine DMSM_SERIAL_VERSION is only for ONE process/node: ", &
                                                                        DMSM_TOTAL_MPI_PROCESSES)
  END IF

  IF(DMSM_THREADS_PP_EXPECTED.NE.1) THEN
     CALL DMSM_ERROR_AND_STOP("The routine DMSM_SERIAL_VERSION is only for ONE thread each node: ", &
                                                                        DMSM_THREADS_PP_EXPECTED)
  END IF

  DMSM_NO_MORE_GROUPS_FROM=2
  DMSM_NO_MORE_GROUPS_TO=1
  DMSM_MY_THREAD_NUMBER=0

  CALL DMSM_SERIAL_CORE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_SERIAL_VERSION worked:    ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_SERIAL_VERSION worked:    ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_SERIAL_VERSION




SUBROUTINE DMSM_PURE_OPENMP_MODEL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  IF(DMSM_TOTAL_MPI_PROCESSES.NE.1) THEN
     CALL DMSM_ERROR_AND_STOP("The routine DMSM_PURE_OPENMP_MODEL is only for ONE process/node: ", &
                                                                        DMSM_TOTAL_MPI_PROCESSES)
  END IF

  DMSM_NO_MORE_GROUPS_FROM=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=DMSM_THREADS_PER_PROCESS_GOT

  !$OMP PARALLEL DEFAULT(SHARED)
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
  IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_PURE_OPENMP_MODEL) ", DMSM_MY_MPI_RANK)
  END IF
  END IF
  !$OMP BARRIER

  CALL DMSM_MASTER_THREADS_ALLOCATE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)

  !$OMP END PARALLEL


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_PURE_OPENMP_MODEL worked: ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_PURE_OPENMP_MODEL worked: ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_PURE_OPENMP_MODEL




SUBROUTINE DMSM_PURE_MPI_MODEL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=DMSM_TOTAL_MPI_PROCESSES-1
  DMSM_TOTAL_M_THREAD_SHOULD_HM=0

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=1
  DMSM_MY_THREAD_NUMBER=0

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_MASTER_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  ELSE
     CALL DMSM_SLAVES_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF

  DMSM_JOBS_TO_THREAD_BY_MASTER=-1


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_PURE_MPI_MODEL worked:    ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_PURE_MPI_MODEL worked:    ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_PURE_MPI_MODEL




SUBROUTINE DMSM_MODEL_PLAN11(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=DMSM_TOTAL_MPI_PROCESSES-1
  DMSM_TOTAL_M_THREAD_SHOULD_HM=0

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=1

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_MASTER_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  ELSE
     CALL DMSM_SLAVE_OUTSIDE_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF

  DMSM_JOBS_TO_THREAD_BY_MASTER=-1


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN11 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN11 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN11




SUBROUTINE DMSM_MODEL_PLAN12(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=DMSM_TOTAL_MPI_PROCESSES-1
  DMSM_TOTAL_M_THREAD_SHOULD_HM=0

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=1

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_MASTER_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  ELSE
    !$OMP PARALLEL DEFAULT(SHARED)
    IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
    IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
       CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN12) ", DMSM_MY_MPI_RANK)
    END IF
    END IF
    !$OMP BARRIER
       CALL DMSM_ONE_SLAVE_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
    !$OMP END PARALLEL
  END IF


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN12 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN12 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN12




SUBROUTINE DMSM_MODEL_PLAN13(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=SUM(DMSM_THREADS_PER_PROCESS_GOT(1:DMSM_TOTAL_MPI_PROCESSES)) &
                                  -DMSM_THREADS_PER_PROCESS_GOT(1+DMSM_MASTER)
  DMSM_TOTAL_M_THREAD_SHOULD_HM=0

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=DMSM_THREADS_PER_PROCESS_GOT

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_MASTER_PURE_MPI(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  ELSE
    !$OMP PARALLEL DEFAULT(SHARED)
    IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
    IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
       CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN13) ", DMSM_MY_MPI_RANK)
    END IF
    END IF
    !$OMP BARRIER
       CALL DMSM_ALL_SLAVES_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
    !$OMP END PARALLEL
  END IF


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN13 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN13 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN13




SUBROUTINE DMSM_MODEL_PLAN21(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=DMSM_TOTAL_MPI_PROCESSES-1
  DMSM_TOTAL_M_THREAD_SHOULD_HM=0

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=1

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN

     !$OMP PARALLEL DEFAULT(SHARED)
     IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
     IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
        CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN21) ", DMSM_MY_MPI_RANK)
     END IF
     END IF
     !$OMP BARRIER

     IF(DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO) THEN
        CALL DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        CALL DMSM_MASTER_THREADS_ALLOCATE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     END IF
     !$OMP END PARALLEL

  ELSE
     CALL DMSM_SLAVE_OUTSIDE_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF

  DMSM_JOBS_TO_THREAD_BY_MASTER=-1


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN21 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN21 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN21




SUBROUTINE DMSM_MODEL_PLAN22(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=DMSM_TOTAL_MPI_PROCESSES-1
  DMSM_TOTAL_M_THREAD_SHOULD_HM=0

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=1

  !$OMP PARALLEL DEFAULT(SHARED)
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
  IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN22) ", DMSM_MY_MPI_RANK)
  END IF
  END IF
  !$OMP BARRIER

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     IF(DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO) THEN
        CALL DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        CALL DMSM_MASTER_THREADS_ALLOCATE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     END IF
  ELSE
     CALL DMSM_ONE_SLAVE_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF
  !$OMP END PARALLEL


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN22 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN22 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN22




SUBROUTINE DMSM_MODEL_PLAN23(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=SUM(DMSM_THREADS_PER_PROCESS_GOT(1:DMSM_TOTAL_MPI_PROCESSES)) &
                                  -DMSM_THREADS_PER_PROCESS_GOT(1+DMSM_MASTER)
  DMSM_TOTAL_M_THREAD_SHOULD_HM=0

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)+1
  DMSM_NO_MORE_GROUPS_TO=DMSM_THREADS_PER_PROCESS_GOT

  !$OMP PARALLEL DEFAULT(SHARED)
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
  IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN23) ", DMSM_MY_MPI_RANK)
  END IF
  END IF
  !$OMP BARRIER

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     IF(DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO) THEN
        CALL DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        CALL DMSM_MASTER_THREADS_ALLOCATE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     END IF
  ELSE
     CALL DMSM_ALL_SLAVES_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF
  !$OMP END PARALLEL


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN23 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN23 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN23




SUBROUTINE DMSM_MODEL_PLAN31(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=DMSM_TOTAL_MPI_PROCESSES-1
  DMSM_TOTAL_M_THREAD_SHOULD_HM=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)-1

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=2
  DMSM_NO_MORE_GROUPS_TO=1
  DMSM_NO_MORE_GROUPS_TO(1)=DMSM_THREADS_PER_PROCESS_GOT(1)

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN

     !$OMP PARALLEL DEFAULT(SHARED)
     IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
     IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
        CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN31) ", DMSM_MY_MPI_RANK)
     END IF
     END IF
     !$OMP BARRIER

     IF(DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO) THEN
        CALL DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        CALL DMSM_MASTER_THREADS_DYNAMIC(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     END IF
     !$OMP END PARALLEL

  ELSE
     CALL DMSM_SLAVE_OUTSIDE_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF

  DMSM_JOBS_TO_THREAD_BY_MASTER=-1


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN31 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN31 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN31




SUBROUTINE DMSM_MODEL_PLAN32(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=DMSM_TOTAL_MPI_PROCESSES-1
  DMSM_TOTAL_M_THREAD_SHOULD_HM=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)-1

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=2
  DMSM_NO_MORE_GROUPS_TO=1
  DMSM_NO_MORE_GROUPS_TO(1)=DMSM_THREADS_PER_PROCESS_GOT(1)

  !$OMP PARALLEL DEFAULT(SHARED)
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
  IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN32) ", DMSM_MY_MPI_RANK)
  END IF
  END IF
  !$OMP BARRIER

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     IF(DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO) THEN
        CALL DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        CALL DMSM_MASTER_THREADS_DYNAMIC(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     END IF
  ELSE
     CALL DMSM_ONE_SLAVE_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF
  !$OMP END PARALLEL


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN32 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN32 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN32




SUBROUTINE DMSM_MODEL_PLAN33(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_G_INTERFACE
  IMPLICIT  NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_TOTAL_SLAVE_SHOULD_HOME=SUM(DMSM_THREADS_PER_PROCESS_GOT(1:DMSM_TOTAL_MPI_PROCESSES)) &
                                  -DMSM_THREADS_PER_PROCESS_GOT(1+DMSM_MASTER)
  DMSM_TOTAL_M_THREAD_SHOULD_HM=DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1)-1

  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_FROM(1)=2
  DMSM_NO_MORE_GROUPS_TO=DMSM_THREADS_PER_PROCESS_GOT

  !$OMP PARALLEL DEFAULT(SHARED)
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
  IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_MODEL_PLAN33) ", DMSM_MY_MPI_RANK)
  END IF
  END IF
  !$OMP BARRIER

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     IF(DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO) THEN
        CALL DMSM_MASTER_ALL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        CALL DMSM_MASTER_THREADS_DYNAMIC(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     END IF
  ELSE
     CALL DMSM_ALL_SLAVES_IN_OPENMP(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
  END IF
  !$OMP END PARALLEL


  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_MODEL_PLAN33 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_MODEL_PLAN33 worked:      ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  RETURN
END SUBROUTINE DMSM_MODEL_PLAN33




SUBROUTINE DMSM_JOB_RECORDER()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  IF((DMSM_MY_JOB.LE.0).OR.(DMSM_MY_JOB.GT.DMSM_TOTAL_JOBS)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB NUMBER IN DMSM_JOB_RECORDER: ",DMSM_MY_JOB," WITH TOTAL ",DMSM_TOTAL_JOBS)
  END IF
  DMSM_JOB_I_DID(1,DMSM_MY_JOB)=DMSM_JOB_I_DID(1,DMSM_MY_JOB)+1
  DMSM_JOB_I_DID(2,DMSM_MY_JOB)=DMSM_MY_MPI_RANK
  DMSM_JOB_I_DID(3,DMSM_MY_JOB)=DMSM_MY_THREAD_NUMBER
  DMSM_JOBS_COMPLETED=DMSM_JOBS_COMPLETED+1
  RETURN
END SUBROUTINE DMSM_JOB_RECORDER




SUBROUTINE DMSM_SET_COMM_CORE(AN_MPI_COMMUNICATOR)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER :: AN_MPI_COMMUNICATOR
  IF(DMSM_SET_COMM_CALLED) &
  CALL DMSM_ERROR_AND_STOP("ERROR: DMSM_SET_COMM_CORE CAN ONLY BE CALLED ONCE. ",0)
  CALL MPI_COMM_DUP( AN_MPI_COMMUNICATOR, DMSM_COMMUNICATOR, DMSM_IERR)
  CALL MPI_COMM_RANK(DMSM_COMMUNICATOR, DMSM_MY_MPI_RANK, DMSM_IERR)
  CALL MPI_COMM_SIZE(DMSM_COMMUNICATOR, DMSM_TOTAL_MPI_PROCESSES, DMSM_IERR)
  DMSM_SET_COMM_CALLED=.TRUE.
  RETURN
END SUBROUTINE DMSM_SET_COMM_CORE




SUBROUTINE DMSM_SET_COMM_01(AN_MPI_COMMUNICATOR)
  USE DMSM_SET_COMM_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER :: AN_MPI_COMMUNICATOR
  CALL DMSM_SET_COMM_CORE(AN_MPI_COMMUNICATOR)
  RETURN
END SUBROUTINE DMSM_SET_COMM_01




SUBROUTINE DMSM_SET_COMM_10(TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                     ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                     , DIST_COMM, DIST_RANK, DIST_PROCESSES )
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_SET_COMM_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER ::                TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                     ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                     , DIST_COMM, DIST_RANK, DIST_PROCESSES
  INTEGER ::                K

  IF(DMSM_SET_COMM_10_CALLED) &
  CALL DMSM_ERROR_AND_STOP("ERROR: DMSM_SET_COMM_10 CAN ONLY BE CALLED ONCE. ",0)

  CALL MPI_COMM_DUP(ALL_COMM, DMSM_ONLY_MPI_ALL_COMM, DMSM_IERR)
  CALL MPI_COMM_RANK(DMSM_ONLY_MPI_ALL_COMM,DMSM_ONLY_MPI_ALL_RANK,DMSM_IERR)
  CALL MPI_COMM_SIZE(DMSM_ONLY_MPI_ALL_COMM,DMSM_ONLY_MPI_ALL_PROCESSES,DMSM_IERR)
  ALL_RANK=     DMSM_ONLY_MPI_ALL_RANK
  ALL_PROCESSES=DMSM_ONLY_MPI_ALL_PROCESSES

  IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.1) THEN
     K=DMSM_MASTER
  ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.10) THEN
     K=DMSM_ONLY_MPI_ALL_PROCESSES-1
  ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.100) THEN
     K=DMSM_ONLY_MPI_ALL_PROCESSES-1
  ELSE
     CALL DMSM_ERROR_AND_STOP("ERROR: ILLEGAL DMSM_ONLY_MPI_TRUE_MASTER_PLAN: ", &
                                              DMSM_ONLY_MPI_TRUE_MASTER_PLAN)
  END IF
  DMSM_ONLY_MPI_JOB_PROCESSES=  TOTAL_PROCESSES_FOR_A_JOB
  CALL MPI_BCAST(DMSM_ONLY_MPI_JOB_PROCESSES,1,MPI_INTEGER,K,&
                 DMSM_ONLY_MPI_ALL_COMM,DMSM_IERR)
  IF(DMSM_ONLY_MPI_JOB_PROCESSES.LE.0) DMSM_ONLY_MPI_JOB_PROCESSES=1
  DMSM_ONLY_MPI_JOB_PROCESSES_E=DMSM_ONLY_MPI_JOB_PROCESSES

  IF(DMSM_ONLY_MPI_JOB_PROCESSES.LT.DMSM_ONLY_MPI_ALL_PROCESSES) THEN
     IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.1) THEN
        K=DMSM_ONLY_MPI_ALL_RANK
        CALL MPI_COMM_SPLIT(DMSM_ONLY_MPI_ALL_COMM, &
                   (K+DMSM_ONLY_MPI_JOB_PROCESSES-1)/DMSM_ONLY_MPI_JOB_PROCESSES, &
                         DMSM_ONLY_MPI_ALL_RANK,DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
     ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.10) THEN
        K=DMSM_ONLY_MPI_ALL_PROCESSES-1-DMSM_ONLY_MPI_ALL_RANK
        CALL MPI_COMM_SPLIT(DMSM_ONLY_MPI_ALL_COMM, &
                   (K+DMSM_ONLY_MPI_JOB_PROCESSES-1)/DMSM_ONLY_MPI_JOB_PROCESSES, &
                         DMSM_ONLY_MPI_ALL_RANK,DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
     ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.100) THEN
        K=DMSM_ONLY_MPI_ALL_RANK+1
        IF(DMSM_ONLY_MPI_ALL_RANK.EQ.(DMSM_ONLY_MPI_ALL_PROCESSES-1)) &
        K=10*(DMSM_ONLY_MPI_ALL_PROCESSES+DMSM_ONLY_MPI_JOB_PROCESSES+20)
        CALL MPI_COMM_SPLIT(DMSM_ONLY_MPI_ALL_COMM, &
                   (K+DMSM_ONLY_MPI_JOB_PROCESSES-1)/DMSM_ONLY_MPI_JOB_PROCESSES, &
                         DMSM_ONLY_MPI_ALL_RANK,DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
     END IF

  ELSE
     IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.1) THEN
        K=DMSM_ONLY_MPI_ALL_RANK
     ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.10) THEN
        K=DMSM_ONLY_MPI_ALL_PROCESSES-1-DMSM_ONLY_MPI_ALL_RANK
     ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.100) THEN
        K=DMSM_ONLY_MPI_ALL_PROCESSES-1-DMSM_ONLY_MPI_ALL_RANK
     END IF
     CALL MPI_COMM_SPLIT(DMSM_ONLY_MPI_ALL_COMM,1,K,DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)

  END IF

  CALL MPI_COMM_RANK(DMSM_ONLY_MPI_JOB_COMM,DMSM_ONLY_MPI_JOB_RANK,DMSM_IERR)
  CALL MPI_COMM_SIZE(DMSM_ONLY_MPI_JOB_COMM,DMSM_ONLY_MPI_JOB_PROCESSES,DMSM_IERR)
  JOB_COMM=     DMSM_ONLY_MPI_JOB_COMM
  JOB_RANK=     DMSM_ONLY_MPI_JOB_RANK
  JOB_PROCESSES=DMSM_ONLY_MPI_JOB_PROCESSES

  IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.1) THEN
     K=DMSM_ONLY_MPI_ALL_RANK
  ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.10) THEN
     K=DMSM_ONLY_MPI_ALL_PROCESSES-DMSM_ONLY_MPI_ALL_RANK
  ELSE IF(DMSM_ONLY_MPI_TRUE_MASTER_PLAN.EQ.100) THEN
     K=DMSM_ONLY_MPI_ALL_PROCESSES-DMSM_ONLY_MPI_ALL_RANK
  END IF

  CALL MPI_COMM_SPLIT(DMSM_ONLY_MPI_ALL_COMM,DMSM_ONLY_MPI_JOB_RANK, &
                      K,DMSM_ONLY_MPI_DIST_COMM,DMSM_IERR)
  CALL MPI_COMM_RANK(DMSM_ONLY_MPI_DIST_COMM,DMSM_ONLY_MPI_DIST_RANK,DMSM_IERR)
  CALL MPI_COMM_SIZE(DMSM_ONLY_MPI_DIST_COMM,DMSM_ONLY_MPI_DIST_PROCESSES,DMSM_IERR)

  DIST_COMM=     DMSM_ONLY_MPI_DIST_COMM
  DIST_RANK=     DMSM_ONLY_MPI_DIST_RANK
  DIST_PROCESSES=DMSM_ONLY_MPI_DIST_PROCESSES

  IF(DMSM_ONLY_MPI_JOB_RANK.EQ.DMSM_JOB_MASTER) THEN
     CALL DMSM_SET_COMM_CORE(DMSM_ONLY_MPI_DIST_COMM)
  END IF

  DMSM_SET_COMM_10_CALLED=.TRUE.
  RETURN
END SUBROUTINE DMSM_SET_COMM_10




SUBROUTINE DMSM_GEN_COMM_MPI_ALL(TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                     ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                     , DIST_COMM, DIST_RANK, DIST_PROCESSES )
  USE DMSM_SET_COMM_INTERFACE
  IMPLICIT NONE
  INTEGER ::                TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                     ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                     , DIST_COMM, DIST_RANK, DIST_PROCESSES
         CALL DMSM_SET_COMM(TOTAL_PROCESSES_FOR_A_JOB,  ALL_COMM,  ALL_RANK,  ALL_PROCESSES &
                                                     ,  JOB_COMM,  JOB_RANK,  JOB_PROCESSES &
                                                     , DIST_COMM, DIST_RANK, DIST_PROCESSES )
  RETURN
END SUBROUTINE DMSM_GEN_COMM_MPI_ALL




INTEGER FUNCTION DMSM_GET_MASTER()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  DMSM_GET_MASTER=DMSM_MASTER
  RETURN
END FUNCTION DMSM_GET_MASTER




SUBROUTINE DMSM_SET_MASTER(M)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_SET_COMM_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER ::  M,M2
  CALL DMSM_ERROR_AND_STOP("ERROR: DMSM_SET_MASTER is not fully tested. Please not use it. ",DMSM_DISTRIBUTION_PLAN)
  IF(DMSM_SET_COMM_CALLED) THEN
     CALL MPI_BCAST(M,1,MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)
     M2=DMSM_TOTAL_MPI_PROCESSES
  ELSE
     CALL MPI_BCAST(M,1,MPI_INTEGER,DMSM_MASTER,MPI_COMM_WORLD,DMSM_IERR)
     CALL MPI_COMM_SIZE(MPI_COMM_WORLD,M2,DMSM_IERR)
  END IF
  DMSM_MASTER=M
  IF((DMSM_MASTER.LT.0).OR.(DMSM_MASTER.GE.M2)) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR DMSM_SET_MASTER: ",DMSM_MASTER)
  RETURN
END SUBROUTINE DMSM_SET_MASTER




SUBROUTINE DMSM_INITIALIZE_CORE(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN, &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
#ifdef DMSM_FORTRAN_RS16_TIMER
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
#else
  USE DMSM_CTIMER_INTERFACE
#endif
#endif
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_SET_COMM_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER ::               THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  INTEGER ::               I, M, TT

  IF(.NOT.DMSM_SET_COMM_CALLED) THEN
      CALL DMSM_SET_COMM_CORE(MPI_COMM_WORLD)
  END IF
#ifdef DMSM_FORTRAN_RS16_TIMER
  CALL CPU_TIME(DMSM_WALL_BEGINNING_TIME)
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
  CALL CPU_TIME(DMSM_WALL_BEGINNING_TIME)
#else
  DMSM_WALL_BEGINNING_TIME=DMSM_C_TIMER()
#endif
#endif
  DMSM_THREADS_PP_EXPECTED=THREADS_PER_PROCESS_EXPECTED
  CALL MPI_BCAST(DMSM_THREADS_PP_EXPECTED,1,MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)
  IF(DMSM_THREADS_PP_EXPECTED.LE.1) DMSM_THREADS_PP_EXPECTED=1
  TT=1
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
      CALL OMP_SET_NUM_THREADS(DMSM_THREADS_PP_EXPECTED)
!$OMP PARALLEL
      IF(OMP_GET_THREAD_NUM().EQ.0) TT=OMP_GET_NUM_THREADS()
!$OMP END PARALLEL
  END IF
  ALLOCATE(DMSM_THREADS_PER_PROCESS_GOT(DMSM_TOTAL_MPI_PROCESSES))
  DMSM_THREADS_PER_PROCESS_GOT=0
  CALL MPI_GATHER(TT,1,MPI_INTEGER, DMSM_THREADS_PER_PROCESS_GOT,1, &
                          MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)
  CALL MPI_BCAST( DMSM_THREADS_PER_PROCESS_GOT,DMSM_TOTAL_MPI_PROCESSES, &
                          MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)
  DO I=1, DMSM_TOTAL_MPI_PROCESSES
     IF(DMSM_THREADS_PP_EXPECTED.LT.DMSM_THREADS_PER_PROCESS_GOT(I)) &
	    DMSM_THREADS_PP_EXPECTED =  DMSM_THREADS_PER_PROCESS_GOT(I)
  END DO

  DMSM_DISTRIBUTION_PLAN=JOB_DISTRIBUTION_PLAN
  CALL MPI_BCAST(DMSM_DISTRIBUTION_PLAN,1,MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)

  DMSM_TOTAL_JOBS=TOTAL_JOBS
  CALL MPI_BCAST(DMSM_TOTAL_JOBS,1,MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)

  DMSM_NUM_OF_JOBS_PER_GROUP=NUM_OF_JOBS_PER_GROUP
  CALL MPI_BCAST(DMSM_NUM_OF_JOBS_PER_GROUP,1,MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)

  DMSM_IDLING_SIGNAL=DMSM_IDLING_CONSTANT
  DMSM_TOTAL_JOB_GROUPS=(DMSM_TOTAL_JOBS-1)/DMSM_NUM_OF_JOBS_PER_GROUP+1
  DMSM_END_OF_GROUP_JOBS=-100-2*DMSM_NUM_OF_JOBS_PER_GROUP

  ALLOCATE(DMSM_JOBS_SENT_BY_MASTER(DMSM_TOTAL_JOBS))
  ALLOCATE(DMSM_JOBS_TO_THREAD_BY_MASTER(DMSM_TOTAL_JOBS))
  ALLOCATE(DMSM_NO_MORE_WORK_INSTRUCTED(DMSM_THREADS_PP_EXPECTED,DMSM_TOTAL_MPI_PROCESSES))
  ALLOCATE(DMSM_NO_MORE_GROUPS_FROM(DMSM_TOTAL_MPI_PROCESSES))
  ALLOCATE(DMSM_NO_MORE_GROUPS_TO(DMSM_TOTAL_MPI_PROCESSES))
  ALLOCATE(DMSM_NO_MORE_WORK_RECEIVED(DMSM_THREADS_PP_EXPECTED,DMSM_TOTAL_MPI_PROCESSES))
  ALLOCATE(DMSM_SUM_NO_MORE_WORK_RECEIVED(DMSM_THREADS_PP_EXPECTED,DMSM_TOTAL_MPI_PROCESSES))
  ALLOCATE(DMSM_JOB_I_DID(    DMSM_JOB_RECORD_LENTH,DMSM_TOTAL_JOBS))
  ALLOCATE(DMSM_SUM_JOB_I_DID(DMSM_JOB_RECORD_LENTH,DMSM_TOTAL_JOBS))
  ALLOCATE(DMSM_CPU_TIME_OF_THREADS(DMSM_THREADS_PP_EXPECTED))
  ALLOCATE(DMSM_CPU_TIME_OF_ALL(DMSM_THREADS_PP_EXPECTED,DMSM_TOTAL_MPI_PROCESSES))
  ALLOCATE(DMSM_CPU_TIME_OF_JOBS(DMSM_TOTAL_JOBS))
  ALLOCATE(DMSM_JOB_GROUP_INFORMATION(DMSM_GROUP_INFORMATION_SIZE,DMSM_TOTAL_JOBS))
  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     ALLOCATE(DMSM_CPU_TIME_OF_ALL_JOBS(DMSM_TOTAL_JOBS))
  ELSE
     ALLOCATE(DMSM_CPU_TIME_OF_ALL_JOBS(1))
  END IF
  ALLOCATE(DMSM_JOBS_COMPLETED_BY_THREADS(DMSM_THREADS_PP_EXPECTED))
  ALLOCATE(DMSM_JOBS_COMPLETED_ALL(DMSM_THREADS_PP_EXPECTED,DMSM_TOTAL_MPI_PROCESSES))
  ALLOCATE(DMSM_INTEGER_SIGNALS_BIG(DMSM_NUM_OF_JOBS_PER_GROUP))
  ALLOCATE(DMSM_INTEGER_SIGNALS_SMALL(0:DMSM_THREADS_PP_EXPECTED))
  ALLOCATE(DMSM_JOB_INITIAL_DATA_LOCKS(0:DMSM_THREADS_PP_EXPECTED))

  CALL OMP_INIT_LOCK(DMSM_JOB_NODE_RESULT_LOCK)
  CALL OMP_INIT_LOCK(DMSM_JOB_FINAL_RESULT_LOCK)
  DO I=0, DMSM_THREADS_PP_EXPECTED
     CALL OMP_INIT_LOCK(DMSM_JOB_INITIAL_DATA_LOCKS(I))
  END DO

  IF((DMSM_DISTRIBUTION_PLAN.LT.11) .OR. (33.LT.DMSM_DISTRIBUTION_PLAN)) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR JOB DISTRIBUTION PLAN: ",DMSM_DISTRIBUTION_PLAN)
  IF(DMSM_TOTAL_JOBS.LT.1) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR TOTAL NUMBER OF JOBS: ",DMSM_TOTAL_JOBS)
  IF(DMSM_NUM_OF_JOBS_PER_GROUP.LT.1) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR NUMBER OF JOBS PER GROUP: ",DMSM_NUM_OF_JOBS_PER_GROUP)

  IF(DMSM_TOTAL_MPI_PROCESSES.LT.1) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR NUMBER OF PROCESSES: ",DMSM_TOTAL_MPI_PROCESSES)
  IF(DMSM_THREADS_PP_EXPECTED.LT.1) &
     CALL DMSM_ERROR_AND_STOP("ERROR FOR NUMBER OF THREADS PER PROCESS: ",DMSM_THREADS_PP_EXPECTED)
  !IF((DMSM_TOTAL_MPI_PROCESSES.EQ.1) .AND. (DMSM_THREADS_PP_EXPECTED.EQ.1)) &
  !   CALL DMSM_ERROR_AND_STOP("ERROR FOR NUMBER OF PROCESSES: ",DMSM_TOTAL_MPI_PROCESSES,  &
  !                               "AND NUMBER OF THREADS PER PROCESS: ",DMSM_THREADS_PP_EXPECTED)

  DMSM_JOBS_SENT_BY_MASTER=-1
  DMSM_JOBS_TO_THREAD_BY_MASTER=-1
  DMSM_NO_MORE_WORK_INSTRUCTED=0
  DMSM_NO_MORE_GROUPS_FROM=1
  DMSM_NO_MORE_GROUPS_TO=-1
  DMSM_NO_MORE_WORK_RECEIVED=0
  DMSM_SUM_NO_MORE_WORK_RECEIVED=0
  DMSM_JOB_I_DID=0
  DMSM_JOB_GROUP_INFORMATION=-1
  DMSM_SUM_JOB_I_DID=0
  DMSM_CPU_TIME_OF_THREADS=0.0D0
  DMSM_CPU_TIME_OF_ALL=0.0D0
  DMSM_CPU_TIME_OF_JOBS=0.0D0
  DMSM_CPU_TIME_OF_ALL_JOBS=0.0D0
  DMSM_JOBS_COMPLETED_BY_THREADS=0
  DMSM_JOBS_COMPLETED_ALL=0
  DMSM_INTEGER_SIGNALS_SMALL=0
  DMSM_INTEGER_SIGNALS_BIG=0
  DMSM_CURRENT_GROUP_NUMBER=0

  DMSM_PREALLOCATION_ENABLED=0
  IF((DMSM_DISTRIBUTION_PLAN.GE.20).AND.(DMSM_DISTRIBUTION_PLAN.LT.30)) THEN
      DMSM_PREALLOCATION_ENABLED=1
      IF(DMSM_THREADS_PER_PROCESS_GOT(DMSM_MASTER+1).LE.1) DMSM_PREALLOCATION_ENABLED=0
  END IF
  IF(DMSM_TOTAL_MPI_PROCESSES.LE.1) DMSM_PREALLOCATION_ENABLED=1

  IF(DMSM_PREALLOCATION_ENABLED.GT.0) THEN
     DMSM_FIRST_GROUPS_ALLOCATED=DMSM_TOTAL_JOB_GROUPS/(DMSM_TOTAL_MPI_PROCESSES*2)
     IF(DMSM_TOTAL_MPI_PROCESSES.EQ.1) DMSM_FIRST_GROUPS_ALLOCATED=DMSM_TOTAL_JOB_GROUPS
     DMSM_FIRST_JOBS_ALLOCATED=DMSM_FIRST_GROUPS_ALLOCATED*DMSM_NUM_OF_JOBS_PER_GROUP
     IF(DMSM_FIRST_JOBS_ALLOCATED.GT.DMSM_TOTAL_JOBS) DMSM_FIRST_JOBS_ALLOCATED=DMSM_TOTAL_JOBS
     DMSM_JOBS_SENT_BY_MASTER(1:DMSM_FIRST_JOBS_ALLOCATED)=DMSM_MASTER
  ELSE
     DMSM_FIRST_GROUPS_ALLOCATED=0
     DMSM_FIRST_JOBS_ALLOCATED=0
  END IF

  DMSM_JOB_RANGE(1:2)=DMSM_FIRST_JOBS_ALLOCATED
  DMSM_JOB_RANGE(3:3)=DMSM_FIRST_GROUPS_ALLOCATED

  DMSM_JOB_FROM=3
  DMSM_JOB_TO=1
  DMSM_JOB_ASSIGNED=2
  !IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
  !   DMSM_JOB_FROM=1
  !   DMSM_JOB_TO=DMSM_FIRST_JOBS_ALLOCATED
  !   DMSM_JOB_ASSIGNED=DMSM_JOB_FROM-1
  !ELSE
  !   DMSM_JOB_FROM=3
  !   DMSM_JOB_TO=1
  !   DMSM_JOB_ASSIGNED=2
  !END IF

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_FILE()
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*) "*******************************************"
     WRITE(DMSM_A_FILE_TUNNEL,*) "* Double-layer Master-Slave Model library *"
     WRITE(DMSM_A_FILE_TUNNEL,*) "* Version 4.3  by   HPCVL.org 2015        *"
     WRITE(DMSM_A_FILE_TUNNEL,*) "*******************************************"
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,"JOB DISTRIBUTION PLAN:                        ",I12)') DMSM_DISTRIBUTION_PLAN
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,"TOTAL NUMBER OF JOBS:                         ",I12)') DMSM_TOTAL_JOBS
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,"TOTAL NUMBER OF JOBS PER GROUP:               ",I12)') DMSM_NUM_OF_JOBS_PER_GROUP
     IF(DMSM_PREALLOCATION_ENABLED.GT.0) &
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,"TOTAL NUMBER OF JOBS ALLOCATED TO MASTER NODE:",I12)') DMSM_FIRST_JOBS_ALLOCATED

     IF(.NOT.DMSM_ONLY_MPI_ENABLED) THEN
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,"TOTAL NUMBER OF PROCESSES/NODES:              ",I12)') DMSM_TOTAL_MPI_PROCESSES
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,"TOTAL NUMBER OF THREADS PER PROCESS EXPECTED: ",I12)') DMSM_THREADS_PP_EXPECTED
        WRITE(DMSM_A_FILE_TUNNEL,*)"TRUE  NUMBER OF THREADS IN EACH PROCESS IN RANK ORDER: "
        WRITE(DMSM_A_FILE_TUNNEL,*)"             ", (DMSM_THREADS_PER_PROCESS_GOT(M),M=1,DMSM_TOTAL_MPI_PROCESSES)
     ELSE
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,"TOTAL NUMBER OF PROCESSES:                    ",   I12)') DMSM_ONLY_MPI_ALL_PROCESSES
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,"NUMBER OF PROCESSES PER JOB COMMUNICATOR EXPECTED:",I8)') DMSM_ONLY_MPI_JOB_PROCESSES_E
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,"TOTAL NUMBER OF JOB COMMUNICATORS GENERATED:    ", I10)') DMSM_TOTAL_MPI_PROCESSES
        WRITE(DMSM_A_FILE_TUNNEL,*)"NUMBER OF PROCESSES PER JOB COMMUNICATOR IN ORDER: "
        WRITE(DMSM_A_FILE_TUNNEL,*)"             ", (DMSM_ONLY_MPI_JOB_PROCESSES_T(M),M=1,DMSM_TOTAL_MPI_PROCESSES)
     END IF

     WRITE(DMSM_A_FILE_TUNNEL,*)
!     WRITE(DMSM_A_FILE_TUNNEL,*)"Inside DMSM lib, "
!     IF(DMSM_INITIAL_LOCKS_EMPLOYED.EQ.1) THEN
!        WRITE(DMSM_A_FILE_TUNNEL,*)"       OpenMP Locks activated "
!     ELSE if(DMSM_INITIAL_LOCKS_EMPLOYED.EQ.0) THEN
!        WRITE(DMSM_A_FILE_TUNNEL,*)"       integer signals activated "
!     ELSE
!        WRITE(DMSM_A_FILE_TUNNEL,*)"       both OpenMP Locks and integer signals used "
!     END IF
!     WRITE(DMSM_A_FILE_TUNNEL,*)"       for avoiding race conditions in job initial data. "
!     WRITE(DMSM_A_FILE_TUNNEL,*)"       This is a technique, then users can ignore. "


     WRITE(DMSM_A_FILE_TUNNEL,*)"    METHOD OF JOB GROUPING       : ", DMSM_JOB_GROUPING
     WRITE(DMSM_A_FILE_TUNNEL,*)"    RESULT COLLECTION IN PARALLEL: ", DMSM_COLLECT_RESULTS_PARALLEL

     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"*** Job (group) distribution will be checked up later. *** "
     WRITE(DMSM_A_FILE_TUNNEL,*)"*** If you do not see such a check up passing message, *** "
     WRITE(DMSM_A_FILE_TUNNEL,*)"*** the check up failed.                               *** "
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  DMSM_JOBS_COMPLETED=0
  DMSM_MY_THREAD_NUMBER=0
  CALL DMSM_GET_CPU_TIME(DMSM_BEGINNING_TIME)
  DMSM_A_TAG_FOR_MPI=DMSM_MPI_TAG_ZERO_POINT+DMSM_MY_THREAD_NUMBER+DMSM_MY_MPI_RANK*DMSM_THREADS_PP_EXPECTED
  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
     !$OMP PARALLEL DEFAULT(SHARED)
     DMSM_JOBS_COMPLETED=0
     DMSM_MY_THREAD_NUMBER=OMP_GET_THREAD_NUM()
     DMSM_A_TAG_FOR_MPI=DMSM_MPI_TAG_ZERO_POINT+DMSM_MY_THREAD_NUMBER+DMSM_MY_MPI_RANK*DMSM_THREADS_PP_EXPECTED
     IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
        CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_INITIALIZE_CORE) ", DMSM_MY_MPI_RANK)
     END IF
     CALL DMSM_CPU_TIME_IN_OPENMP(DMSM_BEGINNING_TIME)
     !$OMP END PARALLEL
  END IF

  IF (DMSM_MY_MPI_RANK .EQ. DMSM_MASTER)   THEN
      CALL DMSM_OPEN_PROBLEM_FILE()
      WRITE(DMSM_A_FILE_TUNNEL,*)' 0 '
      CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  DMSM_TOTAL_SLAVE_GONE_HOME=0
  DMSM_TOTAL_M_THREAD_GONE_HOME=0

  DMSM_ALL_JOBS_COMPLETED=.FALSE.
  DMSM_ALREADY_INITIALIZED=.TRUE.

  RETURN
END SUBROUTINE DMSM_INITIALIZE_CORE




SUBROUTINE DMSM_INITIALIZE_1(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN, &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_INITIALIZE_C_INTERFACE
  IMPLICIT NONE
  INTEGER ::               THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  CALL DMSM_INITIALIZE_CORE(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN, &
                            TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)

  RETURN
END SUBROUTINE DMSM_INITIALIZE_1




SUBROUTINE DMSM_FIND_THE_FILE_TUNNEL()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  LOGICAL :: EXISTS, BEING_USED
  INTEGER :: I
  EXISTS=.FALSE.
  BEING_USED=.TRUE.
  SEARCHING_LOOP: DO I=12, DMSM_MAXIMUM_TUNNEL+1
     inquire(I, exist=EXISTS, opened=BEING_USED)
     IF(EXISTS.AND.(.NOT.BEING_USED)) THEN
        DMSM_A_FILE_TUNNEL=I
        EXIT SEARCHING_LOOP
     END IF
  END DO SEARCHING_LOOP
  IF(I.GT.DMSM_MAXIMUM_TUNNEL) &
     CALL DMSM_ERROR_AND_STOP("FAILED IN A FILE TUNNEL SEARCH. ", DMSM_MAXIMUM_TUNNEL)
  RETURN
END SUBROUTINE DMSM_FIND_THE_FILE_TUNNEL




SUBROUTINE DMSM_FINALIZE()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER :: I
  IF(ALLOCATED(DMSM_THREADS_PER_PROCESS_GOT))   DEALLOCATE(DMSM_THREADS_PER_PROCESS_GOT)
  IF(ALLOCATED(DMSM_JOBS_SENT_BY_MASTER))       DEALLOCATE(DMSM_JOBS_SENT_BY_MASTER)
  IF(ALLOCATED(DMSM_JOBS_TO_THREAD_BY_MASTER))  DEALLOCATE(DMSM_JOBS_TO_THREAD_BY_MASTER)
  IF(ALLOCATED(DMSM_NO_MORE_WORK_INSTRUCTED))   DEALLOCATE(DMSM_NO_MORE_WORK_INSTRUCTED)
  IF(ALLOCATED(DMSM_NO_MORE_GROUPS_FROM))       DEALLOCATE(DMSM_NO_MORE_GROUPS_FROM)
  IF(ALLOCATED(DMSM_NO_MORE_GROUPS_TO))         DEALLOCATE(DMSM_NO_MORE_GROUPS_TO)
  IF(ALLOCATED(DMSM_NO_MORE_WORK_RECEIVED))     DEALLOCATE(DMSM_NO_MORE_WORK_RECEIVED)
  IF(ALLOCATED(DMSM_SUM_NO_MORE_WORK_RECEIVED)) DEALLOCATE(DMSM_SUM_NO_MORE_WORK_RECEIVED)
  IF(ALLOCATED(DMSM_JOB_I_DID))                 DEALLOCATE(DMSM_JOB_I_DID)
  IF(ALLOCATED(DMSM_SUM_JOB_I_DID))             DEALLOCATE(DMSM_SUM_JOB_I_DID)
  IF(ALLOCATED(DMSM_CPU_TIME_OF_THREADS))       DEALLOCATE(DMSM_CPU_TIME_OF_THREADS)
  IF(ALLOCATED(DMSM_CPU_TIME_OF_ALL))           DEALLOCATE(DMSM_CPU_TIME_OF_ALL)
  IF(ALLOCATED(DMSM_CPU_TIME_OF_JOBS))          DEALLOCATE(DMSM_CPU_TIME_OF_JOBS)
  IF(ALLOCATED(DMSM_CPU_TIME_OF_ALL_JOBS))      DEALLOCATE(DMSM_CPU_TIME_OF_ALL_JOBS)
  IF(ALLOCATED(DMSM_JOBS_COMPLETED_BY_THREADS)) DEALLOCATE(DMSM_JOBS_COMPLETED_BY_THREADS)
  IF(ALLOCATED(DMSM_JOBS_COMPLETED_ALL))        DEALLOCATE(DMSM_JOBS_COMPLETED_ALL)
  IF(ALLOCATED(DMSM_INTEGER_SIGNALS_BIG))       DEALLOCATE(DMSM_INTEGER_SIGNALS_BIG)
  IF(ALLOCATED(DMSM_INTEGER_SIGNALS_SMALL))     DEALLOCATE(DMSM_INTEGER_SIGNALS_SMALL)
  IF(ALLOCATED(DMSM_JOB_GROUP_INFORMATION))     DEALLOCATE(DMSM_JOB_GROUP_INFORMATION)

  CALL OMP_DESTROY_LOCK(DMSM_JOB_NODE_RESULT_LOCK)
  CALL OMP_DESTROY_LOCK(DMSM_JOB_FINAL_RESULT_LOCK)
  DO I=0, DMSM_THREADS_PP_EXPECTED
     CALL OMP_DESTROY_LOCK(DMSM_JOB_INITIAL_DATA_LOCKS(I))
  END DO
  IF(ALLOCATED(DMSM_JOB_INITIAL_DATA_LOCKS))    DEALLOCATE(DMSM_JOB_INITIAL_DATA_LOCKS)
  CALL MPI_COMM_FREE(DMSM_COMMUNICATOR, DMSM_IERR)
  DMSM_SET_COMM_CALLED=.FALSE.
  DMSM_ALL_JOBS_COMPLETED=.FALSE.
  DMSM_ALREADY_INITIALIZED=.FALSE.
  DMSM_JOURNAL_NUMBER=0
  RETURN
END SUBROUTINE DMSM_FINALIZE




SUBROUTINE DMSM_DO_NOTHING_2(I,J)
  IMPLICIT NONE
  INTEGER :: I,J
  RETURN
END SUBROUTINE DMSM_DO_NOTHING_2




SUBROUTINE DMSM_DO_NOTHING_3(I,J,K)
  IMPLICIT NONE
  INTEGER :: I,J,K
  RETURN
END SUBROUTINE DMSM_DO_NOTHING_3




SUBROUTINE DMSM_DO_NOTHING_4(I,J,K,L)
  IMPLICIT NONE
  INTEGER :: I,J,K,L
  RETURN
END SUBROUTINE DMSM_DO_NOTHING_4




SUBROUTINE DMSM_WORKING_CORE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_LEVEL_E_INTERFACE
  USE DMSM_LAST_RE_COLL_INTERFACE
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  LOGICAL :: RESULT_COLLECTION_ENABLED

  IF(.NOT.DMSM_ALREADY_INITIALIZED) THEN
     CALL DMSM_ERROR_AND_STOP("SORRY! DMSM LIB MUST BE INITIALIZED BEFORE USED: ", -1)
  END IF

  DMSM_RESULT_COLLECTION_ENABLED=RESULT_COLLECTION_ENABLED

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(*,*)"DMSM_WORKING_CORE working:     ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"DMSM_WORKING_CORE working:     ",DMSM_DISTRIBUTION_PLAN, &
                                DMSM_TOTAL_MPI_PROCESSES,DMSM_THREADS_PP_EXPECTED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     CLOSE(DMSM_A_FILE_TUNNEL)
  END IF

  IF(DMSM_TOTAL_MPI_PROCESSES.EQ.1) THEN
     IF(DMSM_THREADS_PP_EXPECTED.EQ.1) THEN
        CALL DMSM_SERIAL_VERSION(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        CALL DMSM_PURE_OPENMP_MODEL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     END IF

  ELSE

     IF(DMSM_THREADS_PP_EXPECTED.EQ.1) THEN
        CALL DMSM_PURE_MPI_MODEL(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
     ELSE
        SELECT CASE (DMSM_DISTRIBUTION_PLAN)
        CASE (11)
           CALL DMSM_MODEL_PLAN11(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (12)
           CALL DMSM_MODEL_PLAN12(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (13)
           CALL DMSM_MODEL_PLAN13(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (21)
           CALL DMSM_MODEL_PLAN21(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (22)
           CALL DMSM_MODEL_PLAN22(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (23)
           CALL DMSM_MODEL_PLAN23(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (31)
           CALL DMSM_MODEL_PLAN31(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (32)
           CALL DMSM_MODEL_PLAN32(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE (33)
           CALL DMSM_MODEL_PLAN33(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION)
        CASE DEFAULT
           CALL DMSM_ERROR_AND_STOP("UNACCEPTABLE PLAN VALUE: ", DMSM_DISTRIBUTION_PLAN)
        END SELECT
     END IF

  END IF

  DMSM_ALL_JOBS_COMPLETED=.TRUE.

  CALL DMSM_LAST_RESULT_COLLECTION(RESULT_COLLECTION)

  RETURN
END SUBROUTINE DMSM_WORKING_CORE




SUBROUTINE DMSM_LAST_RESULT_COLLECTION(RESULT_COLLECTION)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  INTEGER :: I,J

  IF((DMSM_COLLECT_RESULTS_PARALLEL.EQ.1).AND.(DMSM_THREADS_PP_EXPECTED.GE.2)) THEN
     !$OMP PARALLEL DEFAULT(SHARED)
        IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
        IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
           CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_LAST_RESULT_COLLECTION) ", DMSM_MY_MPI_RANK)
        END IF
        END IF

        CALL DMSM_SET_FINAL_RESULT_LOCK()
        IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
           IF(DMSM_MY_THREAD_NUMBER.EQ.DMSM_ALWAYS_ZERO) THEN
              MASTER_COLLECT: DO I=0, DMSM_TOTAL_MPI_PROCESSES-1
                 IF(I.EQ.DMSM_MASTER) CYCLE MASTER_COLLECT
                 DO J=1,DMSM_THREADS_PER_PROCESS_GOT(I+1)
                    CALL RESULT_COLLECTION(DMSM_MY_MPI_RANK,I)
                 END DO
              END DO MASTER_COLLECT
              CALL RESULT_COLLECTION(DMSM_MY_MPI_RANK,DMSM_MY_MPI_RANK)
           ELSE
              CALL RESULT_COLLECTION(DMSM_MY_MPI_RANK,DMSM_MY_MPI_RANK)
           END IF
        ELSE
           CALL RESULT_COLLECTION(DMSM_MY_MPI_RANK,DMSM_MY_MPI_RANK)
        END IF
        CALL DMSM_UNSET_FINAL_RESULT_LOCK()
   !$OMP END PARALLEL

  ELSE
      !IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) CALL RESULT_COLLECTION(DMSM_MY_MPI_RANK,DMSM_MASTER)
      CALL MPI_BARRIER(DMSM_COMMUNICATOR, DMSM_IERR)
      DO I=0, DMSM_TOTAL_MPI_PROCESSES-1
         IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
            CALL RESULT_COLLECTION(DMSM_MY_MPI_RANK,I)
         ELSE
           IF(DMSM_MY_MPI_RANK.EQ.I) CALL RESULT_COLLECTION(DMSM_MY_MPI_RANK,I)
         END IF
         CALL MPI_BARRIER(DMSM_COMMUNICATOR, DMSM_IERR)
      END DO
  END IF

  RETURN
END SUBROUTINE DMSM_LAST_RESULT_COLLECTION



SUBROUTINE DMSM_WORKING_1(DO_THE_JOB)
  USE DMSM_WORKING_CORE_INTERFACE
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
  DMSM_INITIAL_DATA_PREP_ENABLED=.FALSE.
  CALL DMSM_WORKING_CORE(DO_THE_JOB, DMSM_DO_NOTHING_4, DMSM_DO_NOTHING_2, .FALSE.)
  RETURN
END SUBROUTINE DMSM_WORKING_1




SUBROUTINE DMSM_WORKING_2(DO_THE_JOB, JOB_GROUP_PREPARATION)
  USE DMSM_WORKING_CORE_INTERFACE
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
  DMSM_INITIAL_DATA_PREP_ENABLED=.TRUE.
  CALL DMSM_WORKING_CORE(DO_THE_JOB, JOB_GROUP_PREPARATION, DMSM_DO_NOTHING_2, .FALSE.)
  RETURN
END SUBROUTINE DMSM_WORKING_2




SUBROUTINE DMSM_WORKING_3(DO_THE_JOB, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_WORKING_CORE_INTERFACE
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  LOGICAL :: RESULT_COLLECTION_ENABLED
  DMSM_INITIAL_DATA_PREP_ENABLED=.FALSE.
  CALL DMSM_WORKING_CORE(DO_THE_JOB, DMSM_DO_NOTHING_4, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  RETURN
END SUBROUTINE DMSM_WORKING_3




SUBROUTINE DMSM_WORKING_4(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_WORKING_CORE_INTERFACE
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE
  LOGICAL :: RESULT_COLLECTION_ENABLED
  DMSM_INITIAL_DATA_PREP_ENABLED=.TRUE.
  CALL DMSM_WORKING_CORE(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  RETURN
END SUBROUTINE DMSM_WORKING_4




SUBROUTINE DMSM_SET_INITIAL_LOCK()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  IF(.NOT.DMSM_INITIAL_DATA_PREP_ENABLED) RETURN
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  IF(DMSM_INITIAL_LOCKS_EMPLOYED.NE.0) THEN
    !CALL OMP_UNSET_LOCK(DMSM_JOB_INITIAL_DATA_LOCKS(DMSM_MY_THREAD_NUMBER))
    !$OMP FLUSH(DMSM_JOB_INITIAL_DATA_LOCKS)
     CALL OMP_SET_LOCK(DMSM_JOB_INITIAL_DATA_LOCKS(DMSM_MY_THREAD_NUMBER))
    !$OMP FLUSH(DMSM_JOB_INITIAL_DATA_LOCKS)
  END IF
  IF(DMSM_INITIAL_LOCKS_EMPLOYED.NE.1) THEN
     DMSM_INTEGER_SIGNALS_SMALL(DMSM_MY_THREAD_NUMBER)=1
     IF(OMP_IN_PARALLEL()) THEN
        !$OMP flush(DMSM_INTEGER_SIGNALS_SMALL)
     END IF
     DMSM_INTEGER_SIGNALS_SMALL(DMSM_MY_THREAD_NUMBER)=      &
          DMSM_INTEGER_SIGNALS_SMALL(DMSM_MY_THREAD_NUMBER)+1
     DMSM_INTEGER_SIGNALS_SMALL(DMSM_MY_THREAD_NUMBER)=      &
          DMSM_INTEGER_SIGNALS_SMALL(DMSM_MY_THREAD_NUMBER)+1
     IF(OMP_IN_PARALLEL()) THEN
        !$OMP flush(DMSM_INTEGER_SIGNALS_SMALL)
     END IF
  END IF
  RETURN
END SUBROUTINE DMSM_SET_INITIAL_LOCK




SUBROUTINE DMSM_UNSET_AN_INITIAL_LOCK()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  IF(.NOT.DMSM_INITIAL_DATA_PREP_ENABLED) RETURN
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  IF(DMSM_INITIAL_LOCKS_EMPLOYED.NE.0) THEN
    !$OMP FLUSH(DMSM_JOB_INITIAL_DATA_LOCKS)
     CALL OMP_UNSET_LOCK(DMSM_JOB_INITIAL_DATA_LOCKS(DMSM_MY_THREAD_NUMBER))
    !$OMP FLUSH(DMSM_JOB_INITIAL_DATA_LOCKS)
  END IF
  IF(DMSM_INITIAL_LOCKS_EMPLOYED.NE.1) THEN
     DMSM_INTEGER_SIGNALS_SMALL(DMSM_MY_THREAD_NUMBER)=0
  END IF
  RETURN
END SUBROUTINE DMSM_UNSET_AN_INITIAL_LOCK




SUBROUTINE DMSM_WAIT_FOR_INITIAL_LOCKS()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER  :: I
  IF(.NOT.DMSM_INITIAL_DATA_PREP_ENABLED) RETURN
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  IF(DMSM_INITIAL_LOCKS_EMPLOYED.NE.0) THEN
     DO I=0, DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)-1
       !$OMP FLUSH(DMSM_JOB_INITIAL_DATA_LOCKS)
        CALL OMP_SET_LOCK(DMSM_JOB_INITIAL_DATA_LOCKS(I))
       !$OMP FLUSH(DMSM_JOB_INITIAL_DATA_LOCKS)
        CALL OMP_UNSET_LOCKF(DMSM_JOB_INITIAL_DATA_LOCKS(I))
       !$OMP FLUSH(DMSM_JOB_INITIAL_DATA_LOCKS)
     END DO
  END IF
  IF(DMSM_INITIAL_LOCKS_EMPLOYED.NE.1) THEN
     DO I=0, DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)-1
        CHECKING_DO: DO
           IF(DMSM_INTEGER_SIGNALS_SMALL(I).EQ.0) EXIT CHECKING_DO
           IF(OMP_IN_PARALLEL()) THEN
              !$OMP flush(DMSM_INTEGER_SIGNALS_SMALL)
           END IF
        END DO CHECKING_DO
     END DO
  END IF
  RETURN
END SUBROUTINE DMSM_WAIT_FOR_INITIAL_LOCKS





SUBROUTINE DMSM_SET_NODE_RESULT_LOCK()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  !$OMP FLUSH(DMSM_JOB_NODE_RESULT_LOCK)
  CALL OMP_SET_LOCK(DMSM_JOB_NODE_RESULT_LOCK)
  !$OMP FLUSH(DMSM_JOB_NODE_RESULT_LOCK)
  RETURN
END SUBROUTINE DMSM_SET_NODE_RESULT_LOCK




SUBROUTINE DMSM_UNSET_NODE_RESULT_LOCK()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  !$OMP FLUSH(DMSM_JOB_NODE_RESULT_LOCK)
  CALL OMP_UNSET_LOCK(DMSM_JOB_NODE_RESULT_LOCK)
  !$OMP FLUSH(DMSM_JOB_NODE_RESULT_LOCK)
  RETURN
END SUBROUTINE DMSM_UNSET_NODE_RESULT_LOCK




SUBROUTINE DMSM_SET_FINAL_RESULT_LOCK()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  !$OMP FLUSH(DMSM_JOB_FINAL_RESULT_LOCK)
  CALL OMP_SET_LOCK(DMSM_JOB_FINAL_RESULT_LOCK)
  !$OMP FLUSH(DMSM_JOB_FINAL_RESULT_LOCK)
  RETURN
END SUBROUTINE DMSM_SET_FINAL_RESULT_LOCK




SUBROUTINE DMSM_UNSET_FINAL_RESULT_LOCK()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  !$OMP FLUSH(DMSM_JOB_FINAL_RESULT_LOCK)
  CALL OMP_UNSET_LOCK(DMSM_JOB_FINAL_RESULT_LOCK)
  !$OMP FLUSH(DMSM_JOB_FINAL_RESULT_LOCK)
  RETURN
END SUBROUTINE DMSM_UNSET_FINAL_RESULT_LOCK




LOGICAL FUNCTION DMSM_ALL_JOBS_DONE()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  DMSM_ALL_JOBS_DONE=DMSM_ALL_JOBS_COMPLETED
  RETURN
END FUNCTION DMSM_ALL_JOBS_DONE




SUBROUTINE DMSM_SET_JOB_SIGNALS(GROUP_NUMBER)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER:: GROUP_NUMBER
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  IF((GROUP_NUMBER.LT.1).OR.(GROUP_NUMBER.GT.DMSM_TOTAL_JOB_GROUPS)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR OF JOB GROUP: ", GROUP_NUMBER, " WITH TOTAL ",DMSM_TOTAL_JOB_GROUPS)
  END IF
  DMSM_INTEGER_SIGNALS_BIG=0
  IF(GROUP_NUMBER.LT.DMSM_TOTAL_JOB_GROUPS) THEN
     DMSM_TOTAL_JOB_SIGNALS_BIG=DMSM_NUM_OF_JOBS_PER_GROUP
  ELSE
     DMSM_TOTAL_JOB_SIGNALS_BIG=DMSM_TOTAL_JOBS-((DMSM_TOTAL_JOBS-1)/DMSM_NUM_OF_JOBS_PER_GROUP)*&
                                                                     DMSM_NUM_OF_JOBS_PER_GROUP
  END IF
  IF(DMSM_TOTAL_JOB_SIGNALS_BIG.GT.0) DMSM_INTEGER_SIGNALS_BIG(1:DMSM_TOTAL_JOB_SIGNALS_BIG)=1
  IF(OMP_IN_PARALLEL()) THEN
     !$OMP flush(DMSM_INTEGER_SIGNALS_BIG)
  END IF
  IF(DMSM_TOTAL_JOB_SIGNALS_BIG.GT.0) DMSM_INTEGER_SIGNALS_BIG(1:DMSM_TOTAL_JOB_SIGNALS_BIG)=1
  IF(OMP_IN_PARALLEL()) THEN
     !$OMP flush(DMSM_INTEGER_SIGNALS_BIG)
  END IF
  RETURN
END SUBROUTINE DMSM_SET_JOB_SIGNALS




SUBROUTINE DMSM_UNSET_A_JOB_SIGNAL(JOB_NUMBER)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER:: JOB_NUMBER
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  IF((JOB_NUMBER.LT.1).OR.(JOB_NUMBER.GT.DMSM_TOTAL_JOBS)) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR OF JOB NUMBER: ", JOB_NUMBER, " WITH TOTAL ",DMSM_TOTAL_JOBS)
  END IF
  DMSM_INTEGER_SIGNALS_BIG(JOB_NUMBER-((JOB_NUMBER-1)/DMSM_NUM_OF_JOBS_PER_GROUP)*&
                                                      DMSM_NUM_OF_JOBS_PER_GROUP)=0
  RETURN
END SUBROUTINE DMSM_UNSET_A_JOB_SIGNAL




SUBROUTINE DMSM_WAIT_FOR_JOB_SIGNALS()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER:: I
  IF(.NOT.OMP_IN_PARALLEL()) RETURN
  DO I=1, DMSM_TOTAL_JOB_SIGNALS_BIG
     CHECKING_DO: DO
        IF(DMSM_INTEGER_SIGNALS_BIG(I).EQ.0) EXIT CHECKING_DO
        IF(OMP_IN_PARALLEL()) THEN
           !$OMP flush(DMSM_INTEGER_SIGNALS_BIG)
        END IF
     END DO CHECKING_DO
  END DO
  RETURN
END SUBROUTINE DMSM_WAIT_FOR_JOB_SIGNALS




INTEGER FUNCTION DMSM_GET_DISTRIBUTION_PLAN()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_DISTRIBUTION_PLAN=DMSM_DISTRIBUTION_PLAN
  RETURN
END FUNCTION DMSM_GET_DISTRIBUTION_PLAN




INTEGER FUNCTION DMSM_GET_TAG()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_TAG=DMSM_A_TAG_FOR_MPI
  RETURN
END FUNCTION DMSM_GET_TAG




INTEGER FUNCTION DMSM_GET_GROUP_START(JOB_NUMBER)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER:: JOB_NUMBER
  IF(DMSM_JOB_GROUP_INFORMATION(1,JOB_NUMBER).LT.1) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR : JOB NUMBER ", JOB_NUMBER, &
                                 " IS NOT ASSIGNED TO RANK ", DMSM_MY_MPI_RANK, &
                                 ", BUT REQUESTED FOR GROUP INFORMATION (START) ", DMSM_MY_THREAD_NUMBER)
  END IF
  DMSM_GET_GROUP_START=DMSM_JOB_GROUP_INFORMATION(1,JOB_NUMBER)
  RETURN
END FUNCTION DMSM_GET_GROUP_START




INTEGER FUNCTION DMSM_GET_GROUP_END(JOB_NUMBER)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER:: JOB_NUMBER
  IF(DMSM_JOB_GROUP_INFORMATION(1,JOB_NUMBER).LT.1) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR : JOB NUMBER ", JOB_NUMBER, &
                                 " IS NOT ASSIGNED TO RANK ", DMSM_MY_MPI_RANK, &
                                 ", BUT REQUESTED FOR GROUP INFORMATION (END) ", DMSM_MY_THREAD_NUMBER)
  END IF
  DMSM_GET_GROUP_END=DMSM_JOB_GROUP_INFORMATION(2,JOB_NUMBER)
  RETURN
END FUNCTION DMSM_GET_GROUP_END




INTEGER FUNCTION DMSM_GET_GROUP_NUMBER(JOB_NUMBER)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER:: JOB_NUMBER
  IF(DMSM_JOB_GROUP_INFORMATION(1,JOB_NUMBER).LT.1) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR : JOB NUMBER ", JOB_NUMBER, &
                                 " IS NOT ASSIGNED TO RANK ", DMSM_MY_MPI_RANK, &
                                 ", BUT REQUESTED FOR GROUP INFORMATION (NUMBER) ", DMSM_MY_THREAD_NUMBER)
  END IF
  DMSM_GET_GROUP_NUMBER=DMSM_JOB_GROUP_INFORMATION(3,JOB_NUMBER)
  RETURN
END FUNCTION DMSM_GET_GROUP_NUMBER




SUBROUTINE DMSM_JOB_DISTRIBUTION_CHECKUP()
#ifdef DMSM_FORTRAN_RS16_TIMER
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
#else
  USE DMSM_CTIMER_INTERFACE
#endif
#endif
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_ERROR_NOT_STOP_INTERF
  IMPLICIT NONE
  INTEGER:: I,J,K,L,M,N(10),OCC,GPI,JMAX,JMIN
  REAL   :: TM0, TM1, TM2, TMT, TMM0, TMM1, TMM2, T9MM0

  IF(.NOT.DMSM_ALL_JOBS_COMPLETED) THEN
     CALL DMSM_ERROR_AND_STOP("SORRY! ALL JOBS MUST BE COMPLETED BEFORE CHECKED UP: ", -1)
  END IF

  DMSM_SUM_NO_MORE_WORK_RECEIVED=0
  IF(DMSM_TOTAL_MPI_PROCESSES.GT.1) THEN
     CALL MPI_REDUCE(DMSM_NO_MORE_WORK_RECEIVED, DMSM_SUM_NO_MORE_WORK_RECEIVED, &
                     DMSM_TOTAL_MPI_PROCESSES*DMSM_THREADS_PP_EXPECTED, &
                     MPI_INTEGER, MPI_SUM, DMSM_MASTER, DMSM_COMMUNICATOR, DMSM_IERR)
  ELSE
     DMSM_SUM_NO_MORE_WORK_RECEIVED=DMSM_NO_MORE_WORK_RECEIVED
  END IF

  DMSM_SUM_JOB_I_DID=0
  IF(DMSM_TOTAL_MPI_PROCESSES.GT.1) THEN
     CALL MPI_REDUCE(DMSM_JOB_I_DID, DMSM_SUM_JOB_I_DID,               &
                     DMSM_JOB_RECORD_LENTH*DMSM_TOTAL_JOBS,                 &
                     MPI_INTEGER, MPI_SUM, DMSM_MASTER, DMSM_COMMUNICATOR, DMSM_IERR)
  ELSE
     DMSM_SUM_JOB_I_DID=DMSM_JOB_I_DID
  END IF

  DMSM_CPU_TIME_OF_ALL_JOBS=0
  IF(DMSM_TOTAL_MPI_PROCESSES.GT.1) THEN
#ifdef DMSM_FORTRAN_RS16_TIMER
     CALL MPI_REDUCE(DMSM_CPU_TIME_OF_JOBS, DMSM_CPU_TIME_OF_ALL_JOBS, 2*DMSM_TOTAL_JOBS, &
                     MPI_DOUBLE_PRECISION, MPI_SUM, DMSM_MASTER, DMSM_COMMUNICATOR, DMSM_IERR)
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
    CALL MPI_REDUCE(DMSM_CPU_TIME_OF_JOBS, DMSM_CPU_TIME_OF_ALL_JOBS, DMSM_TOTAL_JOBS, &
                     MPI_DOUBLE_PRECISION, MPI_SUM, DMSM_MASTER, DMSM_COMMUNICATOR, DMSM_IERR)
#else
    CALL MPI_REDUCE(DMSM_CPU_TIME_OF_JOBS, DMSM_CPU_TIME_OF_ALL_JOBS, DMSM_TOTAL_JOBS, &
                     MPI_DOUBLE_PRECISION, MPI_SUM, DMSM_MASTER, DMSM_COMMUNICATOR, DMSM_IERR)
#endif
#endif
  ELSE
     DMSM_CPU_TIME_OF_ALL_JOBS=DMSM_CPU_TIME_OF_JOBS
  END IF


  IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
     !$OMP PARALLEL DEFAULT(SHARED)
     IF(DMSM_THREADS_PP_EXPECTED.GE.2) THEN
     IF(OMP_GET_NUM_THREADS().NE.DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1)) THEN
        CALL DMSM_ERROR_AND_STOP("ERROR FOR DISCRIPANCY OF TOTAL THREADS: ",OMP_GET_NUM_THREADS(), &
                                 " AND ", DMSM_THREADS_PER_PROCESS_GOT(DMSM_MY_MPI_RANK+1), &
                                 " IN THE RANK (DMSM_JOB_DISTRIBUTION_CHECKUP) ", DMSM_MY_MPI_RANK)
     END IF
     END IF
     DMSM_JOBS_COMPLETED_BY_THREADS(DMSM_MY_THREAD_NUMBER+1)=DMSM_JOBS_COMPLETED
     !$OMP END PARALLEL
  ELSE
     DMSM_JOBS_COMPLETED_BY_THREADS(DMSM_MY_THREAD_NUMBER+1)=DMSM_JOBS_COMPLETED
  END IF

  DMSM_JOBS_COMPLETED_ALL=0
  IF(DMSM_TOTAL_MPI_PROCESSES.GT.1) THEN
     CALL MPI_GATHER(DMSM_JOBS_COMPLETED_BY_THREADS,DMSM_THREADS_PP_EXPECTED,MPI_INTEGER, &
                     DMSM_JOBS_COMPLETED_ALL,DMSM_THREADS_PP_EXPECTED,                         &
                             MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)
     CALL MPI_BCAST( DMSM_JOBS_COMPLETED_ALL,DMSM_THREADS_PP_EXPECTED*DMSM_TOTAL_MPI_PROCESSES,&
                             MPI_INTEGER,DMSM_MASTER,DMSM_COMMUNICATOR,DMSM_IERR)
  ELSE
     DMSM_JOBS_COMPLETED_ALL(1:DMSM_THREADS_PP_EXPECTED,1)=DMSM_JOBS_COMPLETED_BY_THREADS(1:DMSM_THREADS_PP_EXPECTED)
  END IF

  IF (DMSM_MY_MPI_RANK .EQ. DMSM_MASTER)   THEN

      DO  I = 1, DMSM_TOTAL_JOBS
          GPI=(I-1)/DMSM_NUM_OF_JOBS_PER_GROUP+1
          IF((I.LE.DMSM_FIRST_JOBS_ALLOCATED).AND.(DMSM_SUM_JOB_I_DID(2,I).NE.DMSM_MASTER)) THEN
              CALL DMSM_ERROR_NOT_STOP("JOB # ", I, " SHOULD BE DONE BY PROCESS 0, BUT BY ", &
                                                  DMSM_SUM_JOB_I_DID(2,I))
          END IF
          IF((DMSM_DISTRIBUTION_PLAN.GE.20) .AND. (DMSM_DISTRIBUTION_PLAN.LT.30))   THEN
          IF((I.GT.DMSM_FIRST_JOBS_ALLOCATED).AND.(DMSM_SUM_JOB_I_DID(2,I).EQ.DMSM_MASTER)) THEN
              CALL DMSM_ERROR_NOT_STOP("JOB # ", I, " SHOULD NOT BE DONE BY PROCESS 0, BUT BY ", &
                                                 DMSM_SUM_JOB_I_DID(2,I))
          END IF
          END IF
          IF(DMSM_SUM_JOB_I_DID(2,I).NE.DMSM_JOBS_SENT_BY_MASTER(I)) THEN
             CALL DMSM_ERROR_NOT_STOP("JOB # ", I, " WAS ASSIGNED TO PROCESS ", &
                                          DMSM_JOBS_SENT_BY_MASTER(I)," BY THE MASTER, BUT DONE BY ", &
                                          DMSM_SUM_JOB_I_DID(2,I))
          END IF
          IF((DMSM_JOBS_TO_THREAD_BY_MASTER(I).GE.0).AND. &
             (DMSM_JOBS_TO_THREAD_BY_MASTER(I).NE.DMSM_SUM_JOB_I_DID(3,I))) THEN
              CALL DMSM_ERROR_NOT_STOP("JOB # ", I, " WAS ASSIGNED TO THREAD # ", &
                                       DMSM_JOBS_TO_THREAD_BY_MASTER(I), " BUT DONE BY ", &
                                       DMSM_SUM_JOB_I_DID(3,I), &
                                       " OF PROCESS ", DMSM_SUM_JOB_I_DID(2,I))
          END IF

          IF(DMSM_SUM_JOB_I_DID(1,I).NE.1) THEN
             CALL DMSM_ERROR_NOT_STOP("JOB # ", I, " HAS NOT BEEN DONE ONCE ONLY, BUT  ", &
                                      DMSM_SUM_JOB_I_DID(1,I)," TIMES  ", DMSM_SUM_JOB_I_DID(1,I), &
                                      " OF PROCESS ", DMSM_SUM_JOB_I_DID(2,I))
          END IF
      END DO

      DO M=1, DMSM_TOTAL_MPI_PROCESSES
         DO I=1, DMSM_THREADS_PP_EXPECTED
              K=0
              IF((DMSM_NO_MORE_GROUPS_FROM(M).LE.I).AND.(I.LE.DMSM_NO_MORE_GROUPS_TO(M))) K=1;
              IF(DMSM_NO_MORE_WORK_INSTRUCTED(I,M) .NE. K) THEN
                 CLOSE(14)
                 CALL DMSM_ERROR_NOT_STOP("ERROR: THE MASTER SENT (", I-1,",", M-1, &
                                             ") NO-MORE-JOB SIGNAL ", &
                                             DMSM_NO_MORE_WORK_INSTRUCTED(I,M), " TIMES ", &
                                             DMSM_NO_MORE_WORK_INSTRUCTED(I,M))
              END IF
              IF(DMSM_SUM_NO_MORE_WORK_RECEIVED(I,M) .NE. K) THEN
                 CLOSE(14)
                 CALL DMSM_ERROR_NOT_STOP("ERROR: THE THREAD (", I-1,",", M-1, &
                                             ") RECEIVED NO-MORE-JOB SIGNAL ", &
                                             DMSM_NO_MORE_WORK_INSTRUCTED(I,M), " TIMES ", &
                                             DMSM_NO_MORE_WORK_INSTRUCTED(I,M))
              END IF
          END DO
       END DO

  END IF


  DMSM_CPU_TIME_OF_ALL=0.0D0

  IF(DMSM_MY_MPI_RANK.EQ.DMSM_MASTER) THEN
     WRITE(*,*)
     WRITE(*,*)
     WRITE(*,*)"The followings have been verified:"
     WRITE(*,*)"      all jobs were done once and only ONCE              ;"
     WRITE(*,*)"      no unwanted jobs were performed                    ;"
     WRITE(*,*)"      group jobs sent to processes were done in them     ;"
     WRITE(*,*)"      the first job of any distributed group was done     "
     WRITE(*,*)"                  by the assigned thread, if applicable  ;"
     WRITE(*,*)"      the master instructed all threads no-more-job ONCE  "
     WRITE(*,*)"                  at the end when true, if applicable    ;"
     WRITE(*,*)"      all threads received no-more-job ONCE at the end    "
     WRITE(*,*)"                  when it became true,      if applicable."
     WRITE(*,*)
     WRITE(*,*)
     CALL DMSM_OPEN_JOURNAL_APPEND()
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"The followings have been verified:"
     WRITE(DMSM_A_FILE_TUNNEL,*)"      all jobs were done once and only ONCE              ;"
     WRITE(DMSM_A_FILE_TUNNEL,*)"      no unwanted jobs were performed                    ;"
     WRITE(DMSM_A_FILE_TUNNEL,*)"      group jobs sent to processes were done in them     ;"
     WRITE(DMSM_A_FILE_TUNNEL,*)"      the first job of any distributed group was done     "
     WRITE(DMSM_A_FILE_TUNNEL,*)"                  by the assigned thread, if applicable  ;"
     WRITE(DMSM_A_FILE_TUNNEL,*)"      the master instructed all threads no-more-job ONCE  "
     WRITE(DMSM_A_FILE_TUNNEL,*)"                  at the end when true, if applicable    ;"
     WRITE(DMSM_A_FILE_TUNNEL,*)"      all threads received no-more-job ONCE at the end    "
     WRITE(DMSM_A_FILE_TUNNEL,*)"                  when it became true,      if applicable."
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)

     IF(DMSM_JOBS_DONE_BY_WHOM_SWITCH.EQ.1) &
        WRITE(DMSM_A_FILE_TUNNEL,*)"Job number, by Thread,  Rank/Node, Times, CPU-Time (seconds): "
     DO  I = 1, DMSM_TOTAL_JOBS
	        IF(DMSM_JOBS_DONE_BY_WHOM_SWITCH.EQ.1) &
            WRITE(DMSM_A_FILE_TUNNEL,'(1X,I10,1X,I10,2X,I10,1X,I6,E18.6)') I, DMSM_SUM_JOB_I_DID(3,I)&
                                                    ,DMSM_SUM_JOB_I_DID(2,I), DMSM_SUM_JOB_I_DID(1,I)&
                                                    ,DMSM_CPU_TIME_OF_ALL_JOBS(I)
            J=DMSM_SUM_JOB_I_DID(3,I)
            M=DMSM_SUM_JOB_I_DID(2,I)
            IF((DMSM_SUM_JOB_I_DID(2,I).LT.0).OR.(DMSM_SUM_JOB_I_DID(2,I).GE.DMSM_TOTAL_MPI_PROCESSES)) &
               CALL DMSM_ERROR_NOT_STOP("ERROR FOR RANK IN DMSM_JOB_DISTRIBUTION_CHECKUP: ",DMSM_SUM_JOB_I_DID(2,I))
            IF((DMSM_SUM_JOB_I_DID(3,I).LT.0).OR.(DMSM_SUM_JOB_I_DID(3,I).GE.DMSM_THREADS_PER_PROCESS_GOT(M+1))) &
               CALL DMSM_ERROR_NOT_STOP("ERROR FOR THREAD NUMBER IN DMSM_JOB_DISTRIBUTION_CHECKUP: ",   &
                                        DMSM_SUM_JOB_I_DID(3,I),' IN RANK ',M)
     END DO

     TM0 = 0.0D0
     TMM0= -5.0D0
     JMAX=-1
     JMIN=-1
     DO  I = 1, DMSM_TOTAL_JOBS
            IF(TM0.LT.DMSM_CPU_TIME_OF_ALL_JOBS(I)) THEN
               TM0=DMSM_CPU_TIME_OF_ALL_JOBS(I)
               JMAX=I
            END IF
            IF(TMM0.LT.0.0D0) THEN
               TMM0=DMSM_CPU_TIME_OF_ALL_JOBS(I)
               JMIN=I
            ELSE IF(TMM0.GT.DMSM_CPU_TIME_OF_ALL_JOBS(I)) THEN
               TMM0=DMSM_CPU_TIME_OF_ALL_JOBS(I)
               JMIN=I
            END IF
            J=DMSM_SUM_JOB_I_DID(3,I)
            M=DMSM_SUM_JOB_I_DID(2,I)
            DMSM_CPU_TIME_OF_ALL(J+1,M+1)=DMSM_CPU_TIME_OF_ALL(J+1,M+1)+DMSM_CPU_TIME_OF_ALL_JOBS(I)
     END DO

     IF(DMSM_JOBS_DONE_BY_WHOM_SWITCH.EQ.1) THEN
        WRITE(DMSM_A_FILE_TUNNEL,*)'The maximum CPU-Time of the above is ', TM0, ' seconds of job # ', JMAX, ' ,'
        WRITE(DMSM_A_FILE_TUNNEL,*)'and the minimum is ', TMM0,' seconds of job # ',JMIN,' .'
        WRITE(DMSM_A_FILE_TUNNEL,*)
        WRITE(DMSM_A_FILE_TUNNEL,*)
     END IF

     DO I=1, DMSM_TOTAL_MPI_PROCESSES
           DO J=1, DMSM_THREADS_PP_EXPECTED
	          IF(DMSM_JOBS_DONE_BY_WHOM_SWITCH.EQ.1) &
              WRITE(DMSM_A_FILE_TUNNEL,*) "Thread ", J-1," of Rank/Node ", I-1," completed jobs: "
              L=0
              M=0
              DO  K = 1, DMSM_TOTAL_JOBS
                  IF((DMSM_SUM_JOB_I_DID(2,K).EQ.I-1).AND.(DMSM_SUM_JOB_I_DID(3,K).EQ.J-1)) THEN
                     M=M+1
                     L=L+1
                     N(L)=K
                     IF(L.GE.10) THEN
  	                    IF(DMSM_JOBS_DONE_BY_WHOM_SWITCH.EQ.1) &
                        WRITE(DMSM_A_FILE_TUNNEL,'(1X,100(I8,1X))') N(1:L)
                        L=0
                     END IF
                  END IF
              END DO
              IF(DMSM_JOBS_DONE_BY_WHOM_SWITCH.EQ.1) THEN
                 IF(L.GE.1) WRITE(DMSM_A_FILE_TUNNEL,'(1X,100(I8,1X))') N(1:L)
                 WRITE(DMSM_A_FILE_TUNNEL,*) "Total: ", M
                 WRITE(DMSM_A_FILE_TUNNEL,*)
              END IF
              IF(M.NE.DMSM_JOBS_COMPLETED_ALL(J,I)) THEN
                 CALL DMSM_ERROR_NOT_STOP('ERROR: Inconsistancy of jobs: ', M, ' and ', &
                      DMSM_JOBS_COMPLETED_ALL(J,I), ' done by thread ',J-1, ' and process ', I-1)
              END IF
           END DO
     END DO

     T9MM0=(0.9D0)*TMM0

     TM1=0.0D0
     TMM1=-5.0D0
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"Thread, Rank/Node, Jobs done, and CPU-Time used (seconds):"
     M=0
	 DMSM_TOTAL_JOB_CPU_TIME=0.0D0
     DO I=1, DMSM_TOTAL_MPI_PROCESSES
        DO J=1, DMSM_THREADS_PP_EXPECTED
           L=0
           WRITE(DMSM_A_FILE_TUNNEL,'(1X,I6,1X,I10,1X,I10,1X,F23.6)') J-1,I-1,DMSM_JOBS_COMPLETED_ALL(J,I), &
                                                                             DMSM_CPU_TIME_OF_ALL(J,I)
           M=M+DMSM_JOBS_COMPLETED_ALL(J,I)
           IF(TM1.LT.DMSM_CPU_TIME_OF_ALL(J,I)) TM1=DMSM_CPU_TIME_OF_ALL(J,I)
		   IF(DMSM_CPU_TIME_OF_ALL(J,I).GT.T9MM0) THEN
              IF(TMM1.LT.0.0D0) THEN
                 TMM1=DMSM_CPU_TIME_OF_ALL(J,I)
              ELSE
                 IF(TMM1.GT.DMSM_CPU_TIME_OF_ALL(J,I)) TMM1=DMSM_CPU_TIME_OF_ALL(J,I)
              END IF
		   END IF
           IF(.NOT.DMSM_ONLY_MPI_ENABLED) THEN
              DMSM_TOTAL_JOB_CPU_TIME=DMSM_TOTAL_JOB_CPU_TIME+DMSM_CPU_TIME_OF_ALL(J,I)
           ELSE
              DMSM_TOTAL_JOB_CPU_TIME=DMSM_TOTAL_JOB_CPU_TIME+DMSM_CPU_TIME_OF_ALL(J,I)*DMSM_ONLY_MPI_JOB_PROCESSES_T(I)
           END IF
        END DO
     END DO

     TM2=0.0D0
     TMM2=-5.0D0
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)" Rank/Node, Jobs done, and CPU-Time used (seconds):"
     DO I=1, DMSM_TOTAL_MPI_PROCESSES
	    TMT=0.0D0
		TMT=SUM(DMSM_CPU_TIME_OF_ALL(1:DMSM_THREADS_PP_EXPECTED,I))
		IF(TM2.LT.TMT) TM2=TMT
        IF(TMT.GT.T9MM0) THEN
		   IF(TMM2.LT.0.0D0) THEN
		      TMM2=TMT
           ELSE
  		      IF(TMM2.GT.TMT) TMM2=TMT
           END IF
		END IF
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,I10,1X,I10,1X,F23.6)') I-1, &
		            SUM(DMSM_JOBS_COMPLETED_ALL(1:DMSM_THREADS_PP_EXPECTED,I)),TMT
     END DO

     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*) "Total jobs done: ", M, "(",DMSM_TOTAL_JOBS,")"
     IF(M.NE.DMSM_TOTAL_JOBS) THEN
        CALL DMSM_ERROR_NOT_STOP('ERROR: Inconsistancy of total jobs: ', M, ' and ', DMSM_TOTAL_JOBS)
     END IF

     IF(DMSM_TOTAL_JOBS.GT.0) THEN

     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The maximum CPU-time of any jobs    : ", &
	                             TM0, " seconds."
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The minimum CPU-time of any jobs    : ", &
	                             TMM0, " seconds."
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The maximum CPU-time of any threads : ", &
	                             TM1, " seconds."
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The minimum CPU-time of any threads : ", &
	                             TMM1, " seconds."
#ifdef DMSM_FORTRAN_RS16_TIMER
     CALL CPU_TIME(DMSM_WALL_ENDDING_TIME)
#else
#ifdef DMSM_FORTRAN_RS08_TIMER
     CALL CPU_TIME(DMSM_WALL_ENDDING_TIME)
#else
     DMSM_WALL_ENDDING_TIME=DMSM_C_TIMER()
#endif
#endif
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The master's total wall clock time  : ", &
	                             DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME, " seconds."
     IF(TM1.GT.(DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)) THEN
        WRITE(DMSM_A_FILE_TUNNEL,*)"UNREASONABLE: THE FORMER IS BIGGER THAN THE LATER !!! "
	 END IF
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The maximum CPU-time of any process : ", &
	                             TM2, " seconds."
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The minimum CPU-time of any process : ", &
	                             TMM2, " seconds."
     WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The total CPU time of all jobs      : ", &
	                             DMSM_TOTAL_JOB_CPU_TIME, " seconds."
     IF(.NOT.DMSM_ONLY_MPI_ENABLED) THEN
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The total wall clock of all threads : ", &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
								   DMSM_TOTAL_MPI_PROCESSES*DMSM_THREADS_PP_EXPECTED, " seconds."
        IF((DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME).GT.1.0D-10) &
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,F7.2,A1)')"The percentage of the last two      : ", &
                                   DMSM_TOTAL_JOB_CPU_TIME*100.000D0/( &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
			 					   DMSM_TOTAL_MPI_PROCESSES*DMSM_THREADS_PP_EXPECTED),"%"
     ELSE
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,E12.4,A9)')"The total wall clock of all threads : ", &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
								   DMSM_ONLY_MPI_ALL_PROCESSES, " seconds."
        IF((DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME).GT.1.0D-10) &
        WRITE(DMSM_A_FILE_TUNNEL,'(1X,A38,F7.2,A1)')"The percentage of the last two      : ", &
                                   DMSM_TOTAL_JOB_CPU_TIME*100.000D0/( &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
			 					   DMSM_ONLY_MPI_ALL_PROCESSES),"%"
     END IF
     WRITE(DMSM_A_FILE_TUNNEL,*)

     IF(DMSM_TIMING_SUMMARY_TO_SCREEN.EQ.1) THEN
        WRITE(*,*) "Total jobs done: ", M, "(",DMSM_TOTAL_JOBS,")"
        WRITE(*,'(1X,A38,E12.4,A9)')"The maximum CPU-time of any jobs    : ", &
	                                TM0, " seconds."
        WRITE(*,'(1X,A38,E12.4,A9)')"The minimum CPU-time of any jobs    : ", &
	                                TMM0, " seconds."
        WRITE(*,'(1X,A38,E12.4,A9)')"The maximum CPU-time of any threads : ", &
	                                 TM1, " seconds."
        WRITE(*,'(1X,A38,E12.4,A9)')"The minimum CPU-time of any threads : ", &
	                                 TMM1, " seconds."
        WRITE(*,'(1X,A38,E12.4,A9)')"The master's total wall clock time  :   ", &
	              DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME, " seconds."
        IF(TM1.GT.(DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)) THEN
           WRITE(*,*)"UNREASONABLE: THE FORMER IS BIGGER THAN THE LATER !!! "
	    END IF
        WRITE(*,'(1X,A38,E12.4,A9)')"The maximum CPU-time of any process : ", &
	                                TM2, " seconds."
        WRITE(*,'(1X,A38,E12.4,A9)')"The minimum CPU-time of any process : ", &
	                                TMM2, " seconds."
        WRITE(*,'(1X,A38,E12.4,A9)')"The total CPU time of all jobs      : ", &
	                                DMSM_TOTAL_JOB_CPU_TIME, " seconds."
        IF(.NOT.DMSM_ONLY_MPI_ENABLED) THEN
           WRITE(*,'(1X,A38,E12.4,A9)')"The total wall clock of all threads : ", &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
								   DMSM_TOTAL_MPI_PROCESSES*DMSM_THREADS_PP_EXPECTED, " seconds."
           IF((DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME).GT.1.0D-10) &
           WRITE(*,'(1X,A38,F7.2,A1)')"The percentage of the last two      : ", &
                                   DMSM_TOTAL_JOB_CPU_TIME*100.000D0/( &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
			 					   DMSM_TOTAL_MPI_PROCESSES*DMSM_THREADS_PP_EXPECTED),"%"
        ELSE
           WRITE(*,'(1X,A38,E12.4,A9)')"The total wall clock of all threads : ", &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
								   DMSM_ONLY_MPI_ALL_PROCESSES, " seconds."
           IF((DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME).GT.1.0D-10) &
           WRITE(*,'(1X,A38,F7.2,A1)')"The percentage of the last two      : ", &
                                   DMSM_TOTAL_JOB_CPU_TIME*100.000D0/( &
	                              (DMSM_WALL_ENDDING_TIME-DMSM_WALL_BEGINNING_TIME)* &
			 					   DMSM_ONLY_MPI_ALL_PROCESSES),"%"
        END IF
     END IF

     END IF

     WRITE(*,*)"INITIAL DATA PREPARED FOR GROUPS     ENABLED: ", DMSM_INITIAL_DATA_PREP_ENABLED
     WRITE(*,*)"RESULT COLLECTION DURING COMPUTATION ENABLED: ", DMSM_RESULT_COLLECTION_ENABLED
     WRITE(*,*)
     WRITE(*,*) "*******************************************"
     WRITE(*,*) "* Double-layer Master-Slave Model library *"
     WRITE(*,*) "* Version 4.3  by   HPCVL.org 2015        *"
     WRITE(*,*) "*******************************************"
     WRITE(*,*)

     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)"INITIAL DATA PREPARED FOR GROUPS     ENABLED: ", DMSM_INITIAL_DATA_PREP_ENABLED
     WRITE(DMSM_A_FILE_TUNNEL,*)"RESULT COLLECTION DURING COMPUTATION ENABLED: ", DMSM_RESULT_COLLECTION_ENABLED
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*)
     WRITE(DMSM_A_FILE_TUNNEL,*) "*******************************************"
     WRITE(DMSM_A_FILE_TUNNEL,*) "* Double-layer Master-Slave Model library *"
     WRITE(DMSM_A_FILE_TUNNEL,*) "* Version 4.3  by   HPCVL.org 2015        *"
     WRITE(DMSM_A_FILE_TUNNEL,*) "*******************************************"
     WRITE(DMSM_A_FILE_TUNNEL,*)

     CLOSE(DMSM_A_FILE_TUNNEL)

  END IF


  RETURN
END SUBROUTINE DMSM_JOB_DISTRIBUTION_CHECKUP




SUBROUTINE    DMSM_ALL_A(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB)
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_INITIALIZE_INTERFACE
  USE DMSM_WORKING_INTERFACE
  IMPLICIT NONE
  INTEGER ::           THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE

  CALL DMSM_INITIALIZE(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
  CALL DMSM_WORKING(DO_THE_JOB)
  CALL DMSM_JOB_DISTRIBUTION_CHECKUP()
  CALL DMSM_FINALIZE()
  RETURN
END SUBROUTINE DMSM_ALL_A




SUBROUTINE    DMSM_ALL_I(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION)
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_INITIALIZE_INTERFACE
  USE DMSM_WORKING_INTERFACE
  IMPLICIT NONE
  INTEGER ::           THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE

  CALL DMSM_INITIALIZE(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
  CALL DMSM_WORKING(DO_THE_JOB, JOB_GROUP_PREPARATION)
  CALL DMSM_JOB_DISTRIBUTION_CHECKUP()
  CALL DMSM_FINALIZE()
  RETURN
END SUBROUTINE DMSM_ALL_I




SUBROUTINE    DMSM_ALL_S(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_INITIALIZE_INTERFACE
  USE DMSM_WORKING_INTERFACE
  IMPLICIT NONE
  INTEGER ::           THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  LOGICAL ::           RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  CALL DMSM_INITIALIZE(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
  CALL DMSM_WORKING(DO_THE_JOB, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  CALL DMSM_JOB_DISTRIBUTION_CHECKUP()
  CALL DMSM_FINALIZE()
  RETURN
END SUBROUTINE DMSM_ALL_S




SUBROUTINE    DMSM_ALL_Y(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,    &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_INITIALIZE_INTERFACE
  USE DMSM_WORKING_INTERFACE
  IMPLICIT NONE
  INTEGER ::           THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  LOGICAL ::           RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  CALL DMSM_INITIALIZE(THREADS_PER_PROCESS_EXPECTED, JOB_DISTRIBUTION_PLAN,  &
                       TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP)
  CALL DMSM_WORKING(DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  CALL DMSM_JOB_DISTRIBUTION_CHECKUP()
  CALL DMSM_FINALIZE()
  RETURN
END SUBROUTINE DMSM_ALL_Y




INTEGER FUNCTION DMSM_GET_JOB_COMM()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_JOB_COMM=DMSM_ONLY_MPI_JOB_COMM
  RETURN
END FUNCTION DMSM_GET_JOB_COMM




INTEGER FUNCTION DMSM_GET_JOB_RANK()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_JOB_RANK=DMSM_ONLY_MPI_JOB_RANK
  RETURN
END FUNCTION DMSM_GET_JOB_RANK




INTEGER FUNCTION DMSM_GET_JOB_PROCS()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_JOB_PROCS=DMSM_ONLY_MPI_JOB_PROCESSES
  RETURN
END FUNCTION DMSM_GET_JOB_PROCS




INTEGER FUNCTION DMSM_GET_DIST_COMM()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_DIST_COMM=DMSM_COMMUNICATOR
  RETURN
END FUNCTION DMSM_GET_DIST_COMM




INTEGER FUNCTION DMSM_GET_DIST_RANK()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_DIST_RANK=DMSM_MY_MPI_RANK
  RETURN
END FUNCTION DMSM_GET_DIST_RANK




INTEGER FUNCTION DMSM_GET_DIST_PROCS()
  USE DMSM_MODULE_NOT_FOR_USER
  DMSM_GET_DIST_PROCS=DMSM_TOTAL_MPI_PROCESSES
  RETURN
END FUNCTION DMSM_GET_DIST_PROCS




SUBROUTINE DMSM_MPI_ONLY_CORE(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_ALL_INTERFACE
  IMPLICIT NONE
  INTEGER ::           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  LOGICAL ::           RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  DMSM_ONLY_MPI_ENABLED=.TRUE.

  IF(.NOT.DMSM_SET_COMM_10_CALLED) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR THE THREE COMMUNICATORS NOT SET BY CALLING DMSM_SET_COMM ROUTINE: ", 0)
  END IF

  IF(DMSM_ONLY_MPI_JOB_RANK.EQ.DMSM_JOB_MASTER) THEN
     ALLOCATE(DMSM_ONLY_MPI_JOB_PROCESSES_T(DMSM_ONLY_MPI_DIST_PROCESSES))
     CALL MPI_GATHER(DMSM_ONLY_MPI_JOB_PROCESSES,1,MPI_INTEGER, DMSM_ONLY_MPI_JOB_PROCESSES_T,1, &
                     MPI_INTEGER,DMSM_MASTER,DMSM_ONLY_MPI_DIST_COMM,DMSM_IERR)
     CALL DMSM_ALL(1, 11, TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                   DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
     CALL MPI_BCAST(DMSM_ONLY_MPI_NO_MORE_JOBS,1,MPI_INTEGER,DMSM_JOB_MASTER, &
                    DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
     DEALLOCATE(DMSM_ONLY_MPI_JOB_PROCESSES_T)
  ELSE
     ONLY_MPI_JOB_ITERATION: DO
       CALL MPI_BCAST(DMSM_MY_JOB,1,MPI_INTEGER,DMSM_JOB_MASTER,DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
       IF(DMSM_MY_JOB.GE.1) THEN
          CALL DO_THE_JOB(DMSM_MY_JOB)
       ELSE
          EXIT ONLY_MPI_JOB_ITERATION
       END IF
     END DO ONLY_MPI_JOB_ITERATION
  END IF

  CALL MPI_COMM_FREE(DMSM_ONLY_MPI_ALL_COMM,DMSM_IERR)
  CALL MPI_COMM_FREE(DMSM_ONLY_MPI_JOB_COMM,DMSM_IERR)
  CALL MPI_COMM_FREE(DMSM_ONLY_MPI_DIST_COMM,DMSM_IERR)
  DMSM_ONLY_MPI_ENABLED=.FALSE.
  DMSM_SET_COMM_10_CALLED=.FALSE.

  RETURN
END SUBROUTINE DMSM_MPI_ONLY_CORE




SUBROUTINE DMSM_MPI_ONLY_A(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_MPI_ONLY_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER ::           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
  CALL DMSM_MPI_ONLY_CORE(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, DMSM_DO_NOTHING_4, DMSM_DO_NOTHING_2, .FALSE.)
  RETURN
END SUBROUTINE DMSM_MPI_ONLY_A




SUBROUTINE DMSM_MPI_ONLY_I(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_MPI_ONLY_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER ::           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE

  CALL DMSM_MPI_ONLY_CORE(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, DMSM_DO_NOTHING_2, .FALSE.)
  RETURN
END SUBROUTINE DMSM_MPI_ONLY_I




SUBROUTINE DMSM_MPI_ONLY_S(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_MPI_ONLY_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER ::           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  LOGICAL ::           RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  CALL DMSM_MPI_ONLY_CORE(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, DMSM_DO_NOTHING_4, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  RETURN
END SUBROUTINE DMSM_MPI_ONLY_S




SUBROUTINE DMSM_MPI_ONLY_Y(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  USE DMSM_MODULE_NOT_FOR_USER
  USE DMSM_DO_NOTHING_INTERFACE
  USE DMSM_MPI_ONLY_CORE_INTERFACE
  IMPLICIT NONE
  INTEGER ::           TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP
  LOGICAL ::           RESULT_COLLECTION_ENABLED
      INTERFACE
        SUBROUTINE DO_THE_JOB(DUMMY_I)
          IMPLICIT NONE
          INTEGER :: DUMMY_I
        END SUBROUTINE DO_THE_JOB
      END INTERFACE
      INTERFACE
        SUBROUTINE JOB_GROUP_PREPARATION(DUMMY_I,DUMMY_J,DUMMY_K, DUMMY_L)
          IMPLICIT NONE
          INTEGER :: DUMMY_I, DUMMY_J, DUMMY_K, DUMMY_L
        END SUBROUTINE JOB_GROUP_PREPARATION
      END INTERFACE
      INTERFACE
        SUBROUTINE RESULT_COLLECTION(DUMMY_J,DUMMY_K)
          IMPLICIT NONE
          INTEGER :: DUMMY_J,DUMMY_K
        END SUBROUTINE RESULT_COLLECTION
      END INTERFACE

  CALL DMSM_MPI_ONLY_CORE(TOTAL_JOBS, NUM_OF_JOBS_PER_GROUP,&
                       DO_THE_JOB, JOB_GROUP_PREPARATION, RESULT_COLLECTION, RESULT_COLLECTION_ENABLED)
  RETURN
END SUBROUTINE DMSM_MPI_ONLY_Y




FUNCTION DMSM_INTEGER_TO_CHARACTER(I,LENGTH)
IMPLICIT NONE
CHARACTER*20 :: DMSM_INTEGER_TO_CHARACTER
INTEGER :: I,LENGTH,J,K,L

LENGTH=0
DMSM_INTEGER_TO_CHARACTER=' '
IF(I.LT.0) THEN
   LENGTH=1
   DMSM_INTEGER_TO_CHARACTER(1:1)='-'
END IF

J=ABS(I)
K=0
LOOP: DO
   K=K+1
   J=J/10
   IF(J.EQ.0) EXIT LOOP
END DO LOOP
LENGTH=LENGTH+k

J=ABS(I)
DO L=1,K
   DMSM_INTEGER_TO_CHARACTER(LENGTH+1-L:LENGTH+1-L)=CHAR(MOD(J,10)+48)
   J=J/10
END DO

RETURN
END FUNCTION DMSM_INTEGER_TO_CHARACTER




SUBROUTINE DMSM_OPEN_PROBLEM_FILE()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER ::  L
  CHARACTER*20 :: I2CH
  CALL DMSM_FIND_THE_FILE_TUNNEL()
  IF(DMSM_JOURNAL_NUMBER.GT.0) THEN
     I2CH=DMSM_INTEGER_TO_CHARACTER(DMSM_JOURNAL_NUMBER,L)
     OPEN(UNIT=DMSM_A_FILE_TUNNEL, &
          FILE=DMSM_PROBLEM_FILE_HEAD//'_'//I2CH(1:L)//'.txt')
  ELSE
     OPEN(UNIT=DMSM_A_FILE_TUNNEL,FILE=DMSM_PROBLEM_FILE_HEAD//'.txt')
  END IF
  RETURN
END SUBROUTINE DMSM_OPEN_PROBLEM_FILE




SUBROUTINE DMSM_OPEN_JOURNAL_FILE()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER ::  L
  CHARACTER*20 :: I2CH
  CALL DMSM_FIND_THE_FILE_TUNNEL()
  IF(DMSM_JOURNAL_NUMBER.GT.0) THEN
     I2CH=DMSM_INTEGER_TO_CHARACTER(DMSM_JOURNAL_NUMBER,L)
     OPEN(UNIT=DMSM_A_FILE_TUNNEL, &
          FILE=DMSM_JOURNAL_FILE_HEAD//'_'//I2CH(1:L)//'.txt')
  ELSE
     OPEN(UNIT=DMSM_A_FILE_TUNNEL,FILE=DMSM_JOURNAL_FILE_HEAD//'.txt')
  END IF
  RETURN
END SUBROUTINE DMSM_OPEN_JOURNAL_FILE




SUBROUTINE DMSM_OPEN_JOURNAL_APPEND()
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER ::  L
  CHARACTER*20 :: I2CH
  CALL DMSM_FIND_THE_FILE_TUNNEL()
  IF(DMSM_JOURNAL_NUMBER.GT.0) THEN
     I2CH=DMSM_INTEGER_TO_CHARACTER(DMSM_JOURNAL_NUMBER,L)
     OPEN(UNIT=DMSM_A_FILE_TUNNEL, &
          FILE=DMSM_JOURNAL_FILE_HEAD//'_'//I2CH(1:L)//'.txt', &
          position='APPEND')
  ELSE
     OPEN(UNIT=DMSM_A_FILE_TUNNEL, &
          FILE=DMSM_JOURNAL_FILE_HEAD//'.txt', &
          position='APPEND')
  END IF
  RETURN
END SUBROUTINE DMSM_OPEN_JOURNAL_APPEND




SUBROUTINE DMSM_SET_JOURNAL_NUMBER(JN)
  USE DMSM_MODULE_NOT_FOR_USER
  IMPLICIT NONE
  INTEGER :: JN
  IF(JN .LT. 0) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR FOR UNACCEPTABLE JOURNAL NUMBER: ", JN)
  END IF
  IF(DMSM_ALREADY_INITIALIZED) THEN
     CALL DMSM_ERROR_AND_STOP("ERROR: ROUTINE DMSM_SET_JOURNAL_NUMBER() "&
                         &//"MUST BE CALLED BEFORE DMSM_INITIALIZE(). ", 0)
  END IF
  DMSM_JOURNAL_NUMBER=JN
  RETURN
END SUBROUTINE DMSM_SET_JOURNAL_NUMBER



