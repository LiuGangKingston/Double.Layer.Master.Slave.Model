/*
!    Double-layer Master-Slave Model Library
!           Version 4.3
!    Copyright (c) 2009-2015 by High Performance Computing Virtual Laboratory
!    www.hpcvl.org
!
!    This file and dmsm.h containes a set of data
!    structures and routines, as a library for
!    Double-layer Master-Slave Model (DMSM).
!    This library can be used in two scenarios.
!    One is to employ the MPI Master-Slave and
!    OpenMP all-slave Models nestedly for the
!    job distribution and each of many
!    independent jobs is completed by one thread.
!    The other scenario is to employ the MPI
!    Master-Slave model for job distribution
!    and each job is done either by all OpenMP
!    threads of a certain process or by a subset
!    of MPI processes, in parallel. In any
!    scenario, this library does all job
!    distribution work and calls a user supplied
!    routine to perform all user's specific jobs.
!
!    For many independent small jobs with CPU-time
!    unknown and/or uneven, the first scenatio is
!    a good choice, while for many independent
!    relative big jobs, the second scenario is
!    better. While a regular existing
!    serial code can be converted into a user's
!    specific job routine and this library is
!    employed to complete many such jobs as in
!    the first scenario, a regular existing OpenMP
!    or MPI parallel code can be converted into a
!    user's specific job routine and this library
!    is used to perform many such jobs as in the
!    second scenatio, provided all I/O operations
!    and MPI communicators handled properly. An
!    MPI code normally run with 64 processes for a
!    job, for an example, can be converted and
!    combined with this library to run with 257
!    processes at a time. While one of the processes
!    works as the Master to distribute jobs, all the
!    rest processes are divided into four subsets
!    with 64 processes each. Any subset can accept a
!    certain job, which is then executed by the 64
!    processes of the subset in parallel. As a
!    result, total 4 jobs at maximum can be executed
!    by all subsets simultaneously. In one word,
!    this library can be used as an outter additional
!    layer parallelism (of job distribution) over a
!    regular existing serial/parallel code.
!
!    This library strictly adopts a naming
!    convention: all names of global constants,
!    variables, routines, and files generated by
!    this file begin with DMSM_ as a prefix.
!
!    The cluster to complete the whole calculation
!    task is assumed made of some nodes and each
!    node is assumed as a shared-memory machine.
!    Although many MPI processes may be in the same
!    shared-memory node, for simplicity and clarity,
!    let us further regard the (MPI) Master and
!    (MPI) Slaves as corresponding nodes and assume
!    that inside each node there are OpenMP threads,
!    including the main thread and the rest threads,
!    as usual.
!
!    In order to reduce communication for job
!    distribution, all the independent jobs are
!    grouped into groups, each of which has
!    Num_Of_Jobs_Per_Group independent jobs.
!    In this library, the Master always assign
!    individual job group as a whole to the
!    Slaves, by employing the MPI Master-slave
!    model.
!
!    Then in the first scenario, the Slaves
!    normally start an OpenMP parallel region
!    to create threads, who then complete all
!    jobs of the group based on OpenMP
!    "all-slave" Model. Let us discuss the
!    usage of this library for the first
!    scenario in the following firstly.
!
!    In this scenario, considering the nature of
!    the mixture of OpenMP and MPI, MPI
!    communication is only allowed between
!    different nodes and with one thread of each
!    node involved at a time (in OpenMP
!    protected regions, if applicable), for the
!    sake of safety. Job group assignment to the
!    rest threads in the Master node by the main
!    thread is done via shared variables. However,
!    this library still allows users to choose any
!    combination of the following three working
!    modes (i=1,2,3) of the Master and three working
!    modes (j=1,2,3) of the Slaves, by supplying
!    corresponding integer value of
!    Job_Distribution_Plan=i*10+j as an argument,
!    when the library is called. All of these nine
!    combinations have been tested successfully
!    in the HPCVL Sun-Fire Clusters.
!
!    In mode i=1, the Master never enters an OpenMP
!    parallel region, but distributes job groups
!    to Slaves only.
!
!    In mode i=2, the Master enters an OpenMP
!    parallel region, where the main thread
!    distributes job groups to Slaves, while the rest
!    threads complete some preallocated job groups
!    in the "all-slave" Model in each job group.
!
!    In mode i=3, the Master enters an OpenMP
!    parallel region, where the main thread
!    distributes job groups to the Slaves and to the
!    rest threads of the same node, while the rest
!    threads complete every dynamically received job
!    group in the "all-slave" Model. Again the
!    communication between the main thread and the
!    rest threads are not done with MPI but with
!    shared varibles for job group assignment,
!    since they are in the same Master node.
!
!    In mode j=1, a Slave (always) talks with and
!    receives a job group from the Master outside
!    any OpenMP parallel region. Then it enters an
!    OpenMP parallel region, spawns threads, and
!    the threads complete all jobs of the received
!    group. When the job group is done, closes the
!    OpenMP region and repeats the whole procedure.
!
!    In mode j=2, any Slave always lives inside an
!    OpenMP parallel region, but only the main
!    thread of it is allowed to talk with and to
!    receive a job group from the Master, whenever
!    it finds out no more job left un-assigned in
!    the node.
!
!    In mode j=3, anything is the same with mode
!    j=2 except that in this mode, any thread is
!    allowed to talk with and to receive job
!    groups from the Master.
!
!    However additionally, the following three
!    special cases for job group distribution are
!    also implimented.
!
!    Special case 1, if both the total number
!    of MPI processes and the total number of
!    threads per process are 1, a pure serial version
!    is employed to complete jobs.
!
!    Special case 2, otherwise, if the total number
!    of MPI processes is 1, a pure OpenMP "all-slave"
!    model is employed.
!
!    Special case 3, otherwise, if the total number
!    of threads per process is 1, a pure MPI
!    Master-Slave model is employed.
!
!    For a simple use of this library, code the
!    following statement
!    #include "dmsm.h"
!    into the user's code firstly, then call the
!    following four routines in sequences:
!       void DMSM_Initialize(...);
!       void DMSM_Working(...);
!       void DMSM_Job_Distribution_Checkup();
!       void DMSM_Finalize();
!    or call the one wrapper routine
!       void DMSM_All(...);
!    for all of the four routines.
!
!    While some of these corresponding routines in
!    FORTRAN 90 are overloaded, any un-available
!    arguments of them in C can be passed as "NULL".
!    For detailed arguments, please check the manual,
!    the header file dmsm.h, and/or the test example
!    files.
!
!    One must code his/her own routine for specific
!    jobs, and uses its name as an argument when calls
!    the above routine(s). The job routine has only
!    one argument, which is the job sequencial number
!    from 1 in FORTRAN and from 0 in C, assigned
!    by this library.
!       void Do_The_Job(int my_job);
!    is a good one. In all the total six test examples,
!    the computational tasks/jobs are the same: to
!    calculate sqare root summation from 0 to an upper
!    integer limit with step 1. Although the summation
!    with the maximum upper limit includes all the
!    rest, we regard all of the summation jobs as
!    independant ones for demonstration purpose.
!
!    The simpliest way to use this library is to
!    broadcast all initial data across processes
!    and make them available to any thread at the
!    beginning, then call this library to complete
!    all jobs, and then collect all results in
!    one's own seperate routine. In other words,
!    this library only assigns jobs essentially.
!    Example 1 is of this situation.
!
!    Some users may want to prepare the initial
!    data for a job group, only when the job group
!    is assigned/received, rather than prepare all
!    initial data beforehand, for any reason. Then
!    users should code such a routine and pass it
!    as another argument into the library, then the
!    library runs it when any job group is assigned
!    and received. Since the initial data for the
!    received job group should be shared in OpenMP
!    essentially, inside each process/node, there
!    would be a race condition when one thread
!    is trying to access the initial data for a job
!    of the current job group, and at the same time
!    another thread is trying to update the whole
!    initial data for the received next job group.
!    In order to avoid this race condition, the
!    library impliments and manages some locks or
!    equivalent ones. Users are required and only
!    required to call
!        void DMSM_Unset_An_Initial_Lock();
!    as soon as after the initial data for the job is
!    fetched (usually copied to some temporary places)
!    in the user's job routine, and required to call
!        void DMSM_Wait_For_Initial_Locks();
!    at the beginning of the user's initial data
!    preparation routine for job groups.
!    Example 2 is of this situation.
!
!    The library also supplies the following integer
!    functions
!      int DMSM_Get_Group_Start(int job_number);
!      int DMSM_Get_Group_End(int job_number);
!      int DMSM_Get_Group_Number(int job_number);
!    returning the beginning job sequential number,
!    the end job sequential number, and the job group
!    number of the job group, where the job number
!    job_number resites, respectively. At least,
!    the first one is often needed in the user's job
!    routine, when the initial data for job groups
!    prepared dynamically.
!
!    Furthermore, some users may want to collect
!    computed results as quickly as possible. So
!    this library also allows users to collect
!    results accessible when a job group is assigned
!    or received. For this purpose, the user's own
!    result collecting routine should be passed as
!    another argument followed by an integer argument
!    when the library is called. If the integer
!    argument is 1, the result collecting
!    routine is called when a job group is
!    assigned/received, when "no more job group"
!    message is sent/received, and when all jobs
!    are finished; if it is 0, called only when all
!    the jobs are finished, to collect any results
!    not collected in any case.
!
!    The user's result collection routine should be
!    coded in the following way. 1, Once any result(s)
!    is(are) sent from any thread to the Master, the
!    result(s) should be deleted by the sender to
!    avoid being sent again. 2, In every MPI
!    communication, a Slave should send the Master
!    an integer first, which is the number (of elements)
!    of results to be sent next immediately. If it
!    is zero, there will be no the expected "next
!    sending" and no the exptected "next receiving".
!
!    As all results are assumed to be collected
!    into some fixed place of the Master node, the
!    user's result collection routine is called
!    by the library from a locked/protected region,
!    only if applicable, to avoid some race
!    condition on the destination data struction.
!
!    As the user's job routine and the user's
!    result collection routine are always called
!    independently, the result from each job can
!    not be placed into any private data structure
!    for later collection. Example 3 uses a
!    THREADPRIVATE array Results_in_Thread,
!    instead, and collects results dynamically.
!
!    Definitely, OpenMP shared data structure
!    can also be employed to hold results for
!    each job and collected later. However
!    whenever such a shared data structure is
!    updated, there is possible race condition.
!    To avoid it, the library supplies and manages
!    additional locks, and users can make use of
!    them by just calling
!       void DMSM_Set_Node_Result_Lock();
!    and
!       void DMSM_Unset_Node_Result_Lock();
!    before and after the updating.
!    Example 4, using the shared array
!    Results_in_Process, is of this situation.
!
!    The last issue regarding result collection is
!    whether it is called from an OpenMP parallel
!    region of this library, after all jobs are
!    finished. The default is in parallel region,
!    as the constant
!        DMSM_COLLECT_RESULTS_PARALLEL=1
!    set at the beginning of this code. If this
!    parameter is changed into any other value,
!    the user's result collection routine is
!    called outside any OpenMP parallel region
!    after all jobs are finished. This way of
!    usage of the library may also need the
!    following logical function
!        int DMSM_All_Jobs_Done();
!    to determine whether all jobs are done.
!    Although the library provides such a
!    flexibility, however, it is not
!    encouraged, as more users' programming is
!    needed. All our examples are coded for the
!    default DMSM_COLLECT_RESULTS_PARALLEL=1
!    setting.
!
!    For any reason, if only some of the
!    processes, rather than all of them, are
!    expected to call this library, create an
!    MPI communicator of the processes, call
!    void DMSM_Set_Comm(MPI_Comm An_MPI_Communicator);
!    and have all and only the processes
!    belonging to the communication
!    An_MPI_Communicator call this library.
!
!    The routine DMSM_Set_Comm(...) must be
!    called before any other routine call of
!    this library. This facility enables the
!    second scenario become possible. From
!    now on, let us discuss the second
!    scenario.
!
!    It is very easy to get any user's job
!    completed by all threads of a slave node in
!    parallel, in OpenMP region(s) of the user's
!    own job routine, with a pure MPI
!    Master-Slave model, special case 3 of this
!    library, enabled to distribute jobs, by
!    passing the total number of threads per
!    process as 1 when this library is called,
!    although it is bigger than 1 in reality.
!    Example dmsm.example.job.openmp.c is of
!    this situation.
!
!    Now let us focus on a more complicated
!    situation, where each job is done by a
!    subset of MPI processes, as the example
!    of 1+4*64=257 processes illustrates at
!    the beginning of this comment area. In
!    order to have this situation to work,
!    more communicators and more
!    communications are needed.
!
!    Since each user's job will be completed
!    by a subset of MPI processes in parallel,
!    an MPI communicator for each subset is
!    needed. Let us call it as job
!    communicator, while the communicator of
!    all processes involved in the whole
!    computation task as the original
!    communicator. The Master process will form
!    a job communicator only containing itself.
!    As inside this library, each job or job
!    group is assigned to one process only,
!    another communicator, made of all processes
!    of zero ranks of all job communicators, is
!    also needed. Let us call this third
!    communicator as distribute communicator,
!    as it will be used for job distribution.
!
!    For this situation, the library supplies
!    two more routines:
!      void DMSM_Gen_Comm_MPI_All(...);
!      void DMSM_MPI_All(...);
!    to replace DMSM_All(...).
!    The former with seven int(*) auguments and
!    three MPI_Comm auguments will generate the
!    job communicator, the distribute communicator
!    from the original communicator (second
!    argument), and set them in the library
!    accordingly for later usage. The job
!    communicator will be created based on the
!    first argument, which is the number of
!    processes in each job communicator. However
!    apparently the last job communicator will
!    get processes whatever it can get. The latter
!    call will perform additional necessary
!    communications and all computational jobs.
!    The code dmsm.example.mpi.c
!    is an example of this situation.
!
!    As in dmsm.example.mpi.c, user's routines
!    to prepare the initial data for a job group
!    and to collect results will only work inside
!    the distribution communicator. This means
!    only the process of rank zero of each job
!    communicator will be involved in these
!    communications. Then further communications
!    for data initialization, computation, and
!    result collection inside each job are
!    usually needed within job communicators
!    respectively.
!
!    In HPCVL machines, examples can be compiled
!    with Cluster_Tool 8 of the command
!    use ct8
!    mpicc -dalign -fast -xopenmp \
!          dmsm.c EXAMPLEk.c -lm
!    and run with commands
!        echo THREADS > \
!          TotalNumberOfOpenMPThreadsPerProcess
!        mpirun -np PROCESSES x THREADS ./a.out
!    or
!        echo SIZE_Of_JOB_COMM > \
!          MPI.PROCESSES.IN.JOB.COMM.dat
!        mpirun -np SIZE_OF_ORIGINAL_COMM ./a.out
!    or by using script files, just command
!    hk PROCESSES THREADS (for example k)
!    h3 4 5 (for an example)
!    or
!    ho PROCESSES THREADS
!    or
!    hm PROCESSES SIZE_Of_JOB_COMM
!    to compile and run.
!
!    Please send problems to
!           gang.liu@queensu.ca
!
!    www.hpcvl.org
!    Copyright (c) 2009-2015 by High Performance Computing Virtual Laboratory
*/




#include <mpi.h>
#include <omp.h>
#include <sys/time.h>
#include <time.h>
#include <stdio.h>
#include <stdlib.h>
#include <malloc.h>
#include "dmsm.h"

#define GIGA_ORDER 1000000000L
#define DMSM_DOUBLE_TIME_TYPE double
#define DMSM_DOUBLE_TIME_MPI_TYPE MPI_DOUBLE
#define DMSM_DOUBLE_TIME_MPI_TYPE_FACTOR 1
#define DMSM_ACTUAL_TIME_FUNCTION for_clock_gettime()
#define DMSM_TIME_REDUCING_FACTOR 1.0e0
/* Choices: clock_gettime(CLOCK_REALTIME, &t_temp), gethrtime(), clock(), time(NULL) with types
            struct timespec t_temp, hrtime_t, clock_t, and time_t respectively */

DMSM_DOUBLE_TIME_TYPE DMSM_ACTUAL_TIME_FUNCTION;
DMSM_DOUBLE_TIME_TYPE DMSM_ACTUAL_TIME_FUNCTION{
 struct timespec t_temp;
 DMSM_DOUBLE_TIME_TYPE l;
 clock_gettime(CLOCK_REALTIME, &t_temp);
 l=(DMSM_DOUBLE_TIME_TYPE)((DMSM_DOUBLE_TIME_TYPE)t_temp.tv_sec+
   ((DMSM_DOUBLE_TIME_TYPE)(t_temp.tv_nsec))/((DMSM_DOUBLE_TIME_TYPE)GIGA_ORDER));
 return(l);
}




void (*DMSM_Do_The_Job) (int);
/* This function pointer will point
to a user's function in the form:
void Do_The_Job(int my_job);
to complete job number my_job.
*/


void (*DMSM_Job_Group_Preparation) (int,int,int,int);
/* This function pointer will point
to a user's function in the form:
void Job_Group_Preparation(int job_from, int job_to,
                       int my_rank, int destination);
to prepare data for the whole job
group of jobs job_from through job_to, if needed.
*/


void (*DMSM_Result_Collection) (int,int);
/* This function pointer will point
to a user's function in the form:
void To_Collect_Some_Results(int my_rank, int from));
to collect available results from process of rank
from to the master, if needed.
*/


void DMSM_Set_Job_Signals(int group_number);
void DMSM_Unset_A_Job_Signal(int job_number);
void DMSM_Wait_For_Job_Signals();


void DMSM_Set_Job_Signals_Small();
void DMSM_Unset_A_Job_Signal_Small();
void DMSM_Wait_For_Job_Signals_Small();


void DMSM_Set_Master(int m);




#define DMSM_IDLING_CONSTANT     -103
#define DMSM_JOB_RECORDER_LENTH     3
#define DMSM_GROUP_INFORMATION_SIZE 3
#define DMSM_JOB_RANGE_SIZE         3
#define DMSM_ONLY_MPI_NO_MORE_JOBS -1
#define DMSM_MPI_TAG_ZERO_POINT    10

int DMSM_MASTER=DMSM_ALWAYS_ZERO;

FILE * DMSM_fileout;
FILE * DMSM_file_problem;


int  DMSM_My_MPI_Rank, DMSM_Total_MPI_Processes;
int  DMSM_My_Thread_Number, DMSM_Total_Threads;
int  DMSM_Total_Jobs_I_Finished, DMSM_a_Tag_for_MPI;
int  DMSM_Job_Assigned, DMSM_Job_From, DMSM_Job_To, DMSM_My_Job;
#pragma omp threadprivate(DMSM_My_Thread_Number, DMSM_Total_Threads, DMSM_Total_Jobs_I_Finished, DMSM_a_Tag_for_MPI,DMSM_My_Job)
int  *DMSM_Jobs_Finished_By_Node;
int  *DMSM_Jobs_Finished_All;
int  *DMSM_Jobs_I_Did, *DMSM_Sum_Jobs_I_Did;
int  *DMSM_Jobs_Assigned_By_Master, *DMSM_Jobs_Assigned_To_Thread_By_Master;
int  *DMSM_Times_No_More_Work_I_Received, *DMSM_Sum_Times_No_More_Work_I_Received, *DMSM_Times_No_More_Work_Instructed;
int  *DMSM_No_More_Groups_From,*DMSM_No_More_Groups_To,DMSM_Total_Times_Of_No_More_Groups_In_MPI;
int  DMSM_Total_Number_Of_Threads_Per_Process,*DMSM_Total_Number_Of_Threads_Per_Process_Got;
int  DMSM_First_Groups_By_Master_Threads,DMSM_First_Groups_By_Master_Threads_Enabled,DMSM_First_Jobs_By_Master_Threads;
int  DMSM_Job_Distribution_Plan;
int  DMSM_End_Of_Group_Jobs;
int  DMSM_Idling_Signal;
int  DMSM_Total_Num_Of_Job_Groups,DMSM_Num_Of_Jobs_Per_Group,DMSM_Total_Num_Of_Jobs,DMSM_Job_Group_Counter;
DMSM_DOUBLE_TIME_TYPE DMSM_Wall_Beginning_Time, DMSM_Wall_Ending_Time;
DMSM_DOUBLE_TIME_TYPE DMSM_Beginning_Time, DMSM_Ending_Time;
#pragma omp threadprivate(DMSM_Beginning_Time, DMSM_Ending_Time)
DMSM_DOUBLE_TIME_TYPE *DMSM_CPU_Time_Of_Jobs;
DMSM_DOUBLE_TIME_TYPE *DMSM_CPU_Time_Of_All_Jobs;
DMSM_DOUBLE_TIME_TYPE DMSM_Total_Job_CPU_Time;
DMSM_DOUBLE_TIME_TYPE *DMSM_Used_Time_By_Node;
DMSM_DOUBLE_TIME_TYPE *DMSM_Used_Time_All;
int  *DMSM_Job_Signals_Small;
int  *DMSM_Job_Signals_Big;
int  DMSM_Total_Job_Signals_Big;

MPI_Comm DMSM_Communicator;
int  DMSM_Set_Comm_Done=0;
int  DMSM_Initial_Data_Preparation_Enabled=0;
int  DMSM_Result_Collection_Enabled=0;

int  DMSM_Total_Slave_Gone_Home=0;
int  DMSM_Total_Slave_Should_Home=0;
int  DMSM_Total_Master_Thread_Gone_Home=0;
int  DMSM_Total_Master_Thread_Should_Home=0;

omp_lock_t       DMSM_Job_Node_Result_Lock;
omp_lock_t       DMSM_Job_Final_Result_Lock;
omp_lock_t      *DMSM_Job_Initial_Data_Locks;

int  DMSM_Initial_Data_Locks_Employed=1;
int  DMSM_Jobs_Done_By_Whom_Switch=1;
int  DMSM_Time_Switch=1;

int  *DMSM_Job_Group_Information;
int  DMSM_Current_Group_Number;
int  DMSM_Job_Range[DMSM_JOB_RANGE_SIZE];

int  DMSM_All_Jobs_Completed=0;
int  DMSM_Already_Initialized=0;

int  DMSM_Only_MPI_Enabled=0;
int  DMSM_Set_Comm_10_Called=0;
MPI_Comm  DMSM_Only_MPI_All_Comm;
MPI_Comm  DMSM_Only_MPI_Job_Comm;
MPI_Comm  DMSM_Only_MPI_Dist_Comm;
int  DMSM_Only_MPI_All_Rank;
int  DMSM_Only_MPI_Job_Rank;
int  DMSM_Only_MPI_Dist_Rank;
int  DMSM_Only_MPI_All_Processes;
int  DMSM_Only_MPI_Job_Processes;
int  DMSM_Only_MPI_Dist_Processes;
int *DMSM_Only_MPI_Job_Processes_T;
int  DMSM_Only_MPI_Job_Processes_E;
int  DMSM_JOB_MASTER=DMSM_ALWAYS_ZERO;
char DMSM_Journal_Head[]="DMSM_journal";
char DMSM_Problem_Head[]="DMSM_problem";
long DMSM_Journal_Number=(long)0;
int  DMSM_ANY_ERROR_FOUND=0;

void DMSM_Serial_Version();
void DMSM_Pure_OPENMP_Model();
void DMSM_Pure_MPI_Model();
void DMSM_Model_Plan11();
void DMSM_Model_Plan12();
void DMSM_Model_Plan13();
void DMSM_Model_Plan21();
void DMSM_Model_Plan22();
void DMSM_Model_Plan23();
void DMSM_Model_Plan31();
void DMSM_Model_Plan32();
void DMSM_Model_Plan33();
void DMSM_Get_Job();
void DMSM_Get_Critical_Job();
void DMSM_Job_Recorder(int p);
void DMSM_Set_Initial_Lock();
void DMSM_Do_Nothing2(int my_rank, int destination);
void DMSM_Do_Nothing4(int from, int to, int my_rank, int destination);
void DMSM_Last_Result_Collection();
void DMSM_Master_Threads_Dynamic();
void DMSM_Master_Threads_Allocate();

void DMSM_Master_All();
void DMSM_Next_Job_Range();
void DMSM_Master_Records(int, int);
void DMSM_Master_Assign_Slave(int, int, int);
void DMSM_Group_Prepare_Wraper(int, int, int);
void DMSM_Result_Collection_Locked(int myrank, int from);
void DMSM_All_Slaves_in_OPENMP();
void DMSM_One_Slave_in_OPENMP();
void DMSM_Slave_Outside_OPENMP();
void DMSM_Do_the_Job_Group();
void DMSM_Do_the_Job_Wraper();
void DMSM_Slave_Applying();
void DMSM_Master_Pure_MPI();
void DMSM_Slaves_Pure_MPI();
void DMSM_Do_the_Serial_Job_Wraper();
void DMSM_Serial_Core();
void DMSM_Error_and_Handling();
void DMSM_Open_Journal_File();
void DMSM_Open_Problem_File();

DMSM_DOUBLE_TIME_TYPE DMSM_CPU_Time_In_OpenMP();




DMSM_DOUBLE_TIME_TYPE DMSM_CPU_Time_In_OpenMP()
{DMSM_DOUBLE_TIME_TYPE  l;
#pragma omp critical (DMSM_Tm)
        {l=(DMSM_DOUBLE_TIME_TYPE)(DMSM_ACTUAL_TIME_FUNCTION/DMSM_TIME_REDUCING_FACTOR);}
return(l);
}




void DMSM_Initialize(int Total_Number_Of_Threads_Per_Process,
                     int Job_Distribution_Plan,
                     int Total_Num_Of_Jobs,
                     int Num_Of_Jobs_Per_Group)
{ int i,j,tt;

  DMSM_ANY_ERROR_FOUND=0;

  if(DMSM_Set_Comm_Done == 0) DMSM_Set_Comm(MPI_COMM_WORLD);

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {DMSM_Open_Problem_File();
     fprintf(DMSM_file_problem," 0\n");
     fclose(DMSM_file_problem);}

  DMSM_Wall_Beginning_Time=(DMSM_DOUBLE_TIME_TYPE)(DMSM_ACTUAL_TIME_FUNCTION/DMSM_TIME_REDUCING_FACTOR);

  DMSM_Total_Number_Of_Threads_Per_Process=Total_Number_Of_Threads_Per_Process;
  MPI_Bcast(&DMSM_Total_Number_Of_Threads_Per_Process,1,MPI_INT,DMSM_MASTER,DMSM_Communicator);
  if(DMSM_Total_Number_Of_Threads_Per_Process < 2) DMSM_Total_Number_Of_Threads_Per_Process=1;

  tt=1;
  if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
    {omp_set_num_threads(DMSM_Total_Number_Of_Threads_Per_Process);
#pragma omp parallel default(shared)
  {if(omp_get_thread_num() == 0) tt=omp_get_num_threads();}
     }

  if((DMSM_Total_Number_Of_Threads_Per_Process_Got=(int *) malloc(DMSM_Total_MPI_Processes*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {DMSM_Total_Number_Of_Threads_Per_Process_Got[i]=0;}

  MPI_Gather(&tt,1,MPI_INT,DMSM_Total_Number_Of_Threads_Per_Process_Got,1,MPI_INT,DMSM_MASTER,DMSM_Communicator);
  MPI_Bcast( DMSM_Total_Number_Of_Threads_Per_Process_Got,DMSM_Total_MPI_Processes,MPI_INT,DMSM_MASTER,DMSM_Communicator);

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(DMSM_Total_Number_Of_Threads_Per_Process<DMSM_Total_Number_Of_Threads_Per_Process_Got[i])
         DMSM_Total_Number_Of_Threads_Per_Process=DMSM_Total_Number_Of_Threads_Per_Process_Got[i];}

  DMSM_Job_Distribution_Plan=Job_Distribution_Plan;
  MPI_Bcast(&DMSM_Job_Distribution_Plan,1,MPI_INT,DMSM_MASTER,DMSM_Communicator);

  DMSM_Total_Num_Of_Jobs=Total_Num_Of_Jobs;
  MPI_Bcast(&DMSM_Total_Num_Of_Jobs,1,MPI_INT,DMSM_MASTER,DMSM_Communicator);

  DMSM_Num_Of_Jobs_Per_Group=Num_Of_Jobs_Per_Group;
  MPI_Bcast(&DMSM_Num_Of_Jobs_Per_Group,1,MPI_INT,DMSM_MASTER,DMSM_Communicator);

  DMSM_Idling_Signal=DMSM_IDLING_CONSTANT;
  DMSM_Total_Num_Of_Job_Groups=(DMSM_Total_Num_Of_Jobs-1)/DMSM_Num_Of_Jobs_Per_Group+1;
  DMSM_End_Of_Group_Jobs=-100-2*DMSM_Num_Of_Jobs_Per_Group;

  if((DMSM_Jobs_I_Did=(int *) malloc(DMSM_JOB_RECORDER_LENTH*(DMSM_Total_Num_Of_Jobs+1)*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}
  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {if((DMSM_Sum_Jobs_I_Did=(int *) malloc(DMSM_JOB_RECORDER_LENTH*(DMSM_Total_Num_Of_Jobs+1)*sizeof(int)))==NULL)
        {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
         MPI_Finalize(); exit(0);}}
  else
    {if((DMSM_Sum_Jobs_I_Did=(int *) malloc(DMSM_JOB_RECORDER_LENTH*sizeof(int)))==NULL)
        {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
         MPI_Finalize(); exit(0);}}

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {if((DMSM_Jobs_Assigned_By_Master=(int *) malloc((DMSM_Total_Num_Of_Jobs+1)*sizeof(int)))==NULL)
       {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
        MPI_Finalize(); exit(0);}}
  else
    {if((DMSM_Jobs_Assigned_By_Master=(int *) malloc((1+1)*sizeof(int)))==NULL)
       {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
        MPI_Finalize(); exit(0);}}

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {if((DMSM_Jobs_Assigned_To_Thread_By_Master=(int *) malloc((DMSM_Total_Num_Of_Jobs+1)*sizeof(int)))==NULL)
       {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
        MPI_Finalize(); exit(0);}}
  else
    {if((DMSM_Jobs_Assigned_To_Thread_By_Master=(int *) malloc((1+1)*sizeof(int)))==NULL)
       {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
        MPI_Finalize(); exit(0);}}

  if((DMSM_No_More_Groups_From=(int *) malloc(DMSM_Total_MPI_Processes*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}
  if((DMSM_No_More_Groups_To=(int *) malloc(DMSM_Total_MPI_Processes*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Times_No_More_Work_I_Received=(int *) malloc(DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}
  if((DMSM_Sum_Times_No_More_Work_I_Received=(int *) malloc(DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}
  if((DMSM_Times_No_More_Work_Instructed=(int *) malloc(DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Job_Initial_Data_Locks=(omp_lock_t *) malloc(DMSM_Total_Number_Of_Threads_Per_Process*sizeof(omp_lock_t)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Job_Signals_Small=(int *) malloc(DMSM_Total_Number_Of_Threads_Per_Process*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Job_Signals_Big=(int *) malloc(DMSM_Num_Of_Jobs_Per_Group*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Jobs_Finished_By_Node=(int *) malloc(DMSM_Total_Number_Of_Threads_Per_Process*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Jobs_Finished_All=(int *) malloc(DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Used_Time_By_Node=(DMSM_DOUBLE_TIME_TYPE *) malloc(DMSM_Total_Number_Of_Threads_Per_Process*sizeof(DMSM_DOUBLE_TIME_TYPE)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Used_Time_All=(DMSM_DOUBLE_TIME_TYPE *) malloc(DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process*sizeof(DMSM_DOUBLE_TIME_TYPE)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_CPU_Time_Of_Jobs=(DMSM_DOUBLE_TIME_TYPE *) malloc((DMSM_Total_Num_Of_Jobs+1)*sizeof(DMSM_DOUBLE_TIME_TYPE)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {if((DMSM_CPU_Time_Of_All_Jobs=(DMSM_DOUBLE_TIME_TYPE *) malloc((DMSM_Total_Num_Of_Jobs+1)*sizeof(DMSM_DOUBLE_TIME_TYPE)))==NULL)
       {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
        MPI_Finalize(); exit(0);}}
  else
    {if((DMSM_CPU_Time_Of_All_Jobs=(DMSM_DOUBLE_TIME_TYPE *) malloc(1*sizeof(DMSM_DOUBLE_TIME_TYPE)))==NULL)
       {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
        MPI_Finalize(); exit(0);}}

  if((DMSM_Job_Group_Information=(int *) malloc(DMSM_GROUP_INFORMATION_SIZE*(DMSM_Total_Num_Of_Jobs+1)*sizeof(int)))==NULL)
     {printf( "Error on memory allocation in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if((DMSM_Job_Distribution_Plan<11) || (33<DMSM_Job_Distribution_Plan))
     {printf( "Error for Job Distribution Plan: %d " , DMSM_Job_Distribution_Plan);
      printf( " in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}
  if(DMSM_Total_MPI_Processes<1)
     {printf( "Error for Number of Processes: %d" , DMSM_Total_MPI_Processes);
      printf( " in DMSM_Initialize().\n" );
      MPI_Finalize(); exit(0);}
  if((DMSM_My_MPI_Rank<0) || (DMSM_Total_MPI_Processes<=DMSM_My_MPI_Rank))
     {printf( "Error for Processe Rank number: %d" ,    DMSM_My_MPI_Rank);
      printf( " in DMSM_Initialize().\n" );
      MPI_Finalize(); exit(0);}
  if(DMSM_Total_Num_Of_Jobs<1)
     {printf( "Error for Total Number of Jobs: %d " , DMSM_Total_Num_Of_Jobs);
      printf( " in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}
  if(DMSM_Num_Of_Jobs_Per_Group<1)
     {printf( "Error for Number of Jobs per Group: %d " , DMSM_Num_Of_Jobs_Per_Group);
      printf( " in DMSM_Initialize().\n") ;
      MPI_Finalize(); exit(0);}

  if(DMSM_Total_Number_Of_Threads_Per_Process<1)
     {printf( "Error for Number of Threads Per Process: %d" , DMSM_Total_Number_Of_Threads_Per_Process);
      printf( " in DMSM_Initialize().\n" );
      MPI_Finalize(); exit(0);}

  for(i=0;i<DMSM_Num_Of_Jobs_Per_Group;i++)  DMSM_Job_Signals_Big[i]=0;
  for(i=0;i<DMSM_Total_Number_Of_Threads_Per_Process;i++)  DMSM_Job_Signals_Small[i]=0;
  for(i=0;i<DMSM_Total_Number_Of_Threads_Per_Process;i++)
      omp_init_lock(&DMSM_Job_Initial_Data_Locks[i]);
  omp_init_lock(&DMSM_Job_Node_Result_Lock);
  omp_init_lock(&DMSM_Job_Final_Result_Lock);

  for(i=0;i<(DMSM_Total_Num_Of_Jobs+1)*DMSM_JOB_RECORDER_LENTH;i++)     DMSM_Jobs_I_Did[i]=0;
  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {for(i=0;i<(DMSM_Total_Num_Of_Jobs+1)*DMSM_JOB_RECORDER_LENTH;i++)  DMSM_Sum_Jobs_I_Did[i]=0;}
  else
    {for(i=0;i<DMSM_JOB_RECORDER_LENTH;i++) DMSM_Sum_Jobs_I_Did[i]=0;}
  if(DMSM_My_MPI_Rank == DMSM_MASTER)
  for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++)       DMSM_Jobs_Assigned_By_Master[i]=-1;
  if(DMSM_My_MPI_Rank == DMSM_MASTER)
  for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++) DMSM_Jobs_Assigned_To_Thread_By_Master[i]=-1;
  for(i=0;i<DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process;i++) DMSM_Times_No_More_Work_I_Received[i]=0;
  for(i=0;i<DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process;i++) DMSM_Sum_Times_No_More_Work_I_Received[i]=0;
  for(i=0;i<DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process;i++) DMSM_Times_No_More_Work_Instructed[i]=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++) DMSM_No_More_Groups_From[i]=1;
  for(i=0;i<DMSM_Total_MPI_Processes;i++) DMSM_No_More_Groups_To[i]=-1;

  for(i=0;i<DMSM_Total_Number_Of_Threads_Per_Process;i++) DMSM_Jobs_Finished_By_Node[i]=0;
  for(i=0;i<DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process;i++) DMSM_Jobs_Finished_All[i]=0;

  for(i=0;i<DMSM_GROUP_INFORMATION_SIZE*(DMSM_Total_Num_Of_Jobs+1);i++) DMSM_Job_Group_Information[i]=-1;

  for(i=0;i<DMSM_Total_Number_Of_Threads_Per_Process;i++) DMSM_Used_Time_By_Node[i]=(DMSM_DOUBLE_TIME_TYPE)0;
  for(i=0;i<DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process;i++) DMSM_Used_Time_All[i]=(DMSM_DOUBLE_TIME_TYPE)0;

  for(i=0;i<(DMSM_Total_Num_Of_Jobs+1);i++)     DMSM_CPU_Time_Of_Jobs[i]=(DMSM_DOUBLE_TIME_TYPE)0;
  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {for(i=0;i<(DMSM_Total_Num_Of_Jobs+1);i++)  DMSM_CPU_Time_Of_All_Jobs[i]=(DMSM_DOUBLE_TIME_TYPE)0;}
  else
    {for(i=0;i<1;i++) DMSM_CPU_Time_Of_All_Jobs[i]=(DMSM_DOUBLE_TIME_TYPE)0;}

  /* if((DMSM_Total_MPI_Processes<2) && (DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]<2))
     {printf( "Error for Number of Processes and Number of Threads: %d %d" , DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]);
      printf( " in Data_Initialize().\n" );
      MPI_Finalize(); exit(0);} */

  DMSM_Current_Group_Number=0;


  DMSM_First_Groups_By_Master_Threads_Enabled=0;
  if((DMSM_Job_Distribution_Plan >= 20) && (DMSM_Job_Distribution_Plan < 30))
    {DMSM_First_Groups_By_Master_Threads_Enabled=1;
     if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]<2) DMSM_First_Groups_By_Master_Threads_Enabled=0;
     }
  if(DMSM_Total_MPI_Processes < 2) DMSM_First_Groups_By_Master_Threads_Enabled=1;



  if(DMSM_First_Groups_By_Master_Threads_Enabled)
    {DMSM_First_Groups_By_Master_Threads=DMSM_Total_Num_Of_Job_Groups/(2*DMSM_Total_MPI_Processes);
     if(DMSM_Total_MPI_Processes==1) DMSM_First_Groups_By_Master_Threads=DMSM_Total_Num_Of_Job_Groups;
     DMSM_First_Jobs_By_Master_Threads=DMSM_First_Groups_By_Master_Threads*DMSM_Num_Of_Jobs_Per_Group;
     if(DMSM_Total_Num_Of_Jobs<DMSM_First_Jobs_By_Master_Threads)DMSM_First_Jobs_By_Master_Threads=DMSM_Total_Num_Of_Jobs;
     if(DMSM_My_MPI_Rank == DMSM_MASTER)
     for(i=1;i<=DMSM_First_Jobs_By_Master_Threads;i++) DMSM_Jobs_Assigned_By_Master[i]=DMSM_MASTER;
    }
  else
    {DMSM_First_Jobs_By_Master_Threads=0;
     DMSM_First_Groups_By_Master_Threads=0;
    }

  DMSM_Job_Range[0]=DMSM_First_Jobs_By_Master_Threads;
  DMSM_Job_Range[1]=DMSM_First_Jobs_By_Master_Threads;
  DMSM_Job_Range[2]=DMSM_First_Groups_By_Master_Threads;

  DMSM_Job_From=2;
  DMSM_Job_To=0;
  /*if((DMSM_My_MPI_Rank==DMSM_MASTER) && DMSM_First_Groups_By_Master_Threads_Enabled)
    {DMSM_Job_From=1;
     DMSM_Job_To=DMSM_First_Jobs_By_Master_Threads;
    } */
  DMSM_Job_Assigned=DMSM_Job_From-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {DMSM_Open_Journal_File();
     fprintf(DMSM_fileout," \n");
     fprintf(DMSM_fileout," ******************************************* \n");
     fprintf(DMSM_fileout," * Double-layer Master-Slave Model library * \n");
     fprintf(DMSM_fileout," * Version 4.3  by   HPCVL.org 2015        * \n");
     fprintf(DMSM_fileout," ******************************************* \n \n \n");
     fprintf(DMSM_fileout," Job Distribution Plan:                                %3d. \n", DMSM_Job_Distribution_Plan);
     fprintf(DMSM_fileout," Total number of jobs:                %20d.\n", DMSM_Total_Num_Of_Jobs);
     fprintf(DMSM_fileout," Total number of jobs per group:      %20d.\n", DMSM_Num_Of_Jobs_Per_Group);
     if(DMSM_First_Groups_By_Master_Threads_Enabled)
     fprintf(DMSM_fileout," Total number of jobs allocated to master node: %10d.\n", DMSM_First_Jobs_By_Master_Threads);

     if(DMSM_Only_MPI_Enabled != 1)
       {fprintf(DMSM_fileout," Total number of processes/nodes:                   %6d.\n", DMSM_Total_MPI_Processes);
        fprintf(DMSM_fileout," Total number of threads per process expected:      %6d.\n", DMSM_Total_Number_Of_Threads_Per_Process);
        fprintf(DMSM_fileout,"                 and in reality in rank order:");
        for(i=0;i<DMSM_Total_MPI_Processes;i++)
        fprintf(DMSM_fileout," %d ", DMSM_Total_Number_Of_Threads_Per_Process_Got[i]);
        fprintf(DMSM_fileout," . \n");
        }
     else
       {fprintf(DMSM_fileout," Total number of processes:                         %6d.\n", DMSM_Only_MPI_All_Processes);
        fprintf(DMSM_fileout," Number of processes per job communicator expected: %6d.\n", DMSM_Only_MPI_Job_Processes_E);
        fprintf(DMSM_fileout," Total number of job communicators generated:       %6d.\n", DMSM_Total_MPI_Processes);
        fprintf(DMSM_fileout," Number of processes per job communicator in order: \n");
        fprintf(DMSM_fileout,"                 ");
        for(i=0;i<DMSM_Total_MPI_Processes;i++)
        fprintf(DMSM_fileout," %d ", DMSM_Only_MPI_Job_Processes_T[i]);
        fprintf(DMSM_fileout," . \n");
        }

/*     fprintf(DMSM_fileout," Inside DMSM lib, \n");
       if(DMSM_Initial_Data_Locks_Employed==1) {fprintf(DMSM_fileout,"       OpenMP Locks activated \n");}
       else
       {if(DMSM_Initial_Data_Locks_Employed==0){fprintf(DMSM_fileout,"       integer signals activated \n");}
        else                          {fprintf(DMSM_fileout,"       both OpenMP Locks and integer signals used \n");}}
       fprintf(DMSM_fileout,"       for avoiding race conditions in job initial data. \n");
       fprintf(DMSM_fileout,"       This is a technique, then users can ignore. \n");  */

     fprintf(DMSM_fileout,"       Method of job grouping       :  %d \n", DMSM_JOB_GROUPING);
     fprintf(DMSM_fileout,"       Result collection in parallel:  %d \n", DMSM_COLLECT_RESULTS_PARALLEL);
     fprintf(DMSM_fileout," \n");
     fprintf(DMSM_fileout," *** Job (group) distribution will be checked up later. *** \n");
     fprintf(DMSM_fileout," *** If you do not see a check up passing message,      *** \n");
     fprintf(DMSM_fileout," *** the check up failed.                               *** \n");
     fprintf(DMSM_fileout," \n");
    }


  DMSM_Total_Jobs_I_Finished=0;
  DMSM_My_Thread_Number=0;
  DMSM_Beginning_Time=(DMSM_DOUBLE_TIME_TYPE)(DMSM_ACTUAL_TIME_FUNCTION/DMSM_TIME_REDUCING_FACTOR);
  DMSM_a_Tag_for_MPI=DMSM_MPI_TAG_ZERO_POINT+DMSM_My_Thread_Number+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process;
  if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
    {
#pragma omp parallel default(shared)
       {
        DMSM_Total_Jobs_I_Finished=0;
        DMSM_My_Thread_Number=omp_get_thread_num();
        DMSM_a_Tag_for_MPI=DMSM_MPI_TAG_ZERO_POINT+DMSM_My_Thread_Number+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process;
        DMSM_Total_Threads=omp_get_num_threads();
        if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
          {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
           DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
           MPI_Finalize(); exit(0);}
        DMSM_Beginning_Time=DMSM_CPU_Time_In_OpenMP();
       }
    }

  DMSM_Total_Slave_Gone_Home=0;
  DMSM_Total_Master_Thread_Gone_Home=0;

  DMSM_All_Jobs_Completed=0;
  DMSM_Already_Initialized=1;

}




void DMSM_Finalize()
{ int i;
  free(DMSM_Jobs_I_Did);
  free(DMSM_Sum_Jobs_I_Did);
  free(DMSM_Jobs_Assigned_By_Master);
  free(DMSM_Jobs_Assigned_To_Thread_By_Master);
  free(DMSM_Times_No_More_Work_I_Received);
  free(DMSM_Sum_Times_No_More_Work_I_Received);
  free(DMSM_Times_No_More_Work_Instructed);
  free(DMSM_No_More_Groups_From);
  free(DMSM_No_More_Groups_To);
  free(DMSM_Job_Signals_Big);
  free(DMSM_Job_Signals_Small);
  for(i=0;i<DMSM_Total_Number_Of_Threads_Per_Process;i++)
      omp_destroy_lock(&DMSM_Job_Initial_Data_Locks[i]);
  omp_destroy_lock(&DMSM_Job_Node_Result_Lock);
  omp_destroy_lock(&DMSM_Job_Final_Result_Lock);
  free(DMSM_Job_Initial_Data_Locks);
  free(DMSM_Total_Number_Of_Threads_Per_Process_Got);
  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     fclose(DMSM_fileout);
  free(DMSM_Jobs_Finished_By_Node);
  free(DMSM_Jobs_Finished_All);
  free(DMSM_Used_Time_By_Node);
  free(DMSM_Used_Time_All);
  free(DMSM_CPU_Time_Of_Jobs);
  free(DMSM_CPU_Time_Of_All_Jobs);
  free(DMSM_Job_Group_Information);
  MPI_Comm_free(&DMSM_Communicator);
  DMSM_Set_Comm_Done=0;
  DMSM_All_Jobs_Completed=0;
  DMSM_Already_Initialized=0;
  DMSM_Journal_Number=(long)0;
}




void DMSM_Set_Initial_Lock()
{
 if(omp_in_parallel() && DMSM_Initial_Data_Preparation_Enabled)
   {
    if(DMSM_Initial_Data_Locks_Employed!=0)
      {
       /*omp_unset_lock(&DMSM_Job_Initial_Data_Locks[DMSM_My_Thread_Number]); */
#pragma omp flush(DMSM_Job_Initial_Data_Locks)
       omp_set_lock(  &DMSM_Job_Initial_Data_Locks[DMSM_My_Thread_Number]);
#pragma omp flush(DMSM_Job_Initial_Data_Locks)

/*
   int i,m;
   m=10000;
   for(i=0; i<=m; i++)
       if(omp_test_lock(&DMSM_Job_Initial_Data_Locks[DMSM_My_Thread_Number]))
         {break;}
       else
         {if(i==m)
            {
             printf( "Error, too many tries in DMSM_Set_Initial_Data_Lock but failed: %d %d.\n", i, m);
             MPI_Finalize(); exit(0);
            }
         }
*/
       };
    if(DMSM_Initial_Data_Locks_Employed!=1)
      {
       DMSM_Job_Signals_Small[DMSM_My_Thread_Number]=1;
#pragma omp flush(DMSM_Job_Signals_Small)
       DMSM_Job_Signals_Small[DMSM_My_Thread_Number]=DMSM_Job_Signals_Small[DMSM_My_Thread_Number]+1;
       DMSM_Job_Signals_Small[DMSM_My_Thread_Number]=DMSM_Job_Signals_Small[DMSM_My_Thread_Number]+1;
#pragma omp flush(DMSM_Job_Signals_Small)
       }
   }
}




void DMSM_Unset_An_Initial_Lock()
{
 if(omp_in_parallel() && DMSM_Initial_Data_Preparation_Enabled)
   {
    if(DMSM_Initial_Data_Locks_Employed!=0)
      {
#pragma omp flush(DMSM_Job_Initial_Data_Locks)
       omp_unset_lock(&DMSM_Job_Initial_Data_Locks[DMSM_My_Thread_Number]);
#pragma omp flush(DMSM_Job_Initial_Data_Locks)
      }
    if(DMSM_Initial_Data_Locks_Employed!=1)
       DMSM_Job_Signals_Small[DMSM_My_Thread_Number]=0;
    }
}




void DMSM_Wait_For_Initial_Locks()
{
 int i;
 if(omp_in_parallel() && DMSM_Initial_Data_Preparation_Enabled)
   {
    if(DMSM_Initial_Data_Locks_Employed!=0)
      {
       for(i=0; i<DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]; i++)
          {
#pragma omp flush(DMSM_Job_Initial_Data_Locks)
           omp_set_lock(&DMSM_Job_Initial_Data_Locks[i]);
#pragma omp flush(DMSM_Job_Initial_Data_Locks)
           omp_unset_lock(&DMSM_Job_Initial_Data_Locks[i]);
#pragma omp flush(DMSM_Job_Initial_Data_Locks)
           }
       };
    if(DMSM_Initial_Data_Locks_Employed!=1)
      {
       for(i=0; i<DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]; i++)
          { while(1)
            {if(DMSM_Job_Signals_Small[i]==0) break;
#pragma omp flush(DMSM_Job_Signals_Small)
             }
           }
       }
   }
}




void DMSM_Set_Job_Signals_Small()
{
   DMSM_Job_Signals_Small[DMSM_My_Thread_Number]=1;
#pragma omp flush(DMSM_Job_Signals_Small)
}




void DMSM_Unset_A_Job_Signal_Small()
{
   DMSM_Job_Signals_Small[DMSM_My_Thread_Number]=0;
}




void DMSM_Wait_For_Job_Signals_Small()
{  int i;
   for(i=0; i<DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]; i++)
      { while(1)
          {if(DMSM_Job_Signals_Small[i]==0) break;
#pragma omp flush(DMSM_Job_Signals_Small)
          }
      }
}




void DMSM_Set_Job_Signals(int group_number)
{  int i;

   if((group_number<0) || (DMSM_Total_Num_Of_Job_Groups<=group_number))
     {printf( "Error, unacceptable group number in DMSM_Set_Job_Signals: %d %d %d.\n", group_number,
      DMSM_Total_Num_Of_Job_Groups, DMSM_My_MPI_Rank) ;
      MPI_Finalize(); exit(0);}

   if(group_number<DMSM_Total_Num_Of_Job_Groups-1)
      DMSM_Total_Job_Signals_Big=DMSM_Num_Of_Jobs_Per_Group;
   else
      DMSM_Total_Job_Signals_Big=DMSM_Total_Num_Of_Jobs-
      ((DMSM_Total_Num_Of_Jobs-1)/DMSM_Num_Of_Jobs_Per_Group)*DMSM_Num_Of_Jobs_Per_Group;

   for(i=0; i<DMSM_Total_Job_Signals_Big; i++) DMSM_Job_Signals_Big[i]=1;
   for(i=DMSM_Total_Job_Signals_Big; i<DMSM_Num_Of_Jobs_Per_Group; i++) DMSM_Job_Signals_Big[i]=0;
}




void DMSM_Unset_A_Job_Signal(int job_number)
{
   if((job_number<0) || (DMSM_Total_Num_Of_Jobs<=job_number))
     {printf( "Error, unacceptable job number in DMSM_Unset_A_Job_Signal: %d %d %d.\n", job_number,
      DMSM_Total_Num_Of_Jobs, DMSM_My_MPI_Rank) ;
      MPI_Finalize(); exit(0);}
   DMSM_Job_Signals_Big[job_number-(job_number/DMSM_Num_Of_Jobs_Per_Group)*DMSM_Num_Of_Jobs_Per_Group]=0;
}




void DMSM_Wait_For_Job_Signals()
{  int i;
   for(i=0; i<DMSM_Total_Job_Signals_Big; i++)
      { while(1)
          {if(DMSM_Job_Signals_Big[i]==0) break;
#pragma omp flush(DMSM_Job_Signals_Big)
          }
      }
}




void DMSM_Set_Node_Result_Lock()
     {if(omp_in_parallel())
        {
#pragma omp flush(DMSM_Job_Node_Result_Lock)
         omp_set_lock(&DMSM_Job_Node_Result_Lock);
#pragma omp flush(DMSM_Job_Node_Result_Lock)
        } }


void DMSM_Unset_Node_Result_Lock()
     {if(omp_in_parallel())
        {
#pragma omp flush(DMSM_Job_Node_Result_Lock)
         omp_unset_lock(&DMSM_Job_Node_Result_Lock);
#pragma omp flush(DMSM_Job_Node_Result_Lock)
        } }



void DMSM_Set_Final_Result_Lock()
     {if(omp_in_parallel())
        {
#pragma omp flush(DMSM_Job_Final_Result_Lock)
         omp_set_lock(&DMSM_Job_Final_Result_Lock);
#pragma omp flush(DMSM_Job_Final_Result_Lock)
        } }



void DMSM_Unset_Final_Result_Lock()
     {if(omp_in_parallel())
        {
#pragma omp flush(DMSM_Job_Final_Result_Lock)
         omp_unset_lock(&DMSM_Job_Final_Result_Lock);
#pragma omp flush(DMSM_Job_Final_Result_Lock)
        } }



int DMSM_Get_Job_Rank() {return(DMSM_Only_MPI_Job_Rank);}

int DMSM_Get_Job_Procs() {return(DMSM_Only_MPI_Job_Processes);}

MPI_Comm DMSM_Get_Job_Comm()   {return(DMSM_Only_MPI_Job_Comm);}


int DMSM_Get_Dist_Rank() {return(DMSM_Only_MPI_Dist_Rank);}

int DMSM_Get_Dist_Procs() {return(DMSM_Only_MPI_Dist_Processes);}

MPI_Comm DMSM_Get_Dist_Comm()   {return(DMSM_Only_MPI_Dist_Comm);}


int DMSM_Get_Distribution_Plan() {return(DMSM_Job_Distribution_Plan);}

int DMSM_Get_Tag() {return(DMSM_a_Tag_for_MPI);}


int DMSM_Get_Group_Start(int job_number)
    {if(DMSM_Job_Group_Information[DMSM_GROUP_INFORMATION_SIZE*job_number+0] < 1)
       {printf( "Error, job number %d is not assigned to rank %d, but requested for group information (start).\n",
                job_number, DMSM_My_MPI_Rank); MPI_Finalize(); exit(0);}
        return(DMSM_Job_Group_Information[DMSM_GROUP_INFORMATION_SIZE*job_number+0]-1);
     }


int DMSM_Get_Group_End(int job_number)
    {if(DMSM_Job_Group_Information[DMSM_GROUP_INFORMATION_SIZE*job_number+0] < 1)
       {printf( "Error, job number %d is not assigned to rank %d, but requested for group information (end).\n",
                job_number, DMSM_My_MPI_Rank); MPI_Finalize(); exit(0);}
        return(DMSM_Job_Group_Information[DMSM_GROUP_INFORMATION_SIZE*job_number+1]-1);
     }


int DMSM_Get_Group_Number(int job_number)
    {if(DMSM_Job_Group_Information[DMSM_GROUP_INFORMATION_SIZE*job_number+0] < 1)
       {printf( "Error, job number %d is not assigned to rank %d, but requested for group information (end).\n",
                job_number, DMSM_My_MPI_Rank); MPI_Finalize(); exit(0);}
        return(DMSM_Job_Group_Information[DMSM_GROUP_INFORMATION_SIZE*job_number+2]);
     }




void DMSM_Job_Distribution_Checkup()
{
   char pc='%';
   int gpi,i,j,k,l,m,t,i7,jmax,jmin;
   DMSM_DOUBLE_TIME_TYPE tm0, tm1, tm2, tmp, tmm0, t9mm0, tmm1, tmm2, tmmp;

   if(DMSM_All_Jobs_Completed==0)
     {printf( "Sorry! All jobs must be completed before checked up: %d.\n", DMSM_My_MPI_Rank) ;
      MPI_Finalize(); exit(0);}

   MPI_Reduce(DMSM_Jobs_I_Did,DMSM_Sum_Jobs_I_Did,DMSM_JOB_RECORDER_LENTH*(DMSM_Total_Num_Of_Jobs+1),MPI_INT,MPI_SUM,DMSM_MASTER,DMSM_Communicator);
   MPI_Reduce(DMSM_Times_No_More_Work_I_Received, DMSM_Sum_Times_No_More_Work_I_Received,
                           DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process, MPI_INT, MPI_SUM, DMSM_MASTER,DMSM_Communicator);

   DMSM_Jobs_Finished_By_Node[DMSM_ALWAYS_ZERO]=DMSM_Total_Jobs_I_Finished;
   if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
   {
#pragma omp parallel default(shared)
    {
     DMSM_My_Thread_Number=omp_get_thread_num();
     DMSM_Total_Threads=omp_get_num_threads();
     if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
       {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
        DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
        MPI_Finalize(); exit(0);}
     DMSM_Jobs_Finished_By_Node[DMSM_My_Thread_Number]=DMSM_Total_Jobs_I_Finished;
    }
   }

   MPI_Gather(DMSM_Jobs_Finished_By_Node,DMSM_Total_Number_Of_Threads_Per_Process,MPI_INT,
              DMSM_Jobs_Finished_All,    DMSM_Total_Number_Of_Threads_Per_Process,MPI_INT,
                                                           DMSM_MASTER,DMSM_Communicator);

   MPI_Reduce(DMSM_CPU_Time_Of_Jobs, DMSM_CPU_Time_Of_All_Jobs,
              DMSM_DOUBLE_TIME_MPI_TYPE_FACTOR*(DMSM_Total_Num_Of_Jobs+1),
              DMSM_DOUBLE_TIME_MPI_TYPE, MPI_SUM, DMSM_MASTER,DMSM_Communicator);

   if(DMSM_My_MPI_Rank == DMSM_MASTER)
      {
       for(i=1;i<=DMSM_Total_Num_Of_Jobs;i++)
          {
           if((i<=DMSM_First_Jobs_By_Master_Threads) && (DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1]!=0))
             {printf( "Job # %d should be done by process 0, but by %d.\n", i,
                                                    DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1]);
              DMSM_Error_and_Handling();}

           if((DMSM_Job_Distribution_Plan >= 20) && (DMSM_Job_Distribution_Plan < 30))
           if((i>DMSM_First_Jobs_By_Master_Threads) && (DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1]==DMSM_MASTER))
             {printf( "Job # %d should not be done by process 0, but by %d.\n", i,
                                                    DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1]);
              DMSM_Error_and_Handling();}

           if(DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1]!=DMSM_Jobs_Assigned_By_Master[i])
             {printf( "Job # %d was assigned to process %d by the Master, but done by %d.\n", i,
                          DMSM_Jobs_Assigned_By_Master[i],DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1]);
              DMSM_Error_and_Handling();}

           if(DMSM_Jobs_Assigned_To_Thread_By_Master[i] >= 0)
           if(DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+2]!=DMSM_Jobs_Assigned_To_Thread_By_Master[i])
             {printf( "Job # %d was assigned to thread %d by the Master, but done by %d.\n", i,
                          DMSM_Jobs_Assigned_To_Thread_By_Master[i],DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+2]);
              DMSM_Error_and_Handling();}

           if(DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i]!=1)
             {printf( "Job # %d has not been done once only, but %d times.\n", i, DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i]);
              DMSM_Error_and_Handling();}

          }

       for(m=0;m<DMSM_Total_MPI_Processes;m++)
          {for(i=0;i<DMSM_Total_Number_Of_Threads_Per_Process;i++)
              {k=0;
               if((DMSM_No_More_Groups_From[m]<=i)&&(i<=DMSM_No_More_Groups_To[m])) k=1;
               l=i+m*DMSM_Total_Number_Of_Threads_Per_Process;
               if(DMSM_Times_No_More_Work_Instructed[l] != k)
                 {printf( "Error: the master sent (%d %d) no-more-job signal %d times.\n",
                                                        m,i,DMSM_Times_No_More_Work_Instructed[l]);
                  DMSM_Error_and_Handling();}
               if(DMSM_Sum_Times_No_More_Work_I_Received[l] != k)
                 {printf( "Error: the thread (%d %d) received no-more-job signal %d times.\n",
                                                        m,i,DMSM_Sum_Times_No_More_Work_I_Received[l]);
                  DMSM_Error_and_Handling();}
              }
          }

       if(DMSM_ANY_ERROR_FOUND)
         {fprintf(DMSM_fileout,"                     \n");
          fprintf(DMSM_fileout,"                     \n");
          fprintf(DMSM_fileout," Some problem/error found! \n");
          fprintf(DMSM_fileout," Some problem/error found! \n");
          fprintf(DMSM_fileout," Some problem/error found! \n");
          printf("                     \n");
          printf("                     \n");
          printf(" Some problem/error found! \n");
          printf(" Some problem/error found! \n");
          printf(" Some problem/error found! \n");
          }
       else
         {fprintf(DMSM_fileout,"                     \n");
          fprintf(DMSM_fileout,"                     \n");
          fprintf(DMSM_fileout," The followings have been verified: \n");
          fprintf(DMSM_fileout,"       all jobs were done once and only ONCE              ; \n");
          fprintf(DMSM_fileout,"       no unwanted jobs were performed                    ; \n");
          fprintf(DMSM_fileout,"       group jobs sent to processes were done in them     ; \n");
          fprintf(DMSM_fileout,"       the first job of any distributed group was done      \n");
          fprintf(DMSM_fileout,"                   by the assigned thread, if applicable  ; \n");
          fprintf(DMSM_fileout,"       the master instructed all threads no-more-job ONCE   \n");
          fprintf(DMSM_fileout,"                   at the end when true, if applicable    ; \n");
          fprintf(DMSM_fileout,"       all threads received no-more-job ONCE at the end     \n");
          fprintf(DMSM_fileout,"                   when it became true,      if applicable. \n");
          fprintf(DMSM_fileout,"                     \n");
          fprintf(DMSM_fileout,"                     \n");

          printf("                     \n");
          printf("                     \n");
          printf(" The followings have been verified: \n");
          printf("       all jobs were done once and only ONCE              ; \n");
          printf("       no unwanted jobs were performed                    ; \n");
          printf("       group jobs sent to processes were done in them     ; \n");
          printf("       the first job of any distributed group was done      \n");
          printf("                   by the assigned thread, if applicable  ; \n");
          printf("       the master instructed all threads no-more-job ONCE   \n");
          printf("                   at the end when true, if applicable    ; \n");
          printf("       all threads received no-more-job ONCE at the end     \n");
          printf("                   when it became true,      if applicable. \n");
          printf("                     \n");
          }

       jmax=0;
       jmin=0;
       tm0=(DMSM_DOUBLE_TIME_TYPE)0;
       tmm0=(DMSM_DOUBLE_TIME_TYPE)(-5);

       DMSM_Jobs_Done_By_Whom_Switch=1;
       if(DMSM_Jobs_Done_By_Whom_Switch)
         {
          fprintf(DMSM_fileout," Job number, by Thread, Rank/Node, Times, and CPU-time used: \n");
          for(i=1;i<=DMSM_Total_Num_Of_Jobs;i++)
             {fprintf(DMSM_fileout," %10d %10d %10d %6d %18.4e \n", i-1, DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+2],
              DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1], DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i],
                                                                DMSM_CPU_Time_Of_All_Jobs[i]);
              if(tm0< DMSM_CPU_Time_Of_All_Jobs[i])  {tm0=DMSM_CPU_Time_Of_All_Jobs[i]; jmax=i;}

                   if(tmm0<(DMSM_DOUBLE_TIME_TYPE)0)    {tmm0=DMSM_CPU_Time_Of_All_Jobs[i]; jmin=i;}
              else if(tmm0>DMSM_CPU_Time_Of_All_Jobs[i]) {tmm0=DMSM_CPU_Time_Of_All_Jobs[i]; jmin=i;}


              k=DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+1];
              j=DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*i+2];
              DMSM_Used_Time_All[k*DMSM_Total_Number_Of_Threads_Per_Process+j]+=DMSM_CPU_Time_Of_All_Jobs[i];
             }
          if(DMSM_Total_Num_Of_Jobs>0) {
          fprintf(DMSM_fileout," The maximum CPU-time of the above is %f for job # %d, \n \n", tm0, jmax-1);
          fprintf(DMSM_fileout," and the minimum is %f for job # %d. \n \n", tmm0, jmin-1);}

          fprintf(DMSM_fileout,"                     \n");
          fprintf(DMSM_fileout,"                     \n");

          for(j=0;j<DMSM_Total_MPI_Processes;j++)
          for(i=0;i<DMSM_Total_Number_Of_Threads_Per_Process;i++)
             {fprintf(DMSM_fileout," Thread %d of Rank/Node %d completed jobs: \n",i,j);
              l=0; t=0;
              for(k=1;k<=DMSM_Total_Num_Of_Jobs;k++)
              if((i==DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*k+2]) &&
                 (j==DMSM_Sum_Jobs_I_Did[DMSM_JOB_RECORDER_LENTH*k+1]))
                {fprintf(DMSM_fileout," %8d",k-1);l++;t++;
                 if(l==10){fprintf(DMSM_fileout,"\n");l=0;}
                }
              if(l!=0){fprintf(DMSM_fileout,"\n");}
              fprintf(DMSM_fileout," Total: %d . \n \n",t);
              if(t!=DMSM_Jobs_Finished_All[i+j*DMSM_Total_Number_Of_Threads_Per_Process])
                {printf( "Error for total number of jobs completed: %d %d  by thread %d, rank %d.\n", t,
                         DMSM_Jobs_Finished_All[i+j*DMSM_Total_Number_Of_Threads_Per_Process],i,j);
                 DMSM_Error_and_Handling();}
             }

        }

      }

  DMSM_Total_Job_CPU_Time=(DMSM_DOUBLE_TIME_TYPE)0;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {t9mm0=0.9*tmm0;
     tm1=(DMSM_DOUBLE_TIME_TYPE)0;
     tmm1=(DMSM_DOUBLE_TIME_TYPE)(-5);
     fprintf(DMSM_fileout," \n");
     fprintf(DMSM_fileout," \n");
     fprintf(DMSM_fileout," Thread, Rank/Node, Jobs done, and CPU-time used: \n");
     k=0;
     t=0;
     for(i=0;i<DMSM_Total_MPI_Processes;i++)
     for(j=0;j<DMSM_Total_Number_Of_Threads_Per_Process;j++)
        {fprintf(DMSM_fileout," %6d %10d %10d %18.4e \n",j,i,DMSM_Jobs_Finished_All[k],DMSM_Used_Time_All[k]);
         if(tm1<DMSM_Used_Time_All[k]) tm1=DMSM_Used_Time_All[k];
         if(DMSM_Used_Time_All[k]>t9mm0)
           {
                 if(tmm1<(DMSM_DOUBLE_TIME_TYPE)0) tmm1=DMSM_Used_Time_All[k];
            else if(tmm1>DMSM_Used_Time_All[k])     tmm1=DMSM_Used_Time_All[k];
           }
         if(DMSM_Only_MPI_Enabled != 1) {DMSM_Total_Job_CPU_Time+=DMSM_Used_Time_All[k];}
         else {if(j==0) {DMSM_Total_Job_CPU_Time+=DMSM_Used_Time_All[k]*
                         DMSM_Only_MPI_Job_Processes_T[i];}}
         t=t+DMSM_Jobs_Finished_All[k];
         k++;
        }

     tm2=(DMSM_DOUBLE_TIME_TYPE)0;
     fprintf(DMSM_fileout," \n");
     fprintf(DMSM_fileout," \n");
     fprintf(DMSM_fileout," Rank/Node, Jobs done, and CPU-time used: \n");

     tmm2=(DMSM_DOUBLE_TIME_TYPE)(-5);
     k=0;
     for(i=0;i<DMSM_Total_MPI_Processes;i++)
        {
         tmp=(DMSM_DOUBLE_TIME_TYPE)0; l=0;
         for(j=0;j<DMSM_Total_Number_Of_Threads_Per_Process;j++)
            {tmp+=DMSM_Used_Time_All[k]; l+=DMSM_Jobs_Finished_All[k];k++;}
         if(tm2<tmp) tm2=tmp;
         if(tmp>t9mm0) {
                 if(tmm2<(DMSM_DOUBLE_TIME_TYPE)0) tmm2=tmp;
            else if(tmm2>tmp)                       tmm2=tmp;
            }
         fprintf(DMSM_fileout,"%10d %10d %18.4e \n",i,l,tmp);
        }

     fprintf(DMSM_fileout," \n");
     fprintf(DMSM_fileout," \n");
     if(t!=DMSM_Total_Num_Of_Jobs)
       {printf( "Error for total number of jobs completed: %d %d .\n", t, DMSM_Total_Num_Of_Jobs);
        DMSM_Error_and_Handling();}

     DMSM_Wall_Ending_Time=(DMSM_DOUBLE_TIME_TYPE)(DMSM_ACTUAL_TIME_FUNCTION/DMSM_TIME_REDUCING_FACTOR);
     tmp=DMSM_Wall_Ending_Time-DMSM_Wall_Beginning_Time;

     fprintf(DMSM_fileout," Total jobs done: %d %d .\n", t, DMSM_Total_Num_Of_Jobs);

     if(DMSM_Total_Num_Of_Jobs>0)
     {
     fprintf(DMSM_fileout," The maximum CPU-time of any jobs    : %18.4e . \n", tm0);
     fprintf(DMSM_fileout," The minimum CPU-time of any jobs    : %18.4e . \n", tmm0);
     fprintf(DMSM_fileout," The maximum CPU-time of any threads : %18.4e . \n", tm1);
     fprintf(DMSM_fileout," The minimum CPU-time of any threads : %18.4e . \n", tmm1);
     fprintf(DMSM_fileout," The master's total wall clock time  : %18.4e . \n", tmp);
     if(tm1>tmp)
     fprintf(DMSM_fileout," UNREASONABLE: the former is bigger than the later !!! \n");
     fprintf(DMSM_fileout," The maximum CPU-time of any process : %18.4e . \n", tm2);
     fprintf(DMSM_fileout," The minimum CPU-time of any process : %18.4e . \n", tmm2);
     fprintf(DMSM_fileout," The total CPU-time of all jobs      : %18.4e . \n", DMSM_Total_Job_CPU_Time);
     if(DMSM_Only_MPI_Enabled != 1) {
        fprintf(DMSM_fileout," The wall clock time of all threads  : %18.4e . \n",
                tmp*DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process);
        if(tmp>(DMSM_DOUBLE_TIME_TYPE)0)
        fprintf(DMSM_fileout," The percentage of the last two      :  %6.2f%c \n \n \n",
                (float)(100.000000*(float)DMSM_Total_Job_CPU_Time/(float)(tmp*
                 DMSM_Total_MPI_Processes*DMSM_Total_Number_Of_Threads_Per_Process)),pc);
        }
     else {
        fprintf(DMSM_fileout," The wall clock time of all threads  : %18.4e . \n",
                tmp*DMSM_Only_MPI_All_Processes );
        if(tmp>(DMSM_DOUBLE_TIME_TYPE)0)
        fprintf(DMSM_fileout," The percentage of the last two      :  %6.2f%c \n \n \n",
                (float)(100.000000*(float)DMSM_Total_Job_CPU_Time/(float)(tmp*
                 DMSM_Only_MPI_All_Processes )),pc);
        }
     }

     if(DMSM_ANY_ERROR_FOUND)
       {fprintf(DMSM_fileout,"                     \n");
        fprintf(DMSM_fileout,"                     \n");
        fprintf(DMSM_fileout," Some problem/error found! \n");
        fprintf(DMSM_fileout," Some problem/error found! \n");
        fprintf(DMSM_fileout," Some problem/error found! \n");
        printf("                     \n");
        printf("                     \n");
        printf(" Some problem/error found! \n");
        printf(" Some problem/error found! \n");
        printf(" Some problem/error found! \n");
        }

     else
       {fprintf(DMSM_fileout, " Initial data prepared for groups     enabled: %d .\n", DMSM_Initial_Data_Preparation_Enabled);
        fprintf(DMSM_fileout, " Result collection during computation enabled: %d .\n", DMSM_Result_Collection_Enabled);
        fprintf(DMSM_fileout," \n \n");
        fprintf(DMSM_fileout," ******************************************* \n");
        fprintf(DMSM_fileout," * Double-layer Master-Slave Model library * \n");
        fprintf(DMSM_fileout," * Version 4.3  by   HPCVL.org 2015        * \n");
        fprintf(DMSM_fileout," ******************************************* \n");
        fprintf(DMSM_fileout," \n");

        printf(" Initial data prepared for groups     enabled: %d .\n", DMSM_Initial_Data_Preparation_Enabled);
        printf(" Result collection during computation enabled: %d .\n", DMSM_Result_Collection_Enabled);
        printf(" \n");
        printf(" ******************************************* \n");
        printf(" * Double-layer Master-Slave Model library * \n");
        printf(" * Version 4.3  by   HPCVL.org 2015        * \n");
        printf(" ******************************************* \n");
        printf(" \n");
        }
    }


}




void DMSM_Working(void (*Do_The_Job) (int),
                  void (*Job_Group_Preparation) (int,int,int,int),
                  void (*Result_Collection) (int,int),
                  int   Result_Collection_Enabled)
{ DMSM_Do_The_Job=Do_The_Job;

  if(Job_Group_Preparation==NULL)
    {DMSM_Job_Group_Preparation=DMSM_Do_Nothing4;
     DMSM_Initial_Data_Preparation_Enabled=0;}
  else
    {DMSM_Job_Group_Preparation=Job_Group_Preparation;
     DMSM_Initial_Data_Preparation_Enabled=1;}

  DMSM_Result_Collection_Enabled=Result_Collection_Enabled;

  if(Result_Collection==NULL)
    {DMSM_Result_Collection=DMSM_Do_Nothing2;
     DMSM_Result_Collection_Enabled=0;}
  else DMSM_Result_Collection=Result_Collection;

  if(DMSM_Already_Initialized == 0)
    {printf( "Sorry! DMSM Lib is not initialized: %d .\n",DMSM_Already_Initialized); MPI_Finalize(); exit(0);}

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout," DMSM Working: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                   DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM Working: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                   DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }

  if(DMSM_Total_MPI_Processes == 1)
    { if(DMSM_Total_Number_Of_Threads_Per_Process == 1)
        {DMSM_Serial_Version();}
      else
        {DMSM_Pure_OPENMP_Model();}
     }
  else
    { if(DMSM_Total_Number_Of_Threads_Per_Process == 1)
        {DMSM_Pure_MPI_Model();}
      else
        {       if(DMSM_Job_Distribution_Plan==11)  DMSM_Model_Plan11();
           else if(DMSM_Job_Distribution_Plan==12)  DMSM_Model_Plan12();
           else if(DMSM_Job_Distribution_Plan==13)  DMSM_Model_Plan13();
           else if(DMSM_Job_Distribution_Plan==21)  DMSM_Model_Plan21();
           else if(DMSM_Job_Distribution_Plan==22)  DMSM_Model_Plan22();
           else if(DMSM_Job_Distribution_Plan==23)  DMSM_Model_Plan23();
           else if(DMSM_Job_Distribution_Plan==31)  DMSM_Model_Plan31();
           else if(DMSM_Job_Distribution_Plan==32)  DMSM_Model_Plan32();
           else if(DMSM_Job_Distribution_Plan==33)  DMSM_Model_Plan33();
           else   {printf( "Not acceptable DMSM_Job_Distribution_Plan: %d .\n",DMSM_Job_Distribution_Plan);
                                                                                   MPI_Finalize(); exit(0);}
         }
     }

  DMSM_All_Jobs_Completed = 1;
  DMSM_Last_Result_Collection();

}




void DMSM_Result_Collection_Locked(int myrank, int from)
{ DMSM_Set_Final_Result_Lock();
  (*DMSM_Result_Collection)(myrank, from);
  DMSM_Unset_Final_Result_Lock();
}




void DMSM_Last_Result_Collection()
{int i, j;
 if((DMSM_COLLECT_RESULTS_PARALLEL == 1) && (DMSM_Total_Number_Of_Threads_Per_Process > 1))
   {
#pragma omp parallel default(shared) private(i,j)
      {if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
         {DMSM_Total_Threads=omp_get_num_threads();
          if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
            {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
             DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
             MPI_Finalize(); exit(0);}
          };

       DMSM_Set_Final_Result_Lock();
       if(DMSM_My_MPI_Rank == DMSM_MASTER)
         {if(DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO)
            {for(i=0;i<DMSM_Total_MPI_Processes;i++)
                {if(i != DMSM_MASTER)
                   {for(j=0;j<DMSM_Total_Number_Of_Threads_Per_Process_Got[i];j++)
                       (*DMSM_Result_Collection)(DMSM_My_MPI_Rank, i);}
                 }
             };
          (*DMSM_Result_Collection)(DMSM_My_MPI_Rank, DMSM_My_MPI_Rank);
          }
       else {(*DMSM_Result_Collection)(DMSM_My_MPI_Rank, DMSM_My_MPI_Rank);}
       DMSM_Unset_Final_Result_Lock();
       }
    }
 else
   {MPI_Barrier(DMSM_Communicator);
    for(i=0;i<DMSM_Total_MPI_Processes;i++)
       {if(DMSM_My_MPI_Rank == DMSM_MASTER) (*DMSM_Result_Collection)(DMSM_My_MPI_Rank, i);
        else {if(DMSM_My_MPI_Rank == i)     (*DMSM_Result_Collection)(DMSM_My_MPI_Rank, i);};
        MPI_Barrier(DMSM_Communicator);
        }
    }
}




void DMSM_Serial_Version()
{ int i;

  if(DMSM_Total_MPI_Processes != 1)
    {printf( "ERROR: DMSM_Serial_Version is only for one process/node: %d .\n",DMSM_Total_MPI_Processes); MPI_Finalize(); exit(0);}

  if(DMSM_Total_Number_Of_Threads_Per_Process != 1)
    {printf( "ERROR: DMSM_Serial_Version is only for one thread each node: %d .\n",DMSM_Total_Number_Of_Threads_Per_Process); MPI_Finalize(); exit(0);}

  DMSM_Total_Slave_Should_Home=0;
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {DMSM_No_More_Groups_From[i]=2;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;

  DMSM_My_Thread_Number=0;

  DMSM_Serial_Core();

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++) DMSM_Jobs_Assigned_To_Thread_By_Master[i]=-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Serial_Version: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                            DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Serial_Version: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                            DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }
}




void DMSM_Pure_OPENMP_Model()
{ int i;

  if(DMSM_Total_MPI_Processes != 1)
    {printf( "ERROR: DMSM_Pure_OPENMP_Model is only for one process/node: %d .\n",DMSM_Total_MPI_Processes); MPI_Finalize(); exit(0);}

  DMSM_Total_Slave_Should_Home=0;
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]+1;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;

#pragma omp parallel default(shared)
  {
   if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
     {DMSM_Total_Threads=omp_get_num_threads();
      if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
        {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
         DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
         MPI_Finalize(); exit(0);}
      }
#pragma omp barrier
   DMSM_Master_Threads_Allocate();
   }

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++) DMSM_Jobs_Assigned_To_Thread_By_Master[i]=-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Pure_OPENMP_Model: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                               DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Pure_OPENMP_Model: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                               DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }

}




void DMSM_Pure_MPI_Model()
{ int i;

  if(DMSM_Total_Number_Of_Threads_Per_Process != 1)
    {printf( "ERROR: DMSM_Pure_MPI_Model is only for one thread each node: %d .\n",DMSM_Total_Number_Of_Threads_Per_Process); MPI_Finalize(); exit(0);}

  DMSM_Total_Slave_Should_Home=DMSM_Total_MPI_Processes-1;
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]+1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;

  DMSM_My_Thread_Number=0;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     DMSM_Master_Pure_MPI();
  else
     DMSM_Slaves_Pure_MPI();

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++) DMSM_Jobs_Assigned_To_Thread_By_Master[i]=-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Pure_MPI_Model: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                            DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Pure_MPI_Model: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                            DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }
}




void DMSM_Model_Plan11()
{ int i;

  DMSM_Total_Slave_Should_Home=DMSM_Total_MPI_Processes-1;
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]+1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     DMSM_Master_Pure_MPI();
  else
     DMSM_Slave_Outside_OPENMP();

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++) DMSM_Jobs_Assigned_To_Thread_By_Master[i]=-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan11: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan11: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }
}




void DMSM_Model_Plan12()
{ int i;

  DMSM_Total_Slave_Should_Home=DMSM_Total_MPI_Processes-1;
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]+1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     DMSM_Master_Pure_MPI();
  else
    {
#pragma omp parallel default(shared)
     {
      if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
        {DMSM_Total_Threads=omp_get_num_threads();
         if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
           {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
            DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
            MPI_Finalize(); exit(0);}
         }
#pragma omp barrier
      DMSM_One_Slave_in_OPENMP();
#pragma omp barrier
      }
     }

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan12: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan12: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }
}




void DMSM_Model_Plan13()
{ int i;

  DMSM_Total_Slave_Should_Home=0;
  for(i=0; i<DMSM_Total_MPI_Processes; i++)
      if(i!=DMSM_MASTER) DMSM_Total_Slave_Should_Home+=DMSM_Total_Number_Of_Threads_Per_Process_Got[i];
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[i]+1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[i]-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     DMSM_Master_Pure_MPI();
  else
    {
#pragma omp parallel default(shared)
     {
      if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
        {DMSM_Total_Threads=omp_get_num_threads();
         if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
           {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
            DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
            MPI_Finalize(); exit(0);}
         }
#pragma omp barrier
      DMSM_All_Slaves_in_OPENMP();
#pragma omp barrier
      }
     }

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan13: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan13: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }
}



void DMSM_Model_Plan21()
{
  int i;

  DMSM_Total_Slave_Should_Home=DMSM_Total_MPI_Processes-1;
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[i]+1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;


  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {
#pragma omp parallel default(shared)
      {
       if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
         {DMSM_Total_Threads=omp_get_num_threads();
          if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
            {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
             DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
             MPI_Finalize(); exit(0);}
          }
#pragma omp barrier

      if(DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO)
        {DMSM_Master_All();}
      else
        {DMSM_Master_Threads_Allocate();}
#pragma omp barrier
       }
     }
  else
    {DMSM_Slave_Outside_OPENMP();}

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++) DMSM_Jobs_Assigned_To_Thread_By_Master[i]=-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan21: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan21: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }
}




void DMSM_Model_Plan22()
{
  int i;

  DMSM_Total_Slave_Should_Home=DMSM_Total_MPI_Processes-1;
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]+1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;

#pragma omp parallel default(shared)
  {
   if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
     {DMSM_Total_Threads=omp_get_num_threads();
      if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
        {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
         DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
         MPI_Finalize(); exit(0);}
      }

#pragma omp barrier

   if(DMSM_My_MPI_Rank == DMSM_MASTER)
     {
      if(DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO)
        {DMSM_Master_All();}
      else
        {DMSM_Master_Threads_Allocate();}
      }
   else
     {DMSM_One_Slave_in_OPENMP();}
#pragma omp barrier
   }

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan22: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan22: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }

}




void DMSM_Model_Plan23()
{
  int i;

  DMSM_Total_Slave_Should_Home=0;
  for(i=0; i<DMSM_Total_MPI_Processes; i++)
      if(i!=DMSM_MASTER) DMSM_Total_Slave_Should_Home+=DMSM_Total_Number_Of_Threads_Per_Process_Got[i];
  DMSM_Total_Master_Thread_Should_Home=0;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[i]+1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[i]-1;

#pragma omp parallel default(shared)
  {
   if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
     {DMSM_Total_Threads=omp_get_num_threads();
      if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
        {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
         DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
         MPI_Finalize(); exit(0);}
      }

#pragma omp barrier

   if(DMSM_My_MPI_Rank == DMSM_MASTER)
     {
      if(DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO)
        {DMSM_Master_All();}
      else
        {DMSM_Master_Threads_Allocate();}
      }
   else
     {DMSM_All_Slaves_in_OPENMP();}
#pragma omp barrier
   }

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan23: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan23: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }

}




void DMSM_Model_Plan31()
{
  int i;

  DMSM_Total_Slave_Should_Home=DMSM_Total_MPI_Processes-1;
  DMSM_Total_Master_Thread_Should_Home=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]-1;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;
  DMSM_No_More_Groups_To[DMSM_MASTER]=DMSM_Total_Number_Of_Threads_Per_Process_Got[0]-1;


  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {
#pragma omp parallel default(shared)
      {
       if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
         {DMSM_Total_Threads=omp_get_num_threads();
          if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
            {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
             DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
             MPI_Finalize(); exit(0);}
          }
#pragma omp barrier

      if(DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO)
        {DMSM_Master_All();}
      else
        {DMSM_Master_Threads_Dynamic();}
#pragma omp barrier
       }
     }
  else
    {DMSM_Slave_Outside_OPENMP();}

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
     for(i=0;i<=DMSM_Total_Num_Of_Jobs;i++) DMSM_Jobs_Assigned_To_Thread_By_Master[i]=-1;

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan31: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan31: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }

}




void DMSM_Model_Plan32()
{
  int i;

  DMSM_Total_Slave_Should_Home=DMSM_Total_MPI_Processes-1;
  DMSM_Total_Master_Thread_Should_Home=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]-1;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=0;
  DMSM_No_More_Groups_To[DMSM_MASTER]=DMSM_Total_Number_Of_Threads_Per_Process_Got[0]-1;

#pragma omp parallel default(shared)
  {
   if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
     {DMSM_Total_Threads=omp_get_num_threads();
      if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
        {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
         DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
         MPI_Finalize(); exit(0);}
      }

#pragma omp barrier

   if(DMSM_My_MPI_Rank == DMSM_MASTER)
     {
      if(DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO)
        {DMSM_Master_All();}
      else
        {DMSM_Master_Threads_Dynamic();}
      }
   else
     {DMSM_One_Slave_in_OPENMP();}
#pragma omp barrier

   }

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan32: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan32: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }

}




void DMSM_Model_Plan33()
{
  int i;

  DMSM_Total_Slave_Should_Home=0;
  for(i=0; i<DMSM_Total_MPI_Processes; i++)
      if(i!=DMSM_MASTER) DMSM_Total_Slave_Should_Home+=DMSM_Total_Number_Of_Threads_Per_Process_Got[i];
  DMSM_Total_Master_Thread_Should_Home=DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_MASTER]-1;

  for(i=0;i<DMSM_Total_MPI_Processes;i++)
     {if(i==DMSM_MASTER) DMSM_No_More_Groups_From[i]=1;
      else               DMSM_No_More_Groups_From[i]=0;}
  for(i=0;i<DMSM_Total_MPI_Processes;i++)
      DMSM_No_More_Groups_To[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[i]-1;

#pragma omp parallel default(shared)
  {
   if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
     {DMSM_Total_Threads=omp_get_num_threads();
      if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
        {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
         DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank], DMSM_My_MPI_Rank) ;
         MPI_Finalize(); exit(0);}
      }

   /*
   if(DMSM_My_Thread_Number==0)
     {DMSM_Total_Times_Of_No_More_Groups_In_MPI=0;
      for(i=1;i<DMSM_Total_MPI_Processes;i++)
         {DMSM_No_More_Groups_From[i]=0;
          DMSM_No_More_Groups_To[i]=DMSM_Total_Number_Of_Threads_Per_Process_Got[i]-1;
          DMSM_Total_Times_Of_No_More_Groups_In_MPI+=(DMSM_No_More_Groups_To[i]-DMSM_No_More_Groups_From[i]+1);
          }
      DMSM_No_More_Groups_From[0]=1;
      DMSM_No_More_Groups_To[0]=DMSM_Total_Number_Of_Threads_Per_Process_Got[0]-1;
      }
      */

#pragma omp barrier

   if(DMSM_My_MPI_Rank == DMSM_MASTER)
     {
      if(DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO)
        {DMSM_Master_All();}
      else
        {DMSM_Master_Threads_Dynamic();}
      }
   else
     {DMSM_All_Slaves_in_OPENMP();}
#pragma omp barrier

   }

  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {fprintf(DMSM_fileout,"\n DMSM_Model_Plan33: %d %d %d \n \n", DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     printf(" DMSM_Model_Plan33: %d %d %d \n ",                   DMSM_Job_Distribution_Plan,
                          DMSM_Total_MPI_Processes, DMSM_Total_Number_Of_Threads_Per_Process);
     }

}




void DMSM_Master_All()
{
  int mreceived,i,thesource,the_thread, outter_loop;
  MPI_Status anmpistatus;

  if(DMSM_Total_MPI_Processes < 2)
    {printf(" Error routine DMSM_Master_All is for more processes: %d .\n",
              DMSM_Total_MPI_Processes);
              MPI_Finalize(); exit(0);}

  outter_loop=1;
  while(outter_loop==1)
     {
#pragma omp critical (DMSM_RECEIVE_12)
         {anmpistatus.MPI_SOURCE=-1;
          anmpistatus.MPI_TAG=-1;
          mreceived=-1;
          thesource=anmpistatus.MPI_SOURCE;
          the_thread=mreceived;
          MPI_Recv(&mreceived,1,MPI_INT,MPI_ANY_SOURCE,MPI_ANY_TAG, DMSM_Communicator,&anmpistatus);
          }
#pragma omp critical (DMSM_JOB_GROUP_12)
                 {DMSM_Next_Job_Range();
/* #pragma omp flush(mreceived,anmpistatus) */
                  if((DMSM_MPI_TAG_ZERO_POINT+mreceived+anmpistatus.MPI_SOURCE*DMSM_Total_Number_Of_Threads_Per_Process)!=anmpistatus.MPI_TAG)
                       {printf( "Error when slave (%d %d) reports to the Master in DMSM_Master_All() with tag %d.\n",
                        anmpistatus.MPI_SOURCE,mreceived,anmpistatus.MPI_TAG);
                        MPI_Finalize(); exit(0);}
                  thesource=anmpistatus.MPI_SOURCE;
                  the_thread=mreceived;
                  DMSM_Master_Assign_Slave(thesource, the_thread, anmpistatus.MPI_TAG);
                  }
     /*This flush not necessary, then disabled:
#pragma omp flush(DMSM_Total_Slave_Gone_Home,DMSM_Total_Master_Thread_Gone_Home)
      if((DMSM_Total_Slave_Gone_Home         >= DMSM_Total_Slave_Should_Home) &&
         (DMSM_Total_Master_Thread_Gone_Home >= DMSM_Total_Master_Thread_Should_Home) ) */
      if( DMSM_Total_Slave_Gone_Home         >= DMSM_Total_Slave_Should_Home )
         outter_loop=0;
      }
}




void DMSM_Master_Pure_MPI()
{
  int mreceived,i,thesource,the_thread,job_assigning;
  MPI_Status anmpistatus;

  if(DMSM_Total_MPI_Processes < 2)
    {printf(" Error routine DMSM_Master_Pure_MPI is for more processes: %d .\n",
              DMSM_Total_MPI_Processes);
              MPI_Finalize(); exit(0);}

  job_assigning=1;
  while(job_assigning==1)
     {
      DMSM_Next_Job_Range();
      MPI_Recv(&mreceived,1,MPI_INT,MPI_ANY_SOURCE,MPI_ANY_TAG,DMSM_Communicator,&anmpistatus);
      thesource=anmpistatus.MPI_SOURCE;
      the_thread=mreceived;
      if((DMSM_MPI_TAG_ZERO_POINT+the_thread+thesource*DMSM_Total_Number_Of_Threads_Per_Process)!=anmpistatus.MPI_TAG)
         {printf( "Error when slave (%d %d) reports to the Master in DMSM_Master_Pure_MPI() with tag %d.\n",
                   anmpistatus.MPI_SOURCE,the_thread,anmpistatus.MPI_TAG);
                   MPI_Finalize(); exit(0);}
      DMSM_Master_Assign_Slave(thesource, the_thread, anmpistatus.MPI_TAG);
      if(DMSM_Total_Slave_Gone_Home >= DMSM_Total_Slave_Should_Home) job_assigning=0;
      }
}




void DMSM_Next_Job_Range()
{DMSM_Job_Range[0]=DMSM_Job_Range[1]+1;
 if(DMSM_JOB_GROUPING == 1)
   {if(DMSM_Job_Range[1] < DMSM_Total_Num_Of_Jobs)
       DMSM_Job_Range[1]=DMSM_Job_Range[0]-1+DMSM_Num_Of_Jobs_Per_Group;
    }
 else
   {printf( "Error: not supported job grouping: %d.\n", DMSM_JOB_GROUPING);MPI_Finalize(); exit(0);}

 if(DMSM_Job_Range[1] > DMSM_Total_Num_Of_Jobs) DMSM_Job_Range[1]=DMSM_Total_Num_Of_Jobs;

 DMSM_Job_Group_Counter++;
 DMSM_Job_Range[2]=DMSM_Job_Group_Counter;
}




void DMSM_Master_Records(int sc, int td)
{int i,k;
 if((sc<0)||(sc>=DMSM_Total_MPI_Processes))
   {printf( "Error: for rank in DMSM_Master_Records: %d.\n", sc);MPI_Finalize(); exit(0);}
 if((td<-1)||(td>=DMSM_Total_Number_Of_Threads_Per_Process_Got[sc]))
   {printf( "Error: for thread number in DMSM_Master_Records: %d.\n", td);MPI_Finalize(); exit(0);}
 if(DMSM_Job_Range[0] <= 0)
   {printf( "Error: for job number in DMSM_Master_Records: %d.\n", DMSM_Job_Range[0]);MPI_Finalize(); exit(0);}
 else if(DMSM_Job_Range[0] <= DMSM_Total_Num_Of_Jobs)
        {if(DMSM_MASTER_SLAVE_PROCESS_SHOWING)
            printf( "Master assigned job numbers %d through %d to slave number %d.\n", DMSM_Job_Range[0],
                   DMSM_Job_Range[1], sc);
         DMSM_Jobs_Assigned_To_Thread_By_Master[DMSM_Job_Range[0]]=td;
         k=DMSM_Job_Range[1];
         if(k > DMSM_Total_Num_Of_Jobs) k=DMSM_Total_Num_Of_Jobs;
         for(i=DMSM_Job_Range[0]; i<=k; i++) DMSM_Jobs_Assigned_By_Master[i]=sc;
         }
 else
   {DMSM_Times_No_More_Work_Instructed[td+sc*DMSM_Total_Number_Of_Threads_Per_Process]++;
    if(sc == DMSM_MASTER)
       DMSM_Total_Master_Thread_Gone_Home++;
    else DMSM_Total_Slave_Gone_Home++;
    if(DMSM_MASTER_SLAVE_PROCESS_SHOWING)
       printf( "Master told slave number %d no more job today.\n", sc);
    }

 if(omp_in_parallel())
   {
#pragma omp flush(DMSM_Total_Master_Thread_Gone_Home, DMSM_Total_Slave_Gone_Home)
    }

}




void DMSM_Master_Assign_Slave(int sc, int td, int tag)
{MPI_Send(DMSM_Job_Range, DMSM_JOB_RANGE_SIZE, MPI_INT, sc, tag, DMSM_Communicator);
 DMSM_Master_Records(sc, td);
 if(DMSM_Job_Range[0] <= DMSM_Total_Num_Of_Jobs)
    DMSM_Group_Prepare_Wraper(DMSM_Job_Range[0], DMSM_Job_Range[1], sc);
 else if(DMSM_Result_Collection_Enabled) DMSM_Result_Collection_Locked(DMSM_My_MPI_Rank, sc);
}




void DMSM_Group_Prepare_Wraper(int job_from, int job_to, int destination)
{
#pragma omp critical (DMSM_JOB_WRAPPER)
   {if(DMSM_Initial_Data_Preparation_Enabled)
      (*DMSM_Job_Group_Preparation)(job_from-1, job_to-1, DMSM_My_MPI_Rank, destination);}
 if(DMSM_Result_Collection_Enabled)
   DMSM_Result_Collection_Locked(DMSM_My_MPI_Rank, destination);
}




void DMSM_Job_Recorder(int p)
{  if((p<DMSM_JOB_RECORDER_LENTH) || (DMSM_Total_Num_Of_Jobs*DMSM_JOB_RECORDER_LENTH<p))
     {printf( "Error of job number %d %d in DMSM_Job_Recorder().\n", p, DMSM_Total_Num_Of_Jobs) ;
      MPI_Finalize(); exit(0);}
   (DMSM_Jobs_I_Did[p])++;
   DMSM_Jobs_I_Did[p+1]=DMSM_My_MPI_Rank;
   DMSM_Jobs_I_Did[p+2]=DMSM_My_Thread_Number;
   DMSM_Total_Jobs_I_Finished++;
}




void DMSM_All_Slaves_in_OPENMP()
{int working;
 working=1;
 while (working)
       {
#pragma omp critical (DMSM_05)
            {DMSM_Get_Job();
             if(DMSM_My_Job == DMSM_End_Of_Group_Jobs)
               {DMSM_Slave_Applying();
                DMSM_Get_Job();
                }
             }
        if(DMSM_My_Job != DMSM_End_Of_Group_Jobs) DMSM_Do_the_Job_Wraper();
        else if(DMSM_Times_No_More_Work_I_Received[DMSM_My_Thread_Number+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process]>0)
                working=0;
        }
 }




void DMSM_Slave_Outside_OPENMP()
{int working;
 working=1;
 while (working)
       {DMSM_Slave_Applying();
        if(DMSM_Times_No_More_Work_I_Received[DMSM_My_Thread_Number+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process]>0)
           working=0;
        else DMSM_Do_the_Job_Group();
        }
}



void DMSM_Do_the_Job_Group()
{int working;
#pragma omp parallel default(shared) private(working)
    {if(DMSM_Total_Number_Of_Threads_Per_Process > 1)
        DMSM_Total_Threads=omp_get_num_threads();
        if(DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank]!=DMSM_Total_Threads)
          {printf( "Error of Total Number of Threads in OpenMP: %d %d %d.\n", DMSM_Total_Threads,
           DMSM_Total_Number_Of_Threads_Per_Process_Got[DMSM_My_MPI_Rank],    DMSM_My_MPI_Rank) ;
           MPI_Finalize(); exit(0);}

     working=1;
     while (working)
       {DMSM_Get_Critical_Job();
        if(DMSM_My_Job != DMSM_End_Of_Group_Jobs) DMSM_Do_the_Job_Wraper();
        else working=0;
        }

#pragma omp barrier
     }
}




void DMSM_One_Slave_in_OPENMP()
{int working;
 working=1;
 while (working)
       {
#pragma omp critical (DMSM_06)
            {DMSM_Get_Job();
             if((DMSM_My_Job == DMSM_End_Of_Group_Jobs) && (DMSM_My_Thread_Number == DMSM_ALWAYS_ZERO))
               {DMSM_Slave_Applying();
                DMSM_Get_Job();
                }
             }
        if(DMSM_My_Job != DMSM_End_Of_Group_Jobs) DMSM_Do_the_Job_Wraper();
        else if(DMSM_Times_No_More_Work_I_Received[DMSM_ALWAYS_ZERO+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process]>0)
                working=0;
        }
 }




void DMSM_Slaves_Pure_MPI()
{int working;
 working=1;
 while (working)
       {DMSM_Get_Job();
        if(DMSM_My_Job == DMSM_End_Of_Group_Jobs)
          {DMSM_Slave_Applying();
           DMSM_Get_Job();
           }
         if(DMSM_My_Job != DMSM_End_Of_Group_Jobs) DMSM_Do_the_Serial_Job_Wraper();
         else if(DMSM_Times_No_More_Work_I_Received[DMSM_ALWAYS_ZERO+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process]>0)
                 working=0;
        }
 }




void DMSM_Get_Critical_Job()
{int old;
#pragma omp critical (DMSM_j)
  {old=DMSM_Job_Assigned;
   if(DMSM_Job_Assigned < DMSM_Job_To)
     {
      DMSM_Set_Initial_Lock();
      DMSM_Job_Assigned++;
      DMSM_My_Job=DMSM_Job_Assigned;
      DMSM_Job_Group_Information[DMSM_My_Job*DMSM_GROUP_INFORMATION_SIZE-3]=DMSM_Job_From;
      DMSM_Job_Group_Information[DMSM_My_Job*DMSM_GROUP_INFORMATION_SIZE-2]=DMSM_Job_To;
      DMSM_Job_Group_Information[DMSM_My_Job*DMSM_GROUP_INFORMATION_SIZE-1]=DMSM_Current_Group_Number;

      if(DMSM_My_Job != (old+1))
        {printf( "Error in DMSM_Get_Critical_Job: %d %d %d %d %d %d %d %d.\n", old, DMSM_My_Job, DMSM_Job_From,
                  DMSM_Job_Assigned, DMSM_Job_To, DMSM_Current_Group_Number, DMSM_My_MPI_Rank, DMSM_My_Thread_Number);
         DMSM_Error_and_Handling();}
     }
   else DMSM_My_Job=DMSM_End_Of_Group_Jobs;
  }
}




void DMSM_Get_Job()
{int old=DMSM_Job_Assigned;
   if(DMSM_Job_Assigned < DMSM_Job_To)
     {
      DMSM_Set_Initial_Lock();
      DMSM_Job_Assigned++;
      DMSM_My_Job=DMSM_Job_Assigned;
      DMSM_Job_Group_Information[DMSM_My_Job*DMSM_GROUP_INFORMATION_SIZE-3]=DMSM_Job_From;
      DMSM_Job_Group_Information[DMSM_My_Job*DMSM_GROUP_INFORMATION_SIZE-2]=DMSM_Job_To;
      DMSM_Job_Group_Information[DMSM_My_Job*DMSM_GROUP_INFORMATION_SIZE-1]=DMSM_Current_Group_Number;

      if(DMSM_My_Job != (old+1))
        {printf( "Error in DMSM_Get_Job: %d %d %d %d %d %d %d %d.\n", old, DMSM_My_Job, DMSM_Job_From,
                  DMSM_Job_Assigned, DMSM_Job_To, DMSM_Current_Group_Number, DMSM_My_MPI_Rank, DMSM_My_Thread_Number);
         DMSM_Error_and_Handling();}
     }
   else DMSM_My_Job=DMSM_End_Of_Group_Jobs;
}




void DMSM_Do_Nothing2(int my_rank, int destination){}
void DMSM_Do_Nothing4(int from, int to, int my_rank, int destination){}




void DMSM_Do_the_Serial_Job_Wraper()
{DMSM_Job_Recorder(DMSM_My_Job*DMSM_JOB_RECORDER_LENTH);
 DMSM_Beginning_Time=(DMSM_DOUBLE_TIME_TYPE)(DMSM_ACTUAL_TIME_FUNCTION/DMSM_TIME_REDUCING_FACTOR);
 if(DMSM_Only_MPI_Enabled) MPI_Bcast(&DMSM_My_Job,1,MPI_INT,DMSM_MASTER,DMSM_Only_MPI_Job_Comm);
 (*DMSM_Do_The_Job)(DMSM_My_Job-1);
 DMSM_Ending_Time=(DMSM_DOUBLE_TIME_TYPE)(DMSM_ACTUAL_TIME_FUNCTION/DMSM_TIME_REDUCING_FACTOR);
 DMSM_CPU_Time_Of_Jobs[DMSM_My_Job]=(DMSM_DOUBLE_TIME_TYPE)(DMSM_Ending_Time-DMSM_Beginning_Time);
}




void DMSM_Do_the_Job_Wraper()
{DMSM_Job_Recorder(DMSM_My_Job*DMSM_JOB_RECORDER_LENTH);
 DMSM_Beginning_Time=DMSM_CPU_Time_In_OpenMP();
 if(DMSM_Only_MPI_Enabled) MPI_Bcast(&DMSM_My_Job,1,MPI_INT,DMSM_MASTER,DMSM_Only_MPI_Job_Comm);
 (*DMSM_Do_The_Job)(DMSM_My_Job-1);
 DMSM_Ending_Time=DMSM_CPU_Time_In_OpenMP();
 DMSM_CPU_Time_Of_Jobs[DMSM_My_Job]=(DMSM_DOUBLE_TIME_TYPE)(DMSM_Ending_Time-DMSM_Beginning_Time);
}




void DMSM_Slave_Applying()
{  MPI_Status anmpistatus;
#pragma omp critical (DMSM_Slave_Applying_12)
   {DMSM_Job_Range[0]=-1;
    DMSM_Job_Range[1]=-1;
    if(DMSM_MASTER_SLAVE_PROCESS_SHOWING) printf(" Slave number %d reports to the Master that he is idle. \n",DMSM_My_MPI_Rank);
    MPI_Send(&DMSM_My_Thread_Number, 1, MPI_INT, DMSM_MASTER, DMSM_a_Tag_for_MPI, DMSM_Communicator);
    MPI_Recv(DMSM_Job_Range, DMSM_JOB_RANGE_SIZE, MPI_INT, DMSM_MASTER, DMSM_a_Tag_for_MPI, DMSM_Communicator,&anmpistatus);
    }
   if(DMSM_Job_Range[0] <= 0)
     {printf( "Error: for job number in DMSM_Slave_Applying: %d.\n", DMSM_Job_Range[0]);MPI_Finalize(); exit(0);}
   else if(DMSM_Job_Range[0] <= DMSM_Total_Num_Of_Jobs)
          {if(DMSM_MASTER_SLAVE_PROCESS_SHOWING) printf(" Slave number %d received jobs from %d to %d from the Master. \n",DMSM_My_MPI_Rank,DMSM_Job_Range[0],DMSM_Job_Range[1]);
           DMSM_Group_Prepare_Wraper(DMSM_Job_Range[0], DMSM_Job_Range[1], DMSM_My_MPI_Rank);
           DMSM_Job_From=DMSM_Job_Range[0];
           DMSM_Job_To=DMSM_Job_Range[1];
           DMSM_Job_Assigned=DMSM_Job_From-1;
           DMSM_Current_Group_Number=DMSM_Job_Range[2];
           if(DMSM_Job_From > DMSM_Job_To)
             {printf(" Error: for job numbers from and to in DMSM_Slave_Applying: %d %d %d %d.\n",
                     DMSM_Job_From, DMSM_Job_To, DMSM_My_MPI_Rank, DMSM_My_Thread_Number);MPI_Finalize(); exit(0);}
           }
   else
          {if(DMSM_MASTER_SLAVE_PROCESS_SHOWING) printf(" Slave number %d received NO-MORE work from the Master. \n",DMSM_My_MPI_Rank);
           if(DMSM_Result_Collection_Enabled) DMSM_Result_Collection_Locked(DMSM_My_MPI_Rank, DMSM_My_MPI_Rank);
           DMSM_Times_No_More_Work_I_Received[DMSM_My_Thread_Number+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process]++;
           }
}




void DMSM_Master_Threads_Dynamic()
{int working, idle_signal_check;
 working=1;
 while (working)
       {
#pragma omp critical (DMSM_12)
            {DMSM_Get_Job();
             if(DMSM_My_Job == DMSM_End_Of_Group_Jobs)
               {if(DMSM_MASTER_SLAVE_PROCESS_SHOWING) printf(" Slave number %d reports to the Master that he is idle. \n",DMSM_My_MPI_Rank);
#pragma omp critical (DMSM_JOB_GROUP_12)
                     {DMSM_Next_Job_Range();
                      DMSM_Job_From=DMSM_Job_Range[0];
                      DMSM_Job_To=DMSM_Job_Range[1];
                      DMSM_Job_Assigned=DMSM_Job_From-1;
                      DMSM_Current_Group_Number=DMSM_Job_Range[2];
                      DMSM_Master_Records(DMSM_MASTER,DMSM_My_Thread_Number);
                      }
                if(DMSM_Job_From <= 0)
                  {printf( "Error: for job number in DMSM_Master_Threads_Dynamic: %d.\n", DMSM_Job_From);MPI_Finalize(); exit(0);}
                else if(DMSM_Job_From <= DMSM_Total_Num_Of_Jobs)
                       {if(DMSM_MASTER_SLAVE_PROCESS_SHOWING) printf(" Slave number %d received jobs from %d to %d from the Master. \n",DMSM_My_MPI_Rank,DMSM_Job_From,DMSM_Job_To);
                        if(DMSM_Job_From > DMSM_Job_To)
                          {printf(" Error: for job numbers from and to in DMSM_Master_Threads_Dynamic: %d %d %d %d.\n",
                                  DMSM_Job_From, DMSM_Job_To, DMSM_My_MPI_Rank, DMSM_My_Thread_Number);MPI_Finalize(); exit(0);}
                        DMSM_Group_Prepare_Wraper(DMSM_Job_From, DMSM_Job_To, DMSM_My_MPI_Rank);
                        DMSM_Get_Job();
                        }
                else   {if(DMSM_MASTER_SLAVE_PROCESS_SHOWING) printf(" Slave number %d received NO-MORE Working from the Master. \n",DMSM_My_MPI_Rank);
                        if(DMSM_Result_Collection_Enabled) DMSM_Result_Collection_Locked(DMSM_My_MPI_Rank, DMSM_My_MPI_Rank);
                        DMSM_Times_No_More_Work_I_Received[DMSM_My_Thread_Number+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process]++;
                        }
                }
             }
        if(DMSM_My_Job != DMSM_End_Of_Group_Jobs) DMSM_Do_the_Job_Wraper();
        else if(DMSM_Times_No_More_Work_I_Received[DMSM_My_Thread_Number+DMSM_My_MPI_Rank*DMSM_Total_Number_Of_Threads_Per_Process]>0)
                working=0;
        }
 }




void DMSM_Master_Threads_Allocate()
{int working;
 working=1;
 while (working)
       {
#pragma omp critical (DMSM_01)
            {DMSM_Get_Job();
             if(DMSM_My_Job == DMSM_End_Of_Group_Jobs)
               {DMSM_Current_Group_Number++;
                if(DMSM_Current_Group_Number<=DMSM_First_Groups_By_Master_Threads)
                  {DMSM_Job_From=(DMSM_Current_Group_Number-1)*DMSM_Num_Of_Jobs_Per_Group+1;
                   DMSM_Job_To=DMSM_Job_From-1+DMSM_Num_Of_Jobs_Per_Group;
                   if(DMSM_Job_To>DMSM_Total_Num_Of_Jobs) DMSM_Job_To=DMSM_Total_Num_Of_Jobs;
                   DMSM_Job_Assigned=DMSM_Job_From-1;
                   if(DMSM_Job_From > DMSM_Job_To)
                     {printf(" Error: for job numbers from and to in DMSM_Master_Threads_Allocate: %d %d %d %d.\n",
                             DMSM_Job_From, DMSM_Job_To, DMSM_My_MPI_Rank, DMSM_My_Thread_Number);MPI_Finalize(); exit(0);}
                   DMSM_Group_Prepare_Wraper(DMSM_Job_From, DMSM_Job_To, DMSM_My_MPI_Rank);
                   DMSM_Get_Job();
                   }
                else if(DMSM_Result_Collection_Enabled) DMSM_Result_Collection_Locked(DMSM_My_MPI_Rank, DMSM_My_MPI_Rank);
                }
             }
        if(DMSM_My_Job != DMSM_End_Of_Group_Jobs) DMSM_Do_the_Job_Wraper();
        else  working=0;
        }
}




void DMSM_Serial_Core()
{int working;
 working=1;
 while (working)
       {DMSM_Get_Job();
        if(DMSM_My_Job == DMSM_End_Of_Group_Jobs)
          {     DMSM_Current_Group_Number++;
                if(DMSM_Current_Group_Number<=DMSM_Total_Num_Of_Job_Groups)
                  {DMSM_Job_From=(DMSM_Current_Group_Number-1)*DMSM_Num_Of_Jobs_Per_Group+1;
                   DMSM_Job_To=DMSM_Job_From-1+DMSM_Num_Of_Jobs_Per_Group;
                   if(DMSM_Job_To>DMSM_Total_Num_Of_Jobs) DMSM_Job_To=DMSM_Total_Num_Of_Jobs;
                   DMSM_Job_Assigned=DMSM_Job_From-1;
                   if(DMSM_Job_From > DMSM_Job_To)
                     {printf(" Error: for job numbers from and to in DMSM_Serial_Core: %d %d %d %d.\n",
                             DMSM_Job_From, DMSM_Job_To, DMSM_My_MPI_Rank, DMSM_My_Thread_Number);MPI_Finalize(); exit(0);}
                   DMSM_Group_Prepare_Wraper(DMSM_Job_From, DMSM_Job_To, DMSM_My_MPI_Rank);
                   DMSM_Get_Job();
                   }
                else if(DMSM_Result_Collection_Enabled) DMSM_Result_Collection_Locked(DMSM_My_MPI_Rank, DMSM_My_MPI_Rank);
           }
        if(DMSM_My_Job != DMSM_End_Of_Group_Jobs) DMSM_Do_the_Serial_Job_Wraper();
        else working=0;
        }
}




int DMSM_All_Jobs_Done() {return(DMSM_All_Jobs_Completed);}




void DMSM_All(       int  Total_Number_Of_Threads_Per_Process,
                     int  Job_Distribution_Plan,
                     int  Total_Num_Of_Jobs,
                     int  Num_Of_Jobs_Per_Group,
                     void (*Do_The_Job) (int),
                     void (*Job_Group_Preparation) (int,int,int,int),
                     void (*Result_Collection) (int,int),
                     int  Result_Collection_Enabled)
{ DMSM_Initialize(        Total_Number_Of_Threads_Per_Process,
                          Job_Distribution_Plan,
                          Total_Num_Of_Jobs,
                          Num_Of_Jobs_Per_Group);
  DMSM_Working(Do_The_Job,Job_Group_Preparation,Result_Collection,Result_Collection_Enabled);
  DMSM_Job_Distribution_Checkup();
  DMSM_Finalize();
}




int DMSM_Get_Master() {return(DMSM_MASTER);}




void DMSM_Set_Master(int m)
{ int m2;
  printf( "Error: DMSM_Set_Master has not been tested! \n");
    MPI_Finalize(); exit(0);
  if(DMSM_Set_Comm_Done > 0)
    {MPI_Bcast(&m,1,MPI_INT,DMSM_MASTER,DMSM_Communicator);
     m2=DMSM_Total_MPI_Processes;}
  else
    {MPI_Bcast(&m,1,MPI_INT,DMSM_MASTER,MPI_COMM_WORLD);
     MPI_Comm_size(MPI_COMM_WORLD,&m2);}
  DMSM_MASTER=m;
  if((DMSM_MASTER < 0) || (DMSM_MASTER >= m2))
    {printf( "ERROR in DMSM_Set_Master: %d .\n",DMSM_MASTER);
                                    MPI_Finalize(); exit(0);}
}




void DMSM_Set_Comm(MPI_Comm An_MPI_Communicator)
{ if(DMSM_Set_Comm_Done > 0)
    {printf( "ERROR: DMSM_Set_Comm can only be called once: %d .\n",DMSM_Set_Comm_Done);
                                                                MPI_Finalize(); exit(0);}
  MPI_Comm_dup(An_MPI_Communicator, &DMSM_Communicator);
  MPI_Comm_rank(DMSM_Communicator,&DMSM_My_MPI_Rank);
  MPI_Comm_size(DMSM_Communicator,&DMSM_Total_MPI_Processes);
  DMSM_Set_Comm_Done=1;
}




void DMSM_Gen_Comm_MPI_All(int Toal_Processes_For_a_Job,
                      MPI_Comm   All_Communicator, int  *All_Rank, int  *All_Processes,
                      MPI_Comm  *Job_Communicator, int  *Job_Rank, int  *Job_Processes,
                      MPI_Comm *Dist_Communicator, int *Dist_Rank, int *Dist_Processes)
{    DMSM_Set_Comm_10(         Toal_Processes_For_a_Job,
                                All_Communicator,      All_Rank,      All_Processes,
                                Job_Communicator,      Job_Rank,      Job_Processes,
                               Dist_Communicator,     Dist_Rank,     Dist_Processes);
 }




void DMSM_Set_Comm_10_2(int Toal_Processes_For_a_Job,
                      MPI_Comm   All_Communicator, int  *All_Rank, int  *All_Processes,
                      MPI_Comm  *Job_Communicator, int  *Job_Rank, int  *Job_Processes,
                      MPI_Comm *Dist_Communicator, int *Dist_Rank, int *Dist_Processes)
{int k;
 if(DMSM_Set_Comm_10_Called > 0)
   {printf( "ERROR: DMSM_Set_Comm_10 or DMSM_Gen_Comm_MPI_All can only be called once: %d .\n",DMSM_Set_Comm_10_Called);
                                                                                                MPI_Finalize(); exit(0);}
 MPI_Comm_dup(All_Communicator, &DMSM_Only_MPI_All_Comm);
 MPI_Comm_rank(DMSM_Only_MPI_All_Comm, &DMSM_Only_MPI_All_Rank);
 MPI_Comm_size(DMSM_Only_MPI_All_Comm, &DMSM_Only_MPI_All_Processes);
 (*All_Rank)=     DMSM_Only_MPI_All_Rank;
 (*All_Processes)=DMSM_Only_MPI_All_Processes;

 if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
    k=DMSM_MASTER;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
    k=DMSM_Only_MPI_All_Processes-1;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
    k=DMSM_Only_MPI_All_Processes-1;
 else {printf( "ERROR: illegal DMSM_ONLY_MPI_TRUE_MASTER_PLAN: %d .\n",DMSM_ONLY_MPI_TRUE_MASTER_PLAN);}

 DMSM_Only_MPI_Job_Processes=Toal_Processes_For_a_Job;

 MPI_Bcast(&DMSM_Only_MPI_Job_Processes,1,MPI_INT,k,DMSM_Only_MPI_All_Comm);
 if(DMSM_Only_MPI_Job_Processes < 1) DMSM_Only_MPI_Job_Processes=1;
 DMSM_Only_MPI_Job_Processes_E=DMSM_Only_MPI_Job_Processes;

 if(DMSM_Only_MPI_Job_Processes<DMSM_Only_MPI_All_Processes)
   {if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
      {k=DMSM_Only_MPI_All_Rank;
       MPI_Comm_split(DMSM_Only_MPI_All_Comm,(k+DMSM_Only_MPI_Job_Processes-1)/DMSM_Only_MPI_Job_Processes,
                                                           DMSM_Only_MPI_All_Rank,&DMSM_Only_MPI_Job_Comm);
       }
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
      {k=DMSM_Only_MPI_All_Processes-1-DMSM_Only_MPI_All_Rank;
       MPI_Comm_split(DMSM_Only_MPI_All_Comm,(k+DMSM_Only_MPI_Job_Processes-1)/DMSM_Only_MPI_Job_Processes,
                                                           DMSM_Only_MPI_All_Rank,&DMSM_Only_MPI_Job_Comm);
       }
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
      {k=DMSM_Only_MPI_All_Rank+1;
       if(DMSM_Only_MPI_All_Rank == (DMSM_Only_MPI_All_Processes-1))
          k=10*(DMSM_Only_MPI_All_Processes+DMSM_Only_MPI_Job_Processes+20);
       MPI_Comm_split(DMSM_Only_MPI_All_Comm,(k+DMSM_Only_MPI_Job_Processes-1)/DMSM_Only_MPI_Job_Processes,
                                                           DMSM_Only_MPI_All_Rank,&DMSM_Only_MPI_Job_Comm);
       }
    }
 else
   {if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
      {k=DMSM_Only_MPI_All_Rank;}
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
      {k=DMSM_Only_MPI_All_Processes-1-DMSM_Only_MPI_All_Rank;}
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
      {k=DMSM_Only_MPI_All_Processes-1-DMSM_Only_MPI_All_Rank;}
    MPI_Comm_split(DMSM_Only_MPI_All_Comm,1,k,&DMSM_Only_MPI_Job_Comm);
    }

 MPI_Comm_rank(DMSM_Only_MPI_Job_Comm, &DMSM_Only_MPI_Job_Rank);
 MPI_Comm_size(DMSM_Only_MPI_Job_Comm, &DMSM_Only_MPI_Job_Processes);
 MPI_Comm_dup( DMSM_Only_MPI_Job_Comm, Job_Communicator);
 MPI_Comm_rank((*Job_Communicator),    Job_Rank);
 if((*Job_Rank)!=DMSM_Only_MPI_Job_Rank)
   {printf( "ERROR: Job ranks in DMSM_Set_Comm_10 or DMSM_Gen_Comm_MPI_All: %d %d %d .\n",
                             (*Job_Rank), DMSM_Only_MPI_Job_Rank, DMSM_Only_MPI_All_Rank);
                                                                  MPI_Finalize(); exit(0);}
 (*Job_Processes)=DMSM_Only_MPI_Job_Processes;

 if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
    k=DMSM_Only_MPI_All_Rank;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
    k=DMSM_Only_MPI_All_Processes-DMSM_Only_MPI_All_Rank;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
    k=DMSM_Only_MPI_All_Processes-DMSM_Only_MPI_All_Rank;

 MPI_Comm_split(DMSM_Only_MPI_All_Comm,DMSM_Only_MPI_Job_Rank,k,&DMSM_Only_MPI_Dist_Comm);
 MPI_Comm_rank(DMSM_Only_MPI_Dist_Comm, &DMSM_Only_MPI_Dist_Rank);
 MPI_Comm_size(DMSM_Only_MPI_Dist_Comm, &DMSM_Only_MPI_Dist_Processes);
 MPI_Comm_dup( DMSM_Only_MPI_Dist_Comm, Dist_Communicator);
 MPI_Comm_rank((*Dist_Communicator),    Dist_Rank);
 if((*Dist_Rank)!=DMSM_Only_MPI_Dist_Rank)
   {printf( "ERROR: Dist ranks in DMSM_Set_Comm_10 or DMSM_Gen_Comm_MPI_All: %d %d %d .\n",
                             (*Dist_Rank), DMSM_Only_MPI_Dist_Rank, DMSM_Only_MPI_All_Rank);
                                                                  MPI_Finalize(); exit(0);}
 (*Dist_Processes)=DMSM_Only_MPI_Dist_Processes;

 if(DMSM_Only_MPI_Job_Rank == DMSM_JOB_MASTER) DMSM_Set_Comm(DMSM_Only_MPI_Dist_Comm);

 DMSM_Set_Comm_10_Called=1;
 }




void DMSM_Set_Comm_10(int Toal_Processes_For_a_Job,
                      MPI_Comm   All_Communicator, int  *All_Rank, int  *All_Processes,
                      MPI_Comm  *Job_Communicator, int  *Job_Rank, int  *Job_Processes,
                      MPI_Comm *Dist_Communicator, int *Dist_Rank, int *Dist_Processes)
{int k;
 if(DMSM_Set_Comm_10_Called > 0)
   {printf( "ERROR: DMSM_Set_Comm_10 or DMSM_Gen_Comm_MPI_All can only be called once: %d .\n",DMSM_Set_Comm_10_Called);
                                                                                                MPI_Finalize(); exit(0);}
 MPI_Comm_dup(All_Communicator, &DMSM_Only_MPI_All_Comm);
 MPI_Comm_rank(DMSM_Only_MPI_All_Comm, &DMSM_Only_MPI_All_Rank);
 MPI_Comm_size(DMSM_Only_MPI_All_Comm, &DMSM_Only_MPI_All_Processes);
 (*All_Rank)=     DMSM_Only_MPI_All_Rank;
 (*All_Processes)=DMSM_Only_MPI_All_Processes;

 if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
    k=DMSM_MASTER;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
    k=DMSM_Only_MPI_All_Processes-1;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
    k=DMSM_Only_MPI_All_Processes-1;
 else {printf( "ERROR: illegal DMSM_ONLY_MPI_TRUE_MASTER_PLAN: %d .\n",DMSM_ONLY_MPI_TRUE_MASTER_PLAN);}

 DMSM_Only_MPI_Job_Processes=Toal_Processes_For_a_Job;

 MPI_Bcast(&DMSM_Only_MPI_Job_Processes,1,MPI_INT,k,DMSM_Only_MPI_All_Comm);
 if(DMSM_Only_MPI_Job_Processes < 1) DMSM_Only_MPI_Job_Processes=1;
 DMSM_Only_MPI_Job_Processes_E=DMSM_Only_MPI_Job_Processes;

 if(DMSM_Only_MPI_Job_Processes<DMSM_Only_MPI_All_Processes)
   {if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
      {k=DMSM_Only_MPI_All_Rank;
       MPI_Comm_split(DMSM_Only_MPI_All_Comm,(k+DMSM_Only_MPI_Job_Processes-1)/DMSM_Only_MPI_Job_Processes,
                                                           DMSM_Only_MPI_All_Rank,&DMSM_Only_MPI_Job_Comm);
       }
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
      {k=DMSM_Only_MPI_All_Processes-1-DMSM_Only_MPI_All_Rank;
       MPI_Comm_split(DMSM_Only_MPI_All_Comm,(k+DMSM_Only_MPI_Job_Processes-1)/DMSM_Only_MPI_Job_Processes,
                                                           DMSM_Only_MPI_All_Rank,&DMSM_Only_MPI_Job_Comm);
       }
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
      {k=DMSM_Only_MPI_All_Rank+1;
       if(DMSM_Only_MPI_All_Rank == (DMSM_Only_MPI_All_Processes-1))
          k=10*(DMSM_Only_MPI_All_Processes+DMSM_Only_MPI_Job_Processes+20);
       MPI_Comm_split(DMSM_Only_MPI_All_Comm,(k+DMSM_Only_MPI_Job_Processes-1)/DMSM_Only_MPI_Job_Processes,
                                                           DMSM_Only_MPI_All_Rank,&DMSM_Only_MPI_Job_Comm);
       }
    }
 else
   {if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
      {k=DMSM_Only_MPI_All_Rank;}
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
      {k=DMSM_Only_MPI_All_Processes-1-DMSM_Only_MPI_All_Rank;}
    else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
      {k=DMSM_Only_MPI_All_Processes-1-DMSM_Only_MPI_All_Rank;}
    MPI_Comm_split(DMSM_Only_MPI_All_Comm,1,k,&DMSM_Only_MPI_Job_Comm);
    }

 MPI_Comm_rank(DMSM_Only_MPI_Job_Comm, &DMSM_Only_MPI_Job_Rank);
 MPI_Comm_size(DMSM_Only_MPI_Job_Comm, &DMSM_Only_MPI_Job_Processes);
 (*Job_Communicator)=DMSM_Only_MPI_Job_Comm;
 (*Job_Rank)=DMSM_Only_MPI_Job_Rank;
 (*Job_Processes)=DMSM_Only_MPI_Job_Processes;

 if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 1)
    k=DMSM_Only_MPI_All_Rank;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 10)
    k=DMSM_Only_MPI_All_Processes-DMSM_Only_MPI_All_Rank;
 else if(DMSM_ONLY_MPI_TRUE_MASTER_PLAN == 100)
    k=DMSM_Only_MPI_All_Processes-DMSM_Only_MPI_All_Rank;

 MPI_Comm_split(DMSM_Only_MPI_All_Comm,DMSM_Only_MPI_Job_Rank,k,&DMSM_Only_MPI_Dist_Comm);
 MPI_Comm_rank(DMSM_Only_MPI_Dist_Comm, &DMSM_Only_MPI_Dist_Rank);
 MPI_Comm_size(DMSM_Only_MPI_Dist_Comm, &DMSM_Only_MPI_Dist_Processes);
 (*Dist_Communicator)=DMSM_Only_MPI_Dist_Comm;
 (*Dist_Rank)=DMSM_Only_MPI_Dist_Rank;
 (*Dist_Processes)=DMSM_Only_MPI_Dist_Processes;

 if(DMSM_Only_MPI_Job_Rank == DMSM_JOB_MASTER) DMSM_Set_Comm(DMSM_Only_MPI_Dist_Comm);

 DMSM_Set_Comm_10_Called=1;
 }




void DMSM_MPI_All(    int  Total_Num_Of_Jobs,
                      int  Num_Of_Jobs_Per_Group,
                      void (*Do_The_Job) (int),
                      void (*Job_Group_Preparation) (int,int,int,int),
                      void (*Result_Collection) (int,int),
                      int  Result_Collection_Enabled                   )
{int k,only_mpi_running;
 DMSM_Only_MPI_Enabled=1;
 if(DMSM_Set_Comm_10_Called < 1)
   {printf( "ERROR: Communicators not set by calling DMSM_Set_Comm_10 or DMSM_Gen_Comm_MPI_All: %d .\n",DMSM_Set_Comm_10_Called);
                                                                                                         MPI_Finalize(); exit(0);}
 if(DMSM_Only_MPI_Job_Rank == DMSM_JOB_MASTER)
   {if((DMSM_Only_MPI_Job_Processes_T=(int *) malloc(DMSM_Only_MPI_Dist_Processes*sizeof(int)))==NULL)
      {printf( "Error on memory allocation in DMSM_MPI_All().\n") ;
       MPI_Finalize(); exit(0);}
    MPI_Gather(&DMSM_Only_MPI_Job_Processes,1,MPI_INT,DMSM_Only_MPI_Job_Processes_T,1,MPI_INT,DMSM_MASTER,DMSM_Only_MPI_Dist_Comm);
    DMSM_All(1,11,Total_Num_Of_Jobs,Num_Of_Jobs_Per_Group,
             Do_The_Job,Job_Group_Preparation,Result_Collection,Result_Collection_Enabled);
    k=DMSM_ONLY_MPI_NO_MORE_JOBS;
    /* if(DMSM_My_MPI_Rank != DMSM_MASTER) */
    MPI_Bcast(&k,1,MPI_INT,DMSM_JOB_MASTER,DMSM_Only_MPI_Job_Comm);
    free(DMSM_Only_MPI_Job_Processes_T);
    }

 else
   {only_mpi_running=1;
    while(only_mpi_running)
         {MPI_Bcast(&DMSM_My_Job,1,MPI_INT,DMSM_JOB_MASTER,DMSM_Only_MPI_Job_Comm);
          if(DMSM_My_Job > 0) (*Do_The_Job)(DMSM_My_Job-1);
          else only_mpi_running=0;
         }
   }

 MPI_Comm_free(&DMSM_Only_MPI_All_Comm);
 MPI_Comm_free(&DMSM_Only_MPI_Job_Comm);
 MPI_Comm_free(&DMSM_Only_MPI_Dist_Comm);
 DMSM_Only_MPI_Enabled=0;
 DMSM_Set_Comm_10_Called=0;

}




void DMSM_Error_and_Handling()
{ int decided_to_exit=0;
  DMSM_ANY_ERROR_FOUND=1;
  printf(" ERROR found in rank %d. \n", DMSM_My_MPI_Rank);
  if(DMSM_My_MPI_Rank == DMSM_MASTER)
    {DMSM_Open_Problem_File();
     fprintf(DMSM_file_problem," 1\n");
     fprintf(DMSM_file_problem," ERROR found. \n");
     fclose(DMSM_file_problem);}
  if(decided_to_exit) {MPI_Finalize(); exit(0);}
}




void DMSM_Set_Journal_Number(long jn)
{ if(jn < 0)
    {printf( "Error: number for journal file %ld is not acceptable. \n", jn);
     MPI_Finalize(); exit(0);}
  if(DMSM_Already_Initialized)
    {printf( "Error: the routine DMSM_Set_Journal_Number() must be called before DMSM_Initialize(). \n");
     MPI_Finalize(); exit(0);}
  DMSM_Journal_Number=jn;
}




void DMSM_Open_Journal_File()
{long l,l2;
 int i,j,k,c;
 char DMSM_Journal_File_Name[100];
 for(k=0; k<100; k++)
    {if(DMSM_Journal_Head[k]!='\0') DMSM_Journal_File_Name[k]=DMSM_Journal_Head[k];
        else break;
     }

 if(DMSM_Journal_Number>(long)0)
   {DMSM_Journal_File_Name[k]='_';
    k++;
    c=1;j=0;l=DMSM_Journal_Number;
    while(c)
         {if(l>(long)0)
            {j++;l=l/(long)10;}
          else c=0;
          }
    l=DMSM_Journal_Number;
    for(i=1; i<=j; i++)
       {l2=(long)(l-(l/(long)10)*(long)10);
        DMSM_Journal_File_Name[k+j-i]=((int)l2)+48;
        l=l/(long)10;
        }
    k+=j;
   }

 DMSM_Journal_File_Name[k]='.'; k++;
 DMSM_Journal_File_Name[k]='t'; k++;
 DMSM_Journal_File_Name[k]='x'; k++;
 DMSM_Journal_File_Name[k]='t'; k++;
 DMSM_Journal_File_Name[k]='\0';

 if(k > (100-10))
   {printf( "Error: unacceptable long journal file name %d. \n", k);
    MPI_Finalize(); exit(0);}

 DMSM_fileout=fopen(DMSM_Journal_File_Name,"w");

}




void DMSM_Open_Problem_File()
{long l,l2;
 int i,j,k,c;
 char DMSM_Problem_File_Name[100];
 for(k=0; k<100; k++)
    {if(DMSM_Problem_Head[k]!='\0') DMSM_Problem_File_Name[k]=DMSM_Problem_Head[k];
        else break;
     }

 if(DMSM_Journal_Number>(long)0)
   {DMSM_Problem_File_Name[k]='_';
    k++;
    c=1;j=0;l=DMSM_Journal_Number;
    while(c)
         {if(l>(long)0)
            {j++;l=l/(long)10;}
          else c=0;
          }
    l=DMSM_Journal_Number;
    for(i=1; i<=j; i++)
       {l2=(long)(l-(l/(long)10)*(long)10);
        DMSM_Problem_File_Name[k+j-i]=((int)l2)+48;
        l=l/(long)10;
        }
    k+=j;
   }

 DMSM_Problem_File_Name[k]='.'; k++;
 DMSM_Problem_File_Name[k]='t'; k++;
 DMSM_Problem_File_Name[k]='x'; k++;
 DMSM_Problem_File_Name[k]='t'; k++;
 DMSM_Problem_File_Name[k]='\0';

 if(k > (100-10))
   {printf( "Error: unacceptable long problem file name %d. \n", k);
    MPI_Finalize(); exit(0);}

 DMSM_file_problem=fopen(DMSM_Problem_File_Name,"w");
}



